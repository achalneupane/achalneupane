---
Authors: ["**Achal Neupane**"]
title: "Multivariate statistical analysis with agronomic data"
date: 2020-06-21T17:26:23-05:00
draft: false
output: html_document
tags:
- R
- Statistics
- Statistical_programming
summary: Statistics series
---



<div id="part-1" class="section level2">
<h2>Part 1</h2>
<p>In this exercise, I will be analyzing agronomic datasets from four fields. This data comes from four different farm fields and we try to investigate the effects of variable seeding rates in overall yield.</p>
<p>The table summarizes data from four corn fields. The top row is target seeding rates, in seeds per acre, the bottom row is
corn yields in bushels per acre.
This table summarizes all four fields at the Control Rate interval of 1000 which was also reproduced below in my anaysis. More importantly, we are interested in exploring the EffectSize and Required Replicates for properly examining any statistical evidence for the yield and variable seeding rates relationships.</p>
<table>
<thead>
<tr class="header">
<th>Rate</th>
<th>23000</th>
<th>24000</th>
<th>25000</th>
<th>26000</th>
<th>27000</th>
<th>28000</th>
<th>29000</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Yield</td>
<td>111.4216</td>
<td>155.0326</td>
<td>181.1176</td>
<td>227.5800</td>
<td>233.4623</td>
<td>242.1753</td>
<td>231.3890</td>
</tr>
</tbody>
</table>
<p>The algorithm to do this analysis is divided into several steps as described below:</p>
<ol style="list-style-type: decimal">
<li>I have used some external packages like ggplot2 to make the plots.</li>
<li>I have written several functions to analyze each individual field as well as merged field statistics that calculates cohens effect size, required replicates, and does ANOVA analysis on individual field as well as on merged data from all fields.</li>
<li>Function <code>calculate_field_mean_SD_and_get_RR_EffSize</code> also uses mean, sd, and counts from each field to calculate cohens d and Required replicates. It also uses pooled standard deviations and Mean (mean of Means) calculated from all four fields for different control Rates to calculate cohens d and required replicates of combined field data. I have tested this function on all four fields at ControlRate level of 500, 1000, 2000 and 3000 intervals, then based on the results (also merged the output of all four intervals to get the effectSize and Required Replicates compared), I have decided to choose 1000 intervals of ControlRates for further analysis for combined data from all four fields.</li>
<li>I have also shown ANOVA analysis followed by TUKEY HSD test to show which ControlRates of seeding have significant effect on Yield for each field and then for all fields (combined all four fields).</li>
<li>Lastly, I have plotted EffectSize Vs RequiredReplicates for each field and also for combined data from four fields.</li>
</ol>
<p>Here, we first install and load some of the packages (“multcompView”, “ggplot2”, “scales”, “data.table”) I will be using for this exercise.</p>
<pre class="r"><code># First, install missing packages and load them
myPackages &lt;- c(&quot;multcompView&quot;, &quot;ggplot2&quot;, &quot;scales&quot;, &quot;data.table&quot;)
my.installed.packages&lt;- installed.packages()
available.packages &lt;- myPackages %in% my.installed.packages
if (sum(!available.packages) &gt; 0){
  install.packages(myPackages[!available.packages])
}
# Load all required packages 
lapply(myPackages, require, character.only = TRUE)</code></pre>
<pre><code>## Loading required package: multcompView</code></pre>
<pre><code>## Warning: package &#39;multcompView&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Registered S3 methods overwritten by &#39;tibble&#39;:
##   method     from  
##   format.tbl pillar
##   print.tbl  pillar</code></pre>
<pre><code>## Warning: replacing previous import &#39;vctrs::data_frame&#39; by &#39;tibble::data_frame&#39;
## when loading &#39;dplyr&#39;</code></pre>
<pre><code>## Loading required package: scales</code></pre>
<pre><code>## Loading required package: data.table</code></pre>
<pre><code>## [[1]]
## [1] TRUE
## 
## [[2]]
## [1] TRUE
## 
## [[3]]
## [1] TRUE
## 
## [[4]]
## [1] TRUE</code></pre>
<p>We will first write and describe our function here. We will be using these function to manipulate our data, calculate Cohen’s d, required replicates and also do ANOVA and TukeyHSD paired test, then finally to plot our data.
We can calculate EffectSize and RequiredReplicates for these data using functions from previous homework <code>cohen.d</code> and
<code>required.replicates</code>. These two functions are then called within another function <code>calculate_field_mean_SD_and_get_RR_EffSize</code> that analyzes each individual field and also merged data from all four fields to calculate Effect Size, Required Replictaes and then plot the results.</p>
<pre class="r"><code>cohen.d &lt;- function(m1, s1, m2, s2){
  cohens_d &lt;-(abs(m1-m2)/sqrt((s1^2+s2^2)/2))
  return(cohens_d)
}
required.replicates &lt;- function (m1, s1, m2, s2, alpha=0.05, beta=0.2){
  n &lt;- 2* ((((sqrt((s1^2 + s2^2)/2))/(m1-m2))^2) * (qnorm((1-alpha/2)) + qnorm((1-beta)))^2) 
  return(round(n,0))
}</code></pre>
<p>We willl also perform ANOVA analysis with Tukey Test for paired comparision of mean for each field data as well as merged data at different ControlRate intervals. This function does Tukey HSD test and generates label for significant outcomes.</p>
<pre class="r"><code># Create function to get the labels for Tukey HSD:
generate_label_df &lt;- function(TUKEY, variable){
  
  # Extract labels and factor levels from Tukey post-hoc 
  Tukey.levels &lt;- TUKEY[[variable]][,4]
  Tukey.labels &lt;- data.frame(multcompLetters(Tukey.levels)[&#39;Letters&#39;])
  
  #I need to put the labels in the same order as in the boxplot :
  Tukey.labels$treatment=rownames(Tukey.labels)
  Tukey.labels=Tukey.labels[order(Tukey.labels$treatment) , ]
  return(Tukey.labels)
}</code></pre>
<p>This function does ANOVA and makes boxplots with Tukey statistics for comparing Mean yield.</p>
<pre class="r"><code>get_my_box_plot &lt;- function (field, plot_name = &quot;the Field&quot;) {
  model = lm(field$Yield ~ field$ControlRate.Levels)
  ANOVA = aov(model)
  
  # Tukey test to study each pair of treatment :
  TUKEY &lt;- TukeyHSD(x = ANOVA, &#39;field$ControlRate.Levels&#39;, conf.level = 0.95)
  
  # generate labels using function
  labels &lt;- generate_label_df(TUKEY , &quot;field$ControlRate.Levels&quot;)
  
  # rename columns for merging
  names(labels) &lt;- c(&#39;Letters&#39;, &#39;ControlRate.Levels&#39;)
  
  # Obtain letter positions for y axis using means
  yvalue &lt;- aggregate(. ~ ControlRate.Levels, data = field, mean)
  
  part2 &lt;- merge(labels, yvalue) #merge dataframes
  
  p &lt;- ggplot(field, aes(x = ControlRate.Levels, y = Yield)) +
  geom_blank() +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()) +
  labs(x = &#39;Control Rates&#39;, y = &#39;Mean Yield&#39;) +
  ggtitle(paste0(&quot;ControlRates Vs Mean yield for &quot;, plot_name),
  expression(atop(italic(&quot;(Anova:TukeyHSD)&quot;), &quot;&quot;))) +
  theme(plot.title = element_text(hjust = 0.5, face = &#39;bold&#39;)) +
  geom_boxplot(fill = &#39;grey&#39;, stat = &quot;boxplot&quot;) +
  geom_text(
  data = part2,
  aes(x = ControlRate.Levels, y = Yield, label = Letters),
  vjust = -3.5,
  hjust = -.5
  ) +
  geom_vline(aes(xintercept = 4.5), linetype = &quot;dashed&quot;) +
  theme(plot.title = element_text(vjust = -0.6))
  print(p)
  # print(TUKEY)
}</code></pre>
<p>Function to calculate mean and sd for each field for a given interval of
ControlRate and then calculate Required Replicates and EffectSize. I have made
one function to do all that so I can just use this function to analyze all
four fields. To analyze single field set <code>Single.Field.Analysis = TRUE</code>, and to analyze all four field together, set <code>Single.Field.Analysis = FALSE</code>,</p>
<pre class="r"><code>calculate_field_mean_SD_and_get_RR_EffSize &lt;- 
  function(
    field, intervals, Single.Field.Analysis = TRUE, printplot = FALSE){
  plot_NAME &lt;- (deparse(substitute(field)))
  if(Single.Field.Analysis &gt; 0){
  # removing rows with NAs in Yield
  sum(is.na(field$Yield))
  # field &lt;- field[!complete.cases(field$Yield), ]
  field &lt;- field[!is.na(field$Yield),]
  # as.factor(fieldA$ControlRate)
  table.field &lt;- table(as.factor(field$ControlRate))
  field$ControlRate.Levels &lt;- as.factor(intervals * ceiling(field$ControlRate/intervals))
  field.Count &lt;- setNames(aggregate(field$ControlRate, 
          by = list(field$ControlRate.Levels), FUN = length), c(&quot;ControlRate&quot;, &quot;Count&quot;))
  # Degree of freedome = n * k - k
  field.Count$degree.Freedom &lt;- 
    (field.Count$Count * length(field.Count$ControlRate)) -length(field.Count$ControlRate)
  
  field.mean &lt;-
    setNames(aggregate(
    field$Yield,
    by = list(field$ControlRate.Levels),
    FUN = mean
    ),
    c(&quot;ControlRate&quot;, &quot;Mean&quot;))
    
    field.SD &lt;-
    setNames(aggregate(
    field$Yield,
    by = list(field$ControlRate.Levels),
    FUN = sd
    ),
    c(&quot;ControlRate&quot;, &quot;SD&quot;))
  # plot individual fields with tukey test We will print box plot only if we
  # want for certain ControlRates intervals. Otherwise we will have too many
  # plots
  if(printplot == 1 ){
  get_my_box_plot(field, plot_name = plot_NAME)
  }
  # Else we work on the merged data with SD pooled; no need to calculate mean
  # and SD as we will be doing it below
  
  } else {
    temp.Field &lt;- field
    colnames(temp.Field)[colnames(temp.Field) == &quot;ControlRate&quot;] &lt;- &quot;ControlRate.Levels&quot;
    colnames(temp.Field)[colnames(temp.Field) == &quot;Mean&quot;] &lt;- &quot;Yield&quot;
    # get_my_box_plot(temp.Field)
    field.SD  &lt;- 
      as.data.frame(cbind(ControlRate = field[&quot;ControlRate&quot;], SD = field[&quot;SD_pooled&quot;]))
    field.mean &lt;- 
      as.data.frame(cbind(ControlRate = field[&quot;ControlRate&quot;], Mean = field[&quot;Mean&quot;]))
  }
  
# Calculate Required replicate and Effect Size from each
# field for ControlRate i vs i+1
# ReqRep_EffectSize_table &lt;- function (field.mean, field.SD){
Req.Rep.table.field &lt;- {}
for (i in 1:nrow(field.SD)){
  if(i+1 &gt; nrow(field.SD) ){
    break
  }
  temp.Effect.size &lt;-
    cohen.d(
    m1 = field.mean$Mean[i],
    s1 = field.SD$SD[i],
    m2 = field.mean$Mean[i + 1],
    s2 = field.SD$SD[i + 1]
    )
    
    tmp.req.reps &lt;-
    required.replicates(
    m1 = field.mean$Mean[i],
    s1 = field.SD$SD[i],
    m2 = field.mean$Mean[i + 1],
    s2 = field.SD$SD[i + 1]
    )
 
    tmp.table &lt;-
    cbind(
    Group = paste0(field.SD$ControlRate[i], &quot; Vs &quot;, field.SD$ControlRate[i +
    1]),
    EffectSize = temp.Effect.size,
    RequiredReplicates = tmp.req.reps
    )
      
      Req.Rep.table.field &lt;- rbind(Req.Rep.table.field, tmp.table)
}
if (Single.Field.Analysis &gt; 0) {
  return(
    list(
      field.mean = field.mean,
      fieldSD = field.SD,
      field.Count = field.Count,
      Req.Rep.table.field = Req.Rep.table.field
    )
  )
} else{
  return(list(Req.Rep.table.field = Req.Rep.table.field))
}
  }
###############################################End of Functions##########</code></pre>
<p>Now, we read datasets from four fields:</p>
<pre class="r"><code>fieldA &lt;-
  read.table(
  &quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldA.csv&quot;,
  header = TRUE,
  sep = &quot;,&quot;
  )
  # head(fieldA)
  dim(fieldA)</code></pre>
<pre><code>## [1] 6718    6</code></pre>
<pre class="r"><code>  tmp.fieldA &lt;- fieldA
  tmp.fieldA$ControlRate &lt;- as.factor(tmp.fieldA$ControlRate)
  # aggregate(fieldA$Yield, by = list(fieldA$ControlRate), FUN = mean)</code></pre>
<p>Note: There are 40 levels of Control rates in fieldA, we can reduce these levels, so what we can do is relevel them separated by ‘intervals’ of(say 1000) as used in ‘calculate_field_mean_SD’ function above. We can do the same for other fields. However, instead of checking all this one by one, we will be using the functions we described above to analyze these four fields data and get the plots.</p>
<pre class="r"><code>fieldB &lt;-
  read.table(
  &quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldB.csv&quot;,
  header = TRUE,
  sep = &quot;,&quot;
  )
  # head(fieldB)
  dim(fieldB)</code></pre>
<pre><code>## [1] 9321    6</code></pre>
<pre class="r"><code>  # fieldC
  fieldC &lt;-
  read.table(
  &quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldC.csv&quot;,
  header = TRUE,
  sep = &quot;,&quot;
  )
  # head(fieldC)
  dim(fieldC)</code></pre>
<pre><code>## [1] 10404     6</code></pre>
<pre class="r"><code>  # fieldD
  fieldD &lt;-
  read.table(
  &quot;https://raw.githubusercontent.com/achalneupane/data/master/fieldD.csv&quot;,
  header = TRUE,
  sep = &quot;,&quot;
  )
  # head(fieldD)
  dim(fieldD)</code></pre>
<pre><code>## [1] 12654     6</code></pre>
<p>we can calculate EffectSize and RequiredReplicates for these four fields using previous homeworks functions Cohen’s d and
required.replicates as described in the begining of this report.</p>
<p>Here, we can work on each individual field, first for the interval of 500, 1000,
2000 and 3000 ControlRates and decide which ControlRate interval fits the best
for our data. Then we we will work on merged data that we merge after
calculating individual fields (i.e using Mean and SD pooled for each
ControlRate from all fields)</p>
<p>We chan check how many replicates we need for each field if we compare at
the different ControlRates intervals starting with 500 to 3000. Set <code>eval = TRUE</code> to check this code.</p>
<p>At ControlRates interval of 500:</p>
<pre class="r"><code># Now we chan check how many replicates we need for each field if we compare at
# the different ControlRates intervals starting with 500 to 3000:
# At ControlRates interval of 500:
ControlRateInterval &lt;- 500
# field A
fieldA.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )
# field C
fieldC.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )
# field D
fieldD.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )
# Now, we merge all these so we can plot them together later
my_list &lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)
my_list_nms &lt;- setNames(my_list, c(&quot;fieldA&quot;, &quot;fieldB&quot;, &quot;fieldC&quot;, &quot;fieldD&quot;))
Merged.EffectSize.500 &lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &quot;id&quot;))
Merged.EffectSize.500$interval &lt;- 500</code></pre>
<p>At ControlRates interval of 1000:</p>
<pre class="r"><code>ControlRateInterval &lt;- 1000
# field A
fieldA.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )
# field C
fieldC.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )
# field D
fieldD.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )
# Now, we merge all these so we can plot them together later
my_list &lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)
my_list_nms &lt;- setNames(my_list, c(&quot;fieldA&quot;, &quot;fieldB&quot;, &quot;fieldC&quot;, &quot;fieldD&quot;))
Merged.EffectSize.1000 &lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &quot;id&quot;))
Merged.EffectSize.1000$interval &lt;- 1000</code></pre>
<p>At ControlRates interval of 2000:</p>
<pre class="r"><code>ControlRateInterval &lt;- 2000
# field A
fieldA.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )
# field C
fieldC.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )
# field D
fieldD.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )
# Now, we merge all these so we can plot them together later
my_list &lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)
my_list_nms &lt;- setNames(my_list, c(&quot;fieldA&quot;, &quot;fieldB&quot;, &quot;fieldC&quot;, &quot;fieldD&quot;))
Merged.EffectSize.2000 &lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &quot;id&quot;))
Merged.EffectSize.2000$interval &lt;- 2000</code></pre>
<p>At ControlRates interval of 3000:</p>
<pre class="r"><code>ControlRateInterval &lt;- 3000
# field A
fieldA.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, interval = ControlRateInterval
    )
# field B
fieldB.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval
    )
# field C
fieldC.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval
    )
# field D
fieldD.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldD, interval = ControlRateInterval
    )
# Now, we merge all these so we can plot them together later
my_list &lt;- list(fieldA.data$Req.Rep.table.field, fieldB.data$Req.Rep.table.field, 
                fieldC.data$Req.Rep.table.field, fieldD.data$Req.Rep.table.field)
my_list_nms &lt;- setNames(my_list, c(&quot;fieldA&quot;, &quot;fieldB&quot;, &quot;fieldC&quot;, &quot;fieldD&quot;))
Merged.EffectSize.3000 &lt;- 
  data.frame(rbindlist(lapply(my_list_nms, as.data.table), idcol = &quot;id&quot;))
Merged.EffectSize.3000$interval &lt;- 3000</code></pre>
<p>Now, let’s plot these data for EffectSize Vs RequiredReplicates for each Field and fordifferent ControlRate intervals</p>
<pre class="r"><code>EffectSize.Plot &lt;-
  rbind(
  Merged.EffectSize.500,
  Merged.EffectSize.1000,
  Merged.EffectSize.2000,
  Merged.EffectSize.3000
  )
EffectSize.Plot$id &lt;- factor(EffectSize.Plot$id)
EffectSize.Plot$interval &lt;- factor(EffectSize.Plot$interval)
EffectSize.Plot$Group &lt;- factor(EffectSize.Plot$Group)
EffectSize.Plot$EffectSize &lt;- as.numeric(EffectSize.Plot$EffectSize)
EffectSize.Plot$RequiredReplicates &lt;- as.numeric(EffectSize.Plot$RequiredReplicates)
head(EffectSize.Plot)</code></pre>
<pre><code>##       id          Group EffectSize RequiredReplicates interval
## 1 fieldA 23000 Vs 23500 1.03546848                 15      500
## 2 fieldA 23500 Vs 24000 0.73574477                 29      500
## 3 fieldA 24000 Vs 25000 1.48620683                  7      500
## 4 fieldA 25000 Vs 25500 0.41887785                 89      500
## 5 fieldA 25500 Vs 26000 1.10746983                 13      500
## 6 fieldA 26000 Vs 26500 0.07390218               2874      500</code></pre>
<pre class="r"><code>scaleFUN &lt;- function(x) sprintf(&quot;%.2f&quot;, x)
# To eliminate any outliers and noise, we can plot effect Size between 0-1 and
# Required Replicates upto 5000
EffectSize.Plot &lt;- EffectSize.Plot[EffectSize.Plot$RequiredReplicates&lt;= 5000,]
EffectSize.Plot &lt;- EffectSize.Plot[EffectSize.Plot$EffectSize&lt;= 1,]
EffectSize.Plot$Group &lt;- factor(EffectSize.Plot$Group)
ggplot(EffectSize.Plot, aes(x=EffectSize, y=RequiredReplicates)) + 
  ggtitle(&quot;Effect size Vs Required replicates for each field data&quot;) +
  geom_point(aes(shape = Group), size = 2)  + 
  scale_shape_manual(values=1:nlevels(EffectSize.Plot$Group)) +
  geom_line() + 
  scale_x_continuous(&quot;EffectSize&quot;, breaks=c(0,0.2,0.5,1), labels = scaleFUN) +
  theme_bw() +
  theme(axis.title = element_text(size=14,face=&quot;bold&quot;), axis.text = element_text(size=10), 
        plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)) +
  facet_wrap(~id)</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Based on the plot we just plotted above, RequiredReplicates increase significantly with decreasing Effect Size.</p>
<p>Additionally, after running all these <code>ControlRateInterval</code>, I noticed that the Required replicates decrease for the higher ControlRate intervals. The reason I did not include this in my report is because it gives lots of output. SO, if we consider taking the ControlRate intervals of only 1000 and get the field mean and field SD for each field as below.</p>
<p>Now, I will only limit my analysis for all fields (merged) for the Control Rates interval of 1000.</p>
<pre class="r"><code># If we consider taking the ControlRate intervals of 1000 and get the field mean and field
# SD for each field:
ControlRateInterval &lt;- 1000
# field A
fieldA.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldA, intervals = ControlRateInterval, printplot = TRUE
    )</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>fieldA.data</code></pre>
<pre><code>## $field.mean
##   ControlRate      Mean
## 1       23000  63.01258
## 2       24000 100.63078
## 3       25000 172.57527
## 4       26000 237.45902
## 5       27000 237.33206
## 6       28000 238.59221
## 7       29000 195.73072
## 
## $fieldSD
##   ControlRate       SD
## 1       23000 15.45740
## 2       24000 51.36069
## 3       25000 41.82109
## 4       26000 31.64908
## 5       27000 44.09390
## 6       28000 38.11963
## 7       29000 53.59605
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       23000    24            161
## 2       24000    93            644
## 3       25000    44            301
## 4       26000  4381          30660
## 5       27000   786           5495
## 6       28000  1283           8974
## 7       29000    99            686
## 
## $Req.Rep.table.field
##      Group            EffectSize            RequiredReplicates
## [1,] &quot;23000 Vs 24000&quot; &quot;0.991869049115401&quot;   &quot;16&quot;              
## [2,] &quot;24000 Vs 25000&quot; &quot;1.53614606065372&quot;    &quot;7&quot;               
## [3,] &quot;25000 Vs 26000&quot; &quot;1.74957194606738&quot;    &quot;5&quot;               
## [4,] &quot;26000 Vs 27000&quot; &quot;0.00330802194919998&quot; &quot;1434501&quot;         
## [5,] &quot;27000 Vs 28000&quot; &quot;0.0305749319500618&quot;  &quot;16792&quot;           
## [6,] &quot;28000 Vs 29000&quot; &quot;0.921630680790716&quot;   &quot;18&quot;</code></pre>
<p>Based on the plot we can see that the Mean Yield is significant between 23000 vs all ControlRates , 24000 vs all Control Rates, 25000 Vs all Control rates. yield has no significant effect of Seeding from Control Rate between 26000 to 28000. Infact, it starts to decline significantly from 29000 Control Seedings.</p>
<pre class="r"><code># field B
fieldB.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldB, interval = ControlRateInterval, printplot = TRUE
    )</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>fieldB.data</code></pre>
<pre><code>## $field.mean
##   ControlRate     Mean
## 1       24000 125.8102
## 2       25000 180.9337
## 3       26000 215.8033
## 4       27000 229.8309
## 5       28000 240.5746
## 6       29000 238.9680
## 
## $fieldSD
##   ControlRate       SD
## 1       24000 33.51371
## 2       25000 33.36007
## 3       26000 28.10560
## 4       27000 19.11270
## 5       28000 14.00890
## 6       29000 14.26832
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       24000    36            210
## 2       25000   390           2334
## 3       26000  2455          14724
## 4       27000   798           4782
## 5       28000  5173          31032
## 6       29000   469           2808
## 
## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &quot;24000 Vs 25000&quot; &quot;1.64857878536612&quot;  &quot;6&quot;               
## [2,] &quot;25000 Vs 26000&quot; &quot;1.13048091635421&quot;  &quot;12&quot;              
## [3,] &quot;26000 Vs 27000&quot; &quot;0.583669852812804&quot; &quot;46&quot;              
## [4,] &quot;27000 Vs 28000&quot; &quot;0.641176026096802&quot; &quot;38&quot;              
## [5,] &quot;28000 Vs 29000&quot; &quot;0.113628916568796&quot; &quot;1216&quot;</code></pre>
<p>Here, we see significant difference between all Control Rates of Seeding except between 28000 Vs 29000. The seeding rates of 28000 has the highest Mean yield here.</p>
<pre class="r"><code># field C
fieldC.data &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = fieldC, interval = ControlRateInterval, printplot = TRUE
    )</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>fieldC.data</code></pre>
<pre><code>## $field.mean
##   ControlRate     Mean
## 1       23000 149.2422
## 2       24000 185.4175
## 3       25000 195.8427
## 4       26000 225.0309
## 5       27000 231.4881
## 6       28000 240.9121
## 
## $fieldSD
##   ControlRate        SD
## 1       23000 14.416699
## 2       24000 15.340123
## 3       25000 13.316623
## 4       26000 14.335465
## 5       27000 11.247829
## 6       28000  4.463365
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       23000    28            162
## 2       24000   170           1014
## 3       25000    69            408
## 4       26000  5697          34176
## 5       27000  4419          26508
## 6       28000    21            120
## 
## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &quot;23000 Vs 24000&quot; &quot;2.43022553971697&quot;  &quot;3&quot;               
## [2,] &quot;24000 Vs 25000&quot; &quot;0.725780226926124&quot; &quot;30&quot;              
## [3,] &quot;25000 Vs 26000&quot; &quot;2.1096712954133&quot;   &quot;4&quot;               
## [4,] &quot;26000 Vs 27000&quot; &quot;0.501162120939759&quot; &quot;63&quot;              
## [5,] &quot;27000 Vs 28000&quot; &quot;1.10136177756912&quot;  &quot;13&quot;</code></pre>
<p>Here, all Control rates of seeding are significantly diffferent with the highest Mean yield for 28000.</p>
<pre class="r"><code># field D
fieldD.data &lt;- calculate_field_mean_SD_and_get_RR_EffSize(
  field = fieldD, interval = ControlRateInterval, printplot = TRUE
  )</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>fieldD.data</code></pre>
<pre><code>## $field.mean
##   ControlRate     Mean
## 1       23000 113.4783
## 2       24000 157.5754
## 3       25000 176.8111
## 4       26000 227.5933
## 5       27000 240.8722
## 6       28000 247.0608
## 7       29000 230.0359
## 
## $fieldSD
##   ControlRate       SD
## 1       23000 24.58440
## 2       24000 39.35167
## 3       25000 36.11914
## 4       26000 26.50089
## 5       27000 17.44482
## 6       28000 15.63857
## 7       29000 25.17324
## 
## $field.Count
##   ControlRate Count degree.Freedom
## 1       23000    50            343
## 2       24000   372           2597
## 3       25000   132            917
## 4       26000  8284          57981
## 5       27000  1167           8162
## 6       28000  2631          18410
## 7       29000    18            119
## 
## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &quot;23000 Vs 24000&quot; &quot;1.34402785417073&quot;  &quot;9&quot;               
## [2,] &quot;24000 Vs 25000&quot; &quot;0.50928632312699&quot;  &quot;61&quot;              
## [3,] &quot;25000 Vs 26000&quot; &quot;1.60311540536477&quot;  &quot;6&quot;               
## [4,] &quot;26000 Vs 27000&quot; &quot;0.591895252495428&quot; &quot;45&quot;              
## [5,] &quot;27000 Vs 28000&quot; &quot;0.373564449867553&quot; &quot;112&quot;             
## [6,] &quot;28000 Vs 29000&quot; &quot;0.812438594873596&quot; &quot;24&quot;</code></pre>
<p>Here, all Control Rates of seeding are significantly different except for 26000-28000 Vs 29000 with the highest Mean yield for 28000.</p>
<p>Also, to check if my calculation of Control rates and yield mean matches with
the table in instruction (with 1000 intervals), we can check this with the merged field data ( i.e.
fieldA, fieldB, fieldC and fieldD) data. We should get the same Mean values in
the instruction table above.</p>
<pre class="r"><code>all.merged.raw.fields &lt;- rbind(fieldA, fieldB, fieldC, fieldD)
check.the.instruction.table &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(
  field = all.merged.raw.fields, intervals = ControlRateInterval
  )
check.the.instruction.table$field.mean</code></pre>
<pre><code>##   ControlRate     Mean
## 1       23000 111.4216
## 2       24000 155.0326
## 3       25000 181.1176
## 4       26000 227.5779
## 5       27000 233.4717
## 6       28000 242.1698
## 7       29000 231.3890</code></pre>
<p>We can now merge all four fields for SD, means and ControlRate level counts.</p>
<pre class="r"><code>merged.SD.4.plots &lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$fieldSD,
  fieldB.data$fieldSD,
  fieldC.data$fieldSD,
  fieldD.data$fieldSD
  )
  )
  
  
  merged.Mean.4.plots &lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.mean,
  fieldB.data$field.mean,
  fieldC.data$field.mean,
  fieldD.data$field.mean
  )
  )
  
  
  merged.Count.4.plots &lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.Count,
  fieldB.data$field.Count,
  fieldC.data$field.Count,
  fieldD.data$field.Count
  )
  )</code></pre>
<p>We need SD pooled for these levels so we can calculate Cohen’s d and Required
Replicates.</p>
<pre class="r"><code>levels(merged.SD.4.plots$ControlRate)</code></pre>
<pre><code>## [1] &quot;23000&quot; &quot;24000&quot; &quot;25000&quot; &quot;26000&quot; &quot;27000&quot; &quot;28000&quot; &quot;29000&quot;</code></pre>
<p>First, we merged all merged.SD.4.plots, merged.Mean.4.plots and merged.Count.4.plots from all fields.</p>
<pre class="r"><code># Therefore, now merging all three dataframes from all four plots for SD, Mean
# and counts by ControlRate column
Mean_SD_Count.dat &lt;- Reduce(function(...)
  merge(..., by = c(&quot;ControlRate&quot;, &quot;grp&quot;), all.x = TRUE),
  lapply(
    list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots),
    transform,
    grp = ave(seq_along(ControlRate), ControlRate, FUN = seq_along)
  ))
head(Mean_SD_Count.dat)</code></pre>
<pre><code>##   ControlRate grp      Mean       SD Count degree.Freedom
## 1       23000   1  63.01258 14.41670    24            161
## 2       23000   2 113.47830 15.45740    28            162
## 3       23000   3 149.24222 24.58440    50            343
## 4       24000   1 100.63078 15.34012    36            210
## 5       24000   2 125.81017 33.51371    93            644
## 6       24000   3 157.57538 39.35167   170           1014</code></pre>
<p>Now, we can also run ANOVA on the merged data <code>Mean_SD_Count.dat</code></p>
<pre class="r"><code>Mean_SD_Count_merged_for_all_four_plots &lt;- Mean_SD_Count.dat
Mean_SD_Count_merged_for_all_four_plots$Yield &lt;-
Mean_SD_Count_merged_for_all_four_plots$Mean
Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels &lt;-
Mean_SD_Count_merged_for_all_four_plots$ControlRate
get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = &quot;all four fields&quot;)</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Based on this plot, from all fields, we can tell that, Control Rates of seeding of 23000-25000 has significantly less Mean yield as compared to 26000-29000. It looks like Control Rates of Seeding between 27000 and 28000 has the highest Mean yield.</p>
<p>We now calculate pooled SD for merged 4 plots <code>Mean_SD_Count.dat</code> to calculate Effect Size and Required Replicates for all fields (combined analysis) using pooled SD.</p>
<pre class="r"><code>pooled.dat &lt;- Mean_SD_Count.dat
# # Pooled sd can be calculated as: 
pooled.dat$df &lt;- pooled.dat$Count-1
# pooled SD is :
# pooledSD &lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) )
# We can calculate our SD pooled using this formula:
#  s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
# We will derrive this in steps as below:
pooled.dat$df &lt;- pooled.dat$Count-1
pooled.dat$sd.square &lt;- pooled.dat$SD^2 
pooled.dat$ss &lt;- pooled.dat$sd.square * pooled.dat$df
# We can use convenience function (aggregate) for splitting and calculating the necessary sums.
ds &lt;- aggregate(ss ~ ControlRate, data = pooled.dat, sum)
# Two different built in methods for split apply, we could use aggregate for
# both if we wanted. This calculates our degrees of freedom.
ds$df &lt;- tapply(pooled.dat$df, pooled.dat$ControlRate, sum) 
# divide ss by df and then we get sd square
ds$sd.square &lt;- ds$ss / ds$df
# Finally, we can get our sd pooled
ds$SD_pooled &lt;- sqrt(ds$sd.square)
ds</code></pre>
<pre><code>##   ControlRate          ss    df sd.square SD_pooled
## 1       23000    40846.73    99  412.5933  20.31239
## 2       24000  1351941.68   667 2026.8991  45.02110
## 3       25000   934566.13   631 1481.0874  38.48490
## 4       26000 16376572.57 20813  786.8434  28.05073
## 5       27000  9357584.58  7166 1305.8309  36.13628
## 6       28000  8410659.15  9104  923.8422  30.39477
## 7       29000  1409909.69   583 2418.3700  49.17693</code></pre>
<pre class="r"><code># However, we could also calculate our sd_pooled as below and get the same results :
sd_pooled &lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$ControlRate),
         function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) )</code></pre>
<p>Now, we calculate Mean (Mean of Means) from the merged table <code>Mean_SD_Count.dat</code>, so we can calculate Cohen’s d and RequiredReplicates for all four field combined.</p>
<pre class="r"><code>ds.Mean &lt;-
  setNames(aggregate(
  Mean_SD_Count.dat$Mean,
  by = list(Mean_SD_Count.dat$ControlRate),
  FUN = mean
  ),
  c(&quot;ControlRate&quot;, &quot;Mean&quot;))
ds &lt;- merge(ds, ds.Mean, by.x = &quot;ControlRate&quot;)
ds</code></pre>
<pre><code>##   ControlRate          ss    df sd.square SD_pooled     Mean
## 1       23000    40846.73    99  412.5933  20.31239 108.5777
## 2       24000  1351941.68   667 2026.8991  45.02110 142.3585
## 3       25000   934566.13   631 1481.0874  38.48490 181.5407
## 4       26000 16376572.57 20813  786.8434  28.05073 226.4716
## 5       27000  9357584.58  7166 1305.8309  36.13628 234.8808
## 6       28000  8410659.15  9104  923.8422  30.39477 241.7849
## 7       29000  1409909.69   583 2418.3700  49.17693 221.5782</code></pre>
<p>Now we calculate the Effect Size and Cohen’s D for the combined 4 plots using mean yield and sd pooled for different ControlRate</p>
<p>Now we calculate the Effect Size and Cohen’s D for the combined 4 plots using mean yield and sd pooled for different ControlRate usinf our function <code>calculate_field_mean_SD_and_get_RR_EffSize</code>.</p>
<pre class="r"><code>RequiredReplicates_for_all_fields &lt;- 
  calculate_field_mean_SD_and_get_RR_EffSize(
    field = ds, intervals = ControlRateInterval, Single.Field.Analysis = FALSE
    )
RequiredReplicates_for_all_fields</code></pre>
<pre><code>## $Req.Rep.table.field
##      Group            EffectSize          RequiredReplicates
## [1,] &quot;23000 Vs 24000&quot; &quot;0.967241180364881&quot; &quot;17&quot;              
## [2,] &quot;24000 Vs 25000&quot; &quot;0.935567380483301&quot; &quot;18&quot;              
## [3,] &quot;25000 Vs 26000&quot; &quot;1.33427570681431&quot;  &quot;9&quot;               
## [4,] &quot;26000 Vs 27000&quot; &quot;0.259967383137317&quot; &quot;232&quot;             
## [5,] &quot;27000 Vs 28000&quot; &quot;0.206777477800031&quot; &quot;367&quot;             
## [6,] &quot;28000 Vs 29000&quot; &quot;0.49430440628565&quot;  &quot;64&quot;</code></pre>
<p>We can plot this results for better visualization of pattern.</p>
<pre class="r"><code>RequiredReplicates_for_all_fields &lt;-
  as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field)
  
  RequiredReplicates_for_all_fields$Group &lt;-
  factor(RequiredReplicates_for_all_fields$Group)
  
  RequiredReplicates_for_all_fields$RequiredReplicates &lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates))
  
  RequiredReplicates_for_all_fields$EffectSize &lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize))
  
  # Plot RequiredReplicates_for_all_fields
  ggplot(RequiredReplicates_for_all_fields,
  aes(x = EffectSize, y = RequiredReplicates)) +
  ggtitle(&quot;Effect size Vs Required replicates for all fields data \n(Combined Effect)&quot;) +
  geom_point(aes(shape = Group), size = 2)  +
  scale_shape_manual(values = 1:nlevels(EffectSize.Plot$Group)) +
  geom_line() +
  scale_x_continuous(&quot;EffectSize&quot;,
  breaks = c(
  0,
  0.2,
  0.5,
  1,
  max(RequiredReplicates_for_all_fields$EffectSize)
  ),
  labels = scaleFUN) +
  theme_bw() +
  theme(
  axis.title = element_text(size = 14, face = &quot;bold&quot;),
  axis.text = element_text(size = 10),
  plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;)
  )</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Based on the combined effect size (from all four fields), we can see that the Effect size is inversely proportional to Required Replicates. We also see that for the small effect of ~0.2, we need exponentially larger replicates, for medium effect (~0.5), we need about 80 replicates and for larger effect (more than 0.8), we only need about 20 or fewer replicates. Our analysis from all fields also indicates that we have highest mean yield for controlRates of Seeding og 27000-29000.</p>
<p>Final discussion and suggestion:</p>
<p>Based on the results we saw above, if we take individual field (one at a time), we would need thousands of replicates for low effect size. However, for moderate and large effect size, the required replicates number signficantly decreases even if we use individual field data separately. On the other hand, when we combine all four fields, our required replictes decreases for all low, moderate and large effect sizes (Figure: Effect size Vs Required replicates for all fields data (Combined Effect)) as compared to individual fields (Figure: Effect size Vs Required replicates for each field data). I also found that the required replicates decreases for lower rates of seeding (control Rates of 23000 Vs 24000).
Based on these results, we can also conclude that the Required replicates is inversely proportional to the Effect size as seen in all four fields data and merged data.</p>
<p>Aditionally, the mean yield significantly increases when we increase the seeding rates at control rates of 26000 to 28000 for each individual field. This was also consistent for combined field data (Figure: ControlRates Vs Mean yield for all four fields). So, my suggestion for this analysis is that we should used all four fields to calculate required replicates and effect size, though we could still use single field data separately to derrive the conclusion that higher seeding rates (~26000-28000) has higher mean yield which was consistent in all four fields. However, making use of all four fields data is recommended.</p>
</div>
<div id="part-2" class="section level2">
<h2>Part 2</h2>
<p>We will continue working with this data from four corn fields, but will also be looking at some issues with times and dates.
For each field, I’ve two files, seed<em>.csv and harvest</em>.csv, corresponding to seeding rate and yield data, respectively. Both sets of files have a column Timestamp with strings of the form
2018-05-20T13:20:08.201Z
Our task will be to extract the date and time values from these strings, and answer these questions. The text before ’T" is the date string, and the text between ’T" and “Z” is the time string, in universal time. Specifically, we want to know:</p>
<ol style="list-style-type: decimal">
<li>What is the range of planting dates in these data?</li>
<li>Was each field harvested entirely in one day, and where they harvested each at approximately the same time of day?</li>
</ol>
<p>It might be enough to process only the first and last rows in the data. Each should be sampled at one second intervals, so the time difference between the first and last rows, in seconds, should be almost equal to the number of rows. I would not be too worried about gaps on the data on the order of seconds, but I would want to know about hour long gaps in the data, and where they occur. Note that time will reset to 0 at 23:59:59 and date will increment, so account for this in the analysis.
Some additional thoughts:
There is a relationship between planting date and yield. We could review the literature to get a more precise estimate (and you can do that, if you wish), but we’ll start with a rough back-of-the-envelope calculation.
Suppose, we are working with 100-day corn - we expect corn to reach maturity in roughly 100 days. Let’s suppose that the difference between the first field planting date and the last field planting date is 5 days. That’s 5 percent of the growing period, so let %Diff = 5. What is the standard deviation for Yield, with regard to planting date? Again, we can look to the literature, but we’ll use a simplifying assumption that it will be similar to the sd for yield vs planting rate, as determined in the <code>Part 1</code> project. Convert that to CV.
Now we have a first approximation for effect size (%Diff/CV). Is this effect size large enough that we need to worry that the effect of planting date will confound our analysis of the relationship between yield and planting rate? When I gather data from more fields, do I need to be careful and only include fields in a narrow range of dates? What is the first approximation for the number of fields required to test the relationship between planting date and yield?</p>
<p>Not, about gaps in harvest. I’ve been arguing that analyzing yield monitor data should be a two-step process. First, analysis the yield as a time series (as grain moves through the harvester) then analyze as spatial data (the as the harvester moves over the field). There will be auto-correlated errors in each process that should be analyzed independently.
In particular, the value for yield as reported in these data is not the yield of the grain as it is measured going through the harvester. The grain moving through the harvester will be of varying degrees of maturity, thus of grain moisture - less mature grain will have more water content, thus more weight. Yield values are standardized to a define percent moisture, so the harvester has a moisture sensor, and the percent moisture reading is use to normalize yield.
But yield is measured at one second intervals, while moisture can only be measured at approximately 10-15 second intervals. I’m curious if gaps in the harvest record will affect how yield is normalized by moisture - there may be some cases where the moisture reading is uncorrelated with yield, because of this difference in measurement (I suspect it will be very small, but it’s an interesting problem, to me).</p>
<p>I’m also interested in how percent moisture changes over the course of a day, and if there is an effect when fields are harvested at different times of day.
More will follow in the discussions.</p>
<p>The algorithm to do this analysis is divided into several steps as described below:</p>
<ol style="list-style-type: decimal">
<li>First you have to define the path to all csv files for <code>Part 2</code> under (<code>path.part2</code>) and all .csv files from <code>Part 1</code> under <code>path.part1</code> in the begining of the code below.</li>
<li>I have used some external packages like ggplot2 to make the plots.</li>
<li>I have written several functions to analyze each individual field as well as merged field statistics that calculates cohens effect size, required replicates, and does ANOVA analysis on individual field as well as on merged data from all fields.</li>
<li>This function <code>get_question_1_and_2_answers</code> calculates the time spent each day on field, start and end time start and end date. observations each day, and also calculate one hour gap (more than an hour gap in consecutive rows) if present in the data. It spits out a table (<code>range_table</code>) with all the date data required for this project. I then used this range table to plot time, and dates.</li>
<li>I have also plotted the Moisture content in each harvest field and how it gets affected during the day, and across the range of dates.</li>
<li>Function <code>calculate_field_mean_SD_and_get_RR_EffSize</code> also uses mean, sd, and counts from each field to calculate cohens d and Required replicates. It also uses pooled standard deviations and Mean (mean of Means) calculated from all four fields for different control Rates to calculate cohens d and required replicates of combined field data. I have tested this function on all four fields at ControlRate level of 500, 1000, 2000 and 3000 intervals for <code>Part 1</code>, but this time, I have modified this function to calculate cohends d and required replicates for 1000 control rates also taking dates into consideration. This function also calculates these values for date only (neglecting control Rates).</li>
<li>I have also shown ANOVA analysis followed by TUKEY HSD test to show which ControlRates of seeding have significant effect on Yield for each field and then for all fields (combined all four fields).</li>
<li>Lastly, I have plotted EffectSize Vs RequiredReplicates for all combined field.</li>
</ol>
<p>First of all, we will set two directory paths where we have all our files we want to anlalyze. first path is <code>path.part2</code> for <code>Part 2</code> data and <code>path.part1</code> for data in <code>Part 1</code></p>
<p><code>all.files</code> contains the file name of all csv files in our directory.</p>
<pre class="r"><code># # Here, I read all files from the system rather than reading one file at a time.
# # Path to all .csv data files for Part 2.
# path.final &lt;-
# &quot;/Users/owner1/statistics/&quot;
# 
# # Path to all .csv data files for midterm.
# path.midterm &lt;-
#   &quot;/Users/owner1/statistics/&quot;
# instead from local path, we can get files from git repos
library(rvest)</code></pre>
<pre><code>## Loading required package: xml2</code></pre>
<pre><code>## Warning: package &#39;xml2&#39; was built under R version 4.0.2</code></pre>
<pre class="r"><code>path.part2 &lt;- read_html(&quot;https://github.com//achalneupane/data&quot;) %&gt;% html_nodes(&quot;.js-navigation-open&quot;) %&gt;% html_attr(&quot;href&quot;)
path.part1 &lt;- read_html(&quot;https://github.com//achalneupane/data&quot;) %&gt;% html_nodes(&quot;.js-navigation-open&quot;) %&gt;% html_attr(&quot;href&quot;)</code></pre>
<p>Now, we first install and load some of the packages (“multcompView”, “ggplot2”,
“scales”, “data.table”, etc.) we will be using for this exercise.</p>
<pre class="r"><code># First, install missing packages and load them
myPackages &lt;-
c(
&quot;multcompView&quot;,
&quot;ggplot2&quot;,
&quot;scales&quot;,
&quot;data.table&quot;,
&quot;reshape2&quot;,
&quot;RColorBrewer&quot;,
&quot;plyr&quot;,
&quot;ggpmisc&quot;
)
my.installed.packages &lt;- installed.packages()
available.packages &lt;- myPackages %in% my.installed.packages
if (sum(!available.packages) &gt; 0) {
install.packages(myPackages[!available.packages])
}
# Load all required packages
lapply(myPackages, require, character.only = TRUE)</code></pre>
<pre><code>## Loading required package: reshape2</code></pre>
<pre><code>## Warning: package &#39;reshape2&#39; was built under R version 4.0.2</code></pre>
<pre><code>## 
## Attaching package: &#39;reshape2&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:data.table&#39;:
## 
##     dcast, melt</code></pre>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<pre><code>## Loading required package: plyr</code></pre>
<pre><code>## Warning: package &#39;plyr&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Loading required package: ggpmisc</code></pre>
<pre><code>## Warning: package &#39;ggpmisc&#39; was built under R version 4.0.5</code></pre>
<pre><code>## 
## Attaching package: &#39;ggpmisc&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     annotate</code></pre>
<pre><code>## [[1]]
## [1] TRUE
## 
## [[2]]
## [1] TRUE
## 
## [[3]]
## [1] TRUE
## 
## [[4]]
## [1] TRUE
## 
## [[5]]
## [1] TRUE
## 
## [[6]]
## [1] TRUE
## 
## [[7]]
## [1] TRUE
## 
## [[8]]
## [1] TRUE</code></pre>
<p>I have some additional functions for this project written here.</p>
<pre class="r"><code># function for question 1 and 2
get_question_1_and_2_answers &lt;- function(field_or_seed) {
# range of planting dates
field_or_seed$Date &lt;- gsub(&quot;T.*&quot;, &quot;&quot;, field_or_seed$Timestamp)
field_or_seed$Time &lt;- gsub(&quot;.*T|Z&quot;, &quot;&quot;, field_or_seed$Timestamp)
# Range in days
field_or_seed$date_time &lt;-
as.POSIXct(paste(field_or_seed$Date, field_or_seed$Time), tz = &quot;UTC&quot;)
# field_or_seed$date_time &lt;- paste(field_or_seed$Date, field_or_seed$Time)
# num_days &lt;- round(difftime(max(as.POSIXct(field_or_seed$date_time)), min(as.POSIXct(field_or_seed$date_time))), 3)
num_days &lt;-
round(difftime(max(as.POSIXct(
field_or_seed$date_time
)), min(as.POSIXct(
field_or_seed$date_time
)), units = &quot;days&quot;), 3)
range_from_start_to_end_date_in_days &lt;-
ifelse(num_days &lt;= 1,
paste0(num_days, &quot; day&quot;),
paste0(num_days, &quot; days&quot;))
# Range in span of date for the field
range_of_start_end_date &lt;-
paste0(range(as.Date(field_or_seed$Date))[1], &quot; to &quot;, range(as.Date(field_or_seed$Date))[2])
field_or_seed$Date &lt;- as.factor(field_or_seed$Date)
# time range each day
tt &lt;- field_or_seed[, c(&quot;Date&quot;, &quot;Time&quot;)]
time.range.each.day &lt;- aggregate(data.frame(Time = strptime(do.call(paste, tt), &#39;%F %R:%OS&#39;, tz = &#39;UTC&#39;)),
by = list(Date = tt$Date),
function(Time) {
paste(format(min(Time), &#39;%T&#39;),
format(max(Time), &#39;%T&#39;),
sep = &#39; to &#39;)
})
rm(tt)
# Function to find more than an hour gap in consecutive rows
about_an_hour_gap &lt;- function(field_or_seed) {
one_hour_thing &lt;- {
}
for (i in 1:nrow(field_or_seed)) {
# i=3
if (i == nrow(field_or_seed))
{
break
} else
time.lapse &lt;-
abs(as.numeric(
difftime(field_or_seed$date_time[i + 1], field_or_seed$date_time[i]),
units = &quot;mins&quot;
))
# find consecutive rows where the time is more than or equal 60 minutes and
# dates are the same
if (time.lapse &gt;= 60 &amp;
field_or_seed$Date[i + 1] == field_or_seed$Date[i]) {
one_hour_thing.tmp &lt;- field_or_seed[i:(i + 1), ]
one_hour_thing &lt;- rbind(one_hour_thing, one_hour_thing.tmp)
}
}
return(one_hour_thing)
}
about_an_hour_gap(field_or_seed)
one_hour_gap_time_stamp &lt;- about_an_hour_gap(field_or_seed)
# function to calculate total time spent each day
time.elapsed.each.day.function &lt;- function(x) {
difftime(max(as.POSIXct(x)), min(as.POSIXct(x)), units = &quot;mins&quot;)
}
# time.range.each.day(tt)
time.elapsed.each.day &lt;-
aggregate(date_time ~ Date, data = field_or_seed, FUN = time.elapsed.each.day.function)
colnames(time.elapsed.each.day) &lt;-
c(&quot;Date&quot;, &quot;Time_elapsed (minutes)&quot;)
observations.each.day &lt;- table(field_or_seed$Date)
# Elapsed Dates
dates.elapsed.each.field.function &lt;- function(x){
  difftime(max(as.POSIXct(x)), min(as.POSIXct(x)), units = &quot;days&quot;)
}
elapsed.dates.each.field &lt;- as.integer(dates.elapsed.each.field.function(field_or_seed$Date))
return(
list(
observations.each.day = observations.each.day,
range_from_start_to_end_date_in_days = range_from_start_to_end_date_in_days,
range_of_start_end_date = range_of_start_end_date,
time.elapsed.each.day = time.elapsed.each.day,
elapsed.dates.each.field = elapsed.dates.each.field, 
time.range.each.day = time.range.each.day,
one_hour_gap_time_stamp = one_hour_gap_time_stamp
)
)
}
# Functions to calculate cohen&#39;s d and Required Replicates
cohen.d &lt;- function(m1, s1, m2, s2) {
cohens_d &lt;- (abs(m1 - m2) / sqrt((s1 ^ 2 + s2 ^ 2) / 2))
return(cohens_d)
}
required.replicates &lt;-
function (m1,
s1,
m2,
s2,
alpha = 0.05,
beta = 0.2) {
n &lt;-
2 * ((((sqrt((s1 ^ 2 + s2 ^ 2) / 2
)) / (m1 - m2)) ^ 2) * (qnorm((1 - alpha / 2)) + qnorm((1 - beta))) ^ 2)
return(round(n, 0))
}
# Additional functions: They do anova and also calculate required replicates for fields
# We willl also perform ANOVA analysis with Tukey Test for paired comparision of
# mean for each field data as well as merged data at different ControlRate
# intervals. This function does Tukey HSD test and generates label for
# significant outcomes.
# Create function to get the labels for Tukey HSD:
generate_label_df &lt;- function(TUKEY, variable) {
# Extract labels and factor levels from Tukey post-hoc
Tukey.levels &lt;- TUKEY[[variable]][, 4]
Tukey.labels &lt;-
data.frame(multcompLetters(Tukey.levels)[&#39;Letters&#39;])
#I need to put the labels in the same order as in the boxplot :
Tukey.labels$treatment = rownames(Tukey.labels)
Tukey.labels = Tukey.labels[order(Tukey.labels$treatment) ,]
return(Tukey.labels)
}
# This function does ANOVA and makes boxplots with Tukey statistics for
# comparing Mean yield.
get_my_box_plot &lt;- function (field, plot_name = &quot;the Field&quot;) {
field$CR.Date.Levels &lt;- gsub(&quot;-&quot;, &quot;_&quot;, field$CR.Date.Levels)
field$CR.Date.Levels &lt;- as.factor(field$CR.Date.Levels)
model = lm(field$Yield ~ field$CR.Date.Levels)
ANOVA = aov(model)
# If residual degrees of freedom is less than or equal 1, don&#39;t do Tukey test;
# simply return ANOVA summary.
if (ANOVA$df.residual &lt;= 1) {
return(summary.aov(ANOVA))
}
# Tukey test to study each pair of treatment :
TUKEY &lt;-
TukeyHSD(x = ANOVA,
&#39;field$CR.Date.Levels&#39;,
conf.level = 0.95)
# generate labels using function
labels &lt;- generate_label_df(TUKEY , &quot;field$CR.Date.Levels&quot;)
# rename columns for merging
names(labels) &lt;- c(&#39;Letters&#39;, &#39;CR.Date.Levels&#39;)
# Obtain letter positions for y axis using means
yvalue &lt;- aggregate(Yield ~ CR.Date.Levels, data = field, mean)
part2 &lt;- merge(labels, yvalue) #merge dataframes
p &lt;- ggplot(field, aes(x = CR.Date.Levels, y = Yield)) +
geom_blank() +
theme_bw() +
# theme(panel.grid.major = element_blank(),
# panel.grid.minor = element_blank()) +
labs(x = &#39;CR.Date.Levels&#39;, y = &#39;Mean Yield&#39;) +
ggtitle(paste0(&quot;CR.Date.Levels Vs Mean yield for &quot;, plot_name),
expression(atop(italic(
&quot;(Anova:TukeyHSD)&quot;
), &quot;&quot;))) +
# ggtitle(paste0(&quot;CR.Date.Levels Vs Mean yield for &quot;, plot_name)) +
theme(plot.title = element_text(hjust = 0.5, face = &#39;bold&#39;)) +
geom_boxplot(fill = &#39;grey&#39;, stat = &quot;boxplot&quot;) +
# coord_cartesian(clip = &#39;off&#39;) +
geom_text(
data = part2,
aes(x = CR.Date.Levels, y = Yield, label = Letters),
vjust = -3.5,
hjust = -.5
) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
# geom_vline(aes(xintercept = 4.5), linetype = &quot;dashed&quot;) +
theme(plot.title = element_text(vjust = -0.6)) +
coord_cartesian(ylim = c(min(field$Yield), max(field$Yield) + 10))
print(p)
}</code></pre>
<p>Function to calculate mean and sd for each field for a given interval of
ControlRate and then calculate Required Replicates and EffectSize. I have made
one function to do all that so I can just use this function to analyze all
four fields. To analyze single field set <code>Single.Field.Analysis = TRUE</code>, and
to analyze all four field together, set <code>Single.Field.Analysis = FALSE</code>,</p>
<pre class="r"><code>calculate_field_mean_SD_and_get_RR_EffSize &lt;- 
  function(
    field, intervals = 1000, Single.Field.Analysis = TRUE, Date.Only = FALSE, printplot = FALSE){
    options(warn=1)
    plot_NAME &lt;- (deparse(substitute(field)))
    if(Single.Field.Analysis &gt; 0){
      # removing rows with NAs in Yield
      sum(is.na(field$Yield))
      # field &lt;- field[!complete.cases(field$Yield), ]
      field &lt;- field[!is.na(field$Yield),]
      # as.factor(fieldA$ControlRate)
      
      
      # work on timestamp columns
      field$Date &lt;- gsub(&quot;T.*&quot;, &quot;&quot;, field$Timestamp)
      field$Time &lt;- gsub(&quot;.*T|Z&quot;, &quot;&quot;, field$Timestamp)
      # Range in days
      field$date_time &lt;- as.POSIXct(paste(field$Date, field$Time), tz = &quot;UTC&quot;)
      
      
      # table.field &lt;- table(as.factor(field$Date))
      field$ControlRate.Levels &lt;- as.factor(intervals * ceiling(field$ControlRate/intervals))
      if(Date.Only &lt; 1){
      field$CR.Date.Levels &lt;- as.factor(paste(field$ControlRate.Levels, field$Date, sep = &quot;*&quot;))
      } else {
        field$CR.Date.Levels &lt;- as.factor(field$Date)
      }
      table.field &lt;- table(as.factor(field$CR.Date.Levels))
      
      
      
      
      field.Count &lt;- setNames(aggregate(field$CR.Date.Levels, 
                                        by = list(field$CR.Date.Levels), FUN = length), c(&quot;CR.Date.Levels&quot;, &quot;Count&quot;))
      field.mean &lt;-
        setNames(aggregate(
          field$Yield,
          by = list(field$CR.Date.Levels),
          FUN = mean
        ),
        c(&quot;CR.Date.Levels&quot;, &quot;Mean&quot;))
      
      
      field.SD &lt;-
        setNames(aggregate(
          field$Yield,
          by = list(field$CR.Date.Levels),
          FUN = sd
        ),
        c(&quot;CR.Date.Levels&quot;, &quot;SD&quot;))
      
      
      if(printplot == 1 ){
        get_my_box_plot(field, plot_name = plot_NAME)
      }
      
      # plot individual fields with tukey test We will print box plot only if we
      # want for certain ControlRates intervals. Otherwise we will have too many
      # plots
    } else {
      temp.Field &lt;- field
      colnames(temp.Field)[colnames(temp.Field) == &quot;CR.Date.Levels&quot;] &lt;- &quot;ControlRate.Levels&quot;
      colnames(temp.Field)[colnames(temp.Field) == &quot;Mean&quot;] &lt;- &quot;Yield&quot;
      # get_my_box_plot(temp.Field)
      field.SD  &lt;- 
        as.data.frame(cbind(ControlRate = field[&quot;CR.Date.Levels&quot;], SD = field[&quot;SD_pooled&quot;]))
      field.mean &lt;- 
        as.data.frame(cbind(ControlRate = field[&quot;CR.Date.Levels&quot;], Mean = field[&quot;Mean&quot;]))
    }
    
    # Calculate Required replicate and Effect Size from each
    # field for ControlRate i vs i+1
    
    
    Req.Rep.table.field &lt;- {}
    for (i in 1:nrow(field.SD)){
      if(i+1 &gt; nrow(field.SD) ){
        break
      }
      temp.Effect.size &lt;-
        cohen.d(
          m1 = field.mean$Mean[i],
          s1 = field.SD$SD[i],
          m2 = field.mean$Mean[i + 1],
          s2 = field.SD$SD[i + 1]
        )
      
      tmp.req.reps &lt;-
        required.replicates(
          m1 = field.mean$Mean[i],
          s1 = field.SD$SD[i],
          m2 = field.mean$Mean[i + 1],
          s2 = field.SD$SD[i + 1]
        )
      
      tmp.table &lt;-
        cbind(
          Group = paste0(field.SD$CR.Date.Levels[i], &quot; Vs &quot;, field.SD$CR.Date.Levels[i + 1]),
          EffectSize = temp.Effect.size,
          RequiredReplicates = tmp.req.reps
        )
      
      Req.Rep.table.field &lt;- rbind(Req.Rep.table.field, tmp.table)
    }
    if (Single.Field.Analysis &gt; 0) {
      return(
        list(
          field.mean = field.mean,
          fieldSD = field.SD,
          field.Count = field.Count,
          Req.Rep.table.field = Req.Rep.table.field
        )
      )
    } else{
      return(list(Req.Rep.table.field = Req.Rep.table.field))
    }
  }</code></pre>
<p>Here, I am reading all files in my directory we set in the begining
(path.part2). I was being a bit lazy to copy hard coded paths. I then read those
files in for loop and assigned them to their respective variable names [eg,
gsub(“.csv”,"",filename.csv)].</p>
<pre class="r"><code># all.files contains the file name of all csv files in our directory. I have decided to
# read all files from the system rather than reading one file at a time.
# using from github
all.files &lt;- path.part2
all.files &lt;- all.files[grepl(&quot;*.csv&quot;, all.files)]
# # selecting only required files
all.files &lt;- all.files[grepl(&quot;seedA.csv|seedB.csv|seedC.csv|seedD.csv|harvestA.csv|harvestB.csv|harvestC.csv|harvestD.csv&quot;, all.files)]
# # Or choose specifically which data files in path to analyze
# all.files &lt;- all.files &lt;- c(&quot;seedA.csv&quot;, &quot;seedB.csv&quot;, &quot;seedC.csv&quot;, &quot;seedD.csv&quot;)
# Rearranging the vectors, putting seed data first and then harvest data
all.files &lt;-
c(all.files[grepl(&quot;seed&quot;, all.files)], all.files[grepl(&quot;harvest&quot;, all.files)])
# all.files
# [1] &quot;harvestA.csv&quot; &quot;harvestB.csv&quot; &quot;harvestC.csv&quot; &quot;harvestD.csv&quot; &quot;seedA.csv&quot;    &quot;seedB.csv&quot;    &quot;seedC.csv&quot;    &quot;seedD.csv&quot;
#https://raw.githubusercontent.com/achalneupane/data/master/
# reading all csv files within the path and saving them to their respective names.
for (i in 1:length(all.files)) {
assign(
gsub(&quot;.*master/|.csv&quot;, &quot;&quot;, all.files)[i],
read.table(gsub(&quot;blob/&quot;, &quot;&quot;,paste0(&quot;https://raw.githubusercontent.com&quot;,all.files[i])),
header = TRUE,
sep = &quot;,&quot;)
)
print(paste0(&quot;Read file &quot;, all.files[i], &#39; !&#39;))
}</code></pre>
<pre><code>## [1] &quot;Read file /achalneupane/data/blob/master/seedA.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/seedB.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/seedC.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/seedD.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/harvestA.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/harvestB.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/harvestC.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/harvestD.csv !&quot;</code></pre>
<pre class="r"><code># head(harvestA)
# head(harvestB) # and so on..</code></pre>
<p>Here, I am using <code>get_question_1_and_2_answers</code> to extract date information by
looping over all data files we just read above.</p>
<pre class="r"><code># make a table of date range
range_table &lt;- {}
one.hour.time.difference &lt;- data.frame()
for (i in 1:length(all.files)) {
# file.dat &lt;- eval(sub(&quot;.csv&quot;, &quot;&quot;, all.files, fixed = TRUE)[i])
file.dat &lt;- eval(gsub(&quot;.*master/|.csv&quot;, &quot;&quot;, all.files)[i])
# print(paste0(&quot;Doing time thing for &quot;, file.dat, &quot;...&quot;))
dat &lt;- get_question_1_and_2_answers(eval(parse(text = file.dat)))
tmp.range.of.days &lt;-
cbind(
data = file.dat,
range_from_start_to_end_date_in_days = dat$range_from_start_to_end_date_in_days,
elapsed.dates.each.field = dat$elapsed.dates.each.field,
range_of_start_end_date = dat$range_of_start_end_date,
time.range.each.day = dat$time.range.each.day,
time.elapsed.each.day = dat$time.elapsed.each.day
)
range_table &lt;- rbind(range_table, tmp.range.of.days)
tmp.one.hour.time.difference &lt;- dat$one_hour_gap_time_stamp
tmp.one.hour.time.difference$field &lt;- file.dat
if (dim(as.data.frame(tmp.one.hour.time.difference))[1] &gt;= 2) {
one.hour.time.difference &lt;-
rbind.fill(one.hour.time.difference, tmp.one.hour.time.difference)
} else{
one.hour.time.difference &lt;- one.hour.time.difference
}
}
range_table &lt;- as.data.frame(range_table)
range_table$days_number &lt;-
as.numeric(gsub(
&#39; days|day&#39;,
&quot;&quot;,
range_table$range_from_start_to_end_date_in_days
))
# Range table
range_table</code></pre>
<pre><code>##        data range_from_start_to_end_date_in_days elapsed.dates.each.field
## 1     seedA                            0.117 day                        0
## 2     seedB                            0.352 day                        1
## 3     seedB                            0.352 day                        1
## 4     seedC                             0.17 day                        1
## 5     seedC                             0.17 day                        1
## 6     seedD                             0.22 day                        0
## 7  harvestA                          18.824 days                       19
## 8  harvestA                          18.824 days                       19
## 9  harvestA                          18.824 days                       19
## 10 harvestB                            0.363 day                        0
## 11 harvestC                           1.085 days                        1
## 12 harvestC                           1.085 days                        1
## 13 harvestD                          43.214 days                       43
## 14 harvestD                          43.214 days                       43
## 15 harvestD                          43.214 days                       43
##     range_of_start_end_date time.range.each.day.Date time.range.each.day.Time
## 1  2018-05-17 to 2018-05-17               2018-05-17     15:24:19 to 18:13:06
## 2  2018-05-20 to 2018-05-21               2018-05-20     18:50:11 to 21:53:11
## 3  2018-05-20 to 2018-05-21               2018-05-21     02:41:49 to 03:16:59
## 4  2018-05-18 to 2018-05-19               2018-05-18     22:24:15 to 23:59:59
## 5  2018-05-18 to 2018-05-19               2018-05-19     00:00:00 to 02:29:27
## 6  2018-05-20 to 2018-05-20               2018-05-20     13:20:08 to 18:36:20
## 7  2018-10-27 to 2018-11-15               2018-10-27     21:07:30 to 23:06:33
## 8  2018-10-27 to 2018-11-15               2018-10-28     16:22:46 to 22:13:25
## 9  2018-10-27 to 2018-11-15               2018-11-15     16:31:55 to 16:53:40
## 10 2018-11-02 to 2018-11-02               2018-11-02     15:01:21 to 23:43:43
## 11 2018-11-11 to 2018-11-12               2018-11-11     15:34:34 to 23:59:59
## 12 2018-11-11 to 2018-11-12               2018-11-12     00:00:00 to 17:37:01
## 13 2018-10-01 to 2018-11-13               2018-10-01     17:04:25 to 17:32:04
## 14 2018-10-01 to 2018-11-13               2018-11-12     17:42:25 to 23:55:58
## 15 2018-10-01 to 2018-11-13               2018-11-13     15:10:12 to 22:11:56
##    time.elapsed.each.day.Date time.elapsed.each.day.Time_elapsed (minutes)
## 1                  2018-05-17                               168.78665 mins
## 2                  2018-05-20                               183.00582 mins
## 3                  2018-05-21                                35.17333 mins
## 4                  2018-05-18                                95.73995 mins
## 5                  2018-05-19                               149.45087 mins
## 6                  2018-05-20                               316.21000 mins
## 7                  2018-10-27                               119.06642 mins
## 8                  2018-10-28                               350.63592 mins
## 9                  2018-11-15                                21.75020 mins
## 10                 2018-11-02                               522.35150 mins
## 11                 2018-11-11                               505.40003 mins
## 12                 2018-11-12                              1057.01712 mins
## 13                 2018-10-01                                27.65013 mins
## 14                 2018-11-12                               373.55237 mins
## 15                 2018-11-13                               421.71987 mins
##    days_number
## 1        0.117
## 2        0.352
## 3        0.352
## 4        0.170
## 5        0.170
## 6        0.220
## 7       18.824
## 8       18.824
## 9       18.824
## 10       0.363
## 11       1.085
## 12       1.085
## 13      43.214
## 14      43.214
## 15      43.214</code></pre>
<p>This is the range table for all fields. I have calculated the range for total
elapsed time (in terms of days) in <code>range_from_start_to_end_date_in_days</code>
column, total dates elapsed in <code>elapsed.dates.each.field</code> column, Range in dates
in <code>range_of_start_end_date</code> column, time range each day (from start to end
time) in <code>time.range.each.day.Time</code> column, total time spent in the field each
date in <code>time.elapsed.each.day.Time_elapsed (minutes)</code> column.</p>
<pre class="r"><code># This is the list of consecutive rows with more than one hour time gap from all files:
one.hour.time.difference</code></pre>
<pre><code>##       X                Timestamp      LonM      LatM Moisture DISTANCE
## 1 27840 2018-11-12T01:51:27.071Z 208.29634 396.43056    15.33 1.075304
## 2 27849 2018-11-12T16:42:23.002Z  82.56309  62.87237    15.60 5.872164
##   VRYIELDVOL       Date         Time           date_time    field
## 1   133.3961 2018-11-12 01:51:27.071 2018-11-12 01:51:27 harvestC
## 2   119.6537 2018-11-12 16:42:23.002 2018-11-12 16:42:23 harvestC</code></pre>
<p>I was able to find more than an hour gap in consecutive rows in <code>harvestC</code> data
only</p>
<pre class="r"><code># Plotting the total number of days for each field type
ggplot(range_table, aes(x = data, y = days_number)) + geom_point(size = 4) +
  ggtitle(&quot;Plot in terms of elapsed time- \n start to end (in days)&quot;) +
theme_bw() +
theme(
axis.line = element_line(colour = &quot;black&quot;),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
theme(text = element_text(size = 14)) +
scale_y_continuous(breaks = round(c(
1,
max(range_table$days_number) * 1 / 3,
max(range_table$days_number) * 2 / 3,
max(range_table$days_number)
), 2)) +
# to check if any of the data were planted/harvested within one day range
geom_hline(yintercept = 1) +
geom_text((aes(3, 1, label = &quot;Within 1 day (24 hours margin)&quot;, vjust = -1)))</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Based on this plot, we can see that SeedA, seedB, seedC and seedD were all
planted within 24 hours span. We can also check the <code>range_table</code> above to get
the exact timeframe for this. However, if want to see if they were planted or
harvested on the same date, then we can plot this figure below:</p>
<pre class="r"><code># If we want to see whether they were planted or harvested the same day (i.e
# Date) and (not necessarily 24 hours margin), we choose range_table$elapsed.dates.each.field
# Now plot them
ggplot(range_table, aes(x = data, y = elapsed.dates.each.field)) +
  geom_point(size = 2) +
  theme_bw() +
  ggtitle(&quot;Plot in terms of elapsed dates&quot;) +
  theme(axis.line = element_line(colour = &quot;black&quot;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  theme(text = element_text(size=14)) +
# to check if any of the data were planted/harvested the same &quot;Date&quot;
geom_hline(yintercept = 1) +
  geom_hline(yintercept = 0, color=&quot;blue&quot;, linetype=&quot;dashed&quot;) +
geom_text((aes(3, 1, label = &quot;Margin outside the \&quot;same\&quot; date&quot;, vjust = -1)))</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>This plot above shows that seedA and seedD were planted on the same date, and
harvestB was harvested also on the same date. Rest of the field data were
harvested/planted for multiple dates. Bubbles within the black solid line
indicates that the fields were, harvested/planted the same date. Only seedA,
seedD and harvestB are within the solid line. Here, Y axis margin of 1 means
next date. Data within the black and dashed blue lines indicate the same date.</p>
<p>Additionally, if we want to see how many hours were spent on each field in
total, we can plot this below:</p>
<pre class="r"><code>plot.new &lt;- range_table[, c(&quot;data&quot;, &quot;time.elapsed.each.day.Time_elapsed (minutes)&quot;, &quot;time.elapsed.each.day.Date&quot;)]
plot.new$time.elapsed.each.day.Time_elapsed &lt;- as.numeric(plot.new$`time.elapsed.each.day.Time_elapsed (minutes)`)
plot.new$time.elapsed.each.day.Time_elapsed.in.hour &lt;- plot.new$time.elapsed.each.day.Time_elapsed/60
plot.new.data &lt;- setNames(aggregate(plot.new$time.elapsed.each.day.Time_elapsed.in.hour, list(range_table$data), sum), c(&quot;group&quot;, &quot;time.spent.in.hours&quot;))
ggplot(plot.new.data, aes(x = group, y = time.spent.in.hours)) +
  geom_point(size = 4) +
  theme_bw() +
  ggtitle(&quot;Plot in terms of total time spent in hours&quot;) +
  theme(axis.line = element_line(colour = &quot;black&quot;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  theme(text = element_text(size=14)) </code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Based on this above plot, it looks like only harvestC and harvestD had more than
10 hours spent on them.</p>
<p>Now, we can also plot start and end time for everyday record:</p>
<pre class="r"><code>range_table$start_date_time &lt;-
as.POSIXct(paste(
range_table$time.range.each.day.Date ,
gsub(&quot;to.*| &quot;, &quot;&quot;, range_table$time.range.each.day.Time)
), tz = &#39;UTC&#39;)
range_table$end_date_time &lt;-
as.POSIXct(paste(
range_table$time.range.each.day.Date ,
gsub(&quot;.*to| &quot;, &quot;&quot;, range_table$time.range.each.day.Time)
), tz = &#39;UTC&#39;)
tt &lt;- range_table[, c(&quot;data&quot;, &quot;start_date_time&quot;, &quot;end_date_time&quot;)]
tt &lt;- melt(tt, id = &quot;data&quot;)
# Plot for date and time
ggplot(tt, aes(
x = data,
y = as.character.Date(value),
colour = variable
)) +
geom_point(size = 4) +
ylab(&quot;Dates and Time&quot;) +
theme_bw() +
ggtitle(&quot;Start and end time for everyday record&quot;) +
theme(
axis.line = element_line(colour = &quot;black&quot;),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 14)) +
theme(axis.text.y = element_text(angle = 0, hjust = 1, size = 8)) +
# theme(text = element_text(size = 14)) +
geom_hline(yintercept = 1:length(tt$value), linetype = &quot;dotted&quot;) +
labs(color = &#39;Each Day&#39;) +
scale_color_manual(labels = c(&quot;START_time&quot;, &quot;END_time&quot;),
values = c(&quot;blue&quot;, &quot;red&quot;)) </code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-36-1.png" width="672" />
This plot also shows similar result as with previous plot, but with both start
and end time.</p>
<p>We can also check the moisture level differences in the fields by date and
times.</p>
<pre class="r"><code># Now, let&#39;s work on the effect of time/dates on moiture level
tt1 &lt;- harvestA
tt2 &lt;- harvestB
tt3 &lt;- harvestC
tt4 &lt;- harvestD
tt1$type &lt;- &quot;harvestA&quot;
tt2$type &lt;- &quot;harvestB&quot;
tt3$type &lt;- &quot;harvestC&quot;
tt4$type &lt;- &quot;harvestD&quot;
make_posixct &lt;- function(x) {
  x$Date &lt;- gsub(&quot;T.*&quot;, &quot;&quot;, x$Timestamp)
  x$Date &lt;- as.factor(x$Date)
  x$Time &lt;- gsub(&quot;.*T|Z&quot;, &quot;&quot;, x$Timestamp)
  n &lt;- 2
  pat &lt;- paste0(&#39;^([^:]+(?::[^:]+){&#39;, n - 1, &#39;}).*&#39;)
  x$Time  &lt;- as.factor(sub(pat, &#39;\\1&#39;, x$Time))
  return(x)
}
tt1 &lt;- make_posixct(tt1)
tt2 &lt;- make_posixct(tt2)
tt3 &lt;- make_posixct(tt3)
tt4 &lt;- make_posixct(tt4)
colnames(tt2) &lt;- colnames(tt1)
df &lt;- rbind(tt1, tt2, tt3, tt4)
df$Time &lt;-
  as.POSIXct(strptime(df$Time, format = &quot;%H:%M&quot;, tz = &quot;UTC&quot;))
lims &lt;-
  as.POSIXct(strptime(c(&quot;0:00&quot;, &quot;23:59&quot;), format = &quot;%H:%M&quot;, tz = &quot;UTC&quot;))
p &lt;-
  ggplot(data = df, aes(
    x = Time,
    y = Moisture,
    colour = as.factor(Date)
  )) +
  geom_point() +
  scale_x_datetime(
    limits = lims,
    breaks = date_breaks(&quot;2 hour&quot;),
    labels = date_format(&quot;%H:%M&quot;, tz = &quot;UTC&quot;)
  ) +
  labs(color = &#39;Dates&#39;) +
  facet_grid(. ~ type)
p + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-37-1.png" width="672" />
Based on this figure above, we can see that the moisture content is very low
from midnight to 2:00pm, but then after, it increases substantially. I don’t
think there is so much difference in moisture in terms of harvest dates though.</p>
<p>I tried to pull weather data for each observation, however the api I used only
allows 1000 requests per day, so I did not include that data.</p>
<pre class="r"><code># We can use darksky like api&#39;s to pull weather data, however it&#39;s not
# possible as we have so many observations. Only allows 1000 downloads per day.
# install.packages(&quot;darksky&quot;)
library(darksky) darksky_api_key(force = TRUE)
#API: 60b8ca6aaece606b21c0dfec1cb7ec9f
head(seedA)
# field_or_seed &lt;- seedA 
tmp &lt;- get_forecast_for(10.25209, 478.4000, &quot;2013-05-06T12:00:00&quot;)
# Create empty columns and cbind to the field data so you can add new weather
data field_or_seed &lt;- cbind(field_or_seed, setNames(lapply(get.weather.cols,
function(x) x = NA), get.weather.cols)) 
field_or_seed$Date &lt;- gsub(&quot;T.*&quot;, &quot;&quot;, field_or_seed$Timestamp)
field_or_seed$Time &lt;- gsub(&quot;.*T|Z&quot;, &quot;&quot;, field_or_seed$Timestamp) # Range in days field_or_seed$date_time &lt;-
as.POSIXct(paste(field_or_seed$Date, field_or_seed$Time), tz = &quot;UTC&quot;)
field_or_seed_with_weather &lt;- {}
for (i in 1:nrow(field_or_seed)) {
print(paste0(&quot;My iteration is: &quot;, i)) tmp.min &lt;-
get_forecast_for(44.311356, -96.798386, min(field_or_seed$date_time[i]))
tmp.weather.data &lt;- cbind(field_or_seed[i,], tmp.min$daily)
field_or_seed_with_weather &lt;- rbind(field_or_seed_with_weather,
tmp.weather.data)
}</code></pre>
<p>Additionally, let’s read our csv files from <code>Part 1</code></p>
<pre class="r"><code># # If using local path
#   setwd(path.part1)
#   all.files &lt;- list.files(path.part1)
  # here, instead, I am just using the URL
  all.files &lt;- path.part1
  all.files &lt;- all.files[grepl(&quot;*.csv&quot;, all.files)]
  # &gt; all.files
  # [1] &quot;fieldA.csv&quot; &quot;fieldB.csv&quot; &quot;fieldC.csv&quot; &quot;fieldD.csv&quot;
  
all.files &lt;- all.files[grepl(&quot;fieldA.csv|fieldB.csv|fieldC.csv|fieldD.csv&quot;, all.files)]  
  
  
# reading all csv files within the path and saving them with their respective
# names
# https://raw.githubusercontent.com/achalneupane/data/master/
# reading all csv files within the path and saving them to their respective names.
for (i in 1:length(all.files)) {
assign(
gsub(&quot;.*master/|.csv&quot;, &quot;&quot;, all.files)[i],
read.table(gsub(&quot;blob/&quot;, &quot;&quot;,paste0(&quot;https://raw.githubusercontent.com&quot;,all.files[i])),
header = TRUE,
sep = &quot;,&quot;)
)
print(paste0(&quot;Read file &quot;, all.files[i], &#39; !&#39;))
}</code></pre>
<pre><code>## [1] &quot;Read file /achalneupane/data/blob/master/fieldA.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/fieldB.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/fieldC.csv !&quot;
## [1] &quot;Read file /achalneupane/data/blob/master/fieldD.csv !&quot;</code></pre>
<pre class="r"><code>  # Now, let&#39;s merge field and seed data
  
  # head(fieldA)
  # head(seedA)
  
  dataA &lt;-
  merge(
  fieldA,
  seedA,
  by.x = c(&quot;Easting&quot;, &quot;Northing&quot;),
  by.y = c(&quot;LonM&quot;, &quot;LatM&quot;)
  )
  colnames(dataA) &lt;- gsub(&quot;.x&quot;, &quot;&quot;, colnames(dataA))
  dataB &lt;-
  merge(
  fieldB,
  seedB,
  by.x = c(&quot;Easting&quot;, &quot;Northing&quot;),
  by.y = c(&quot;LonM&quot;, &quot;LatM&quot;)
  )
  colnames(dataB) &lt;- gsub(&quot;.x&quot;, &quot;&quot;, colnames(dataB))
  dataC &lt;-
  merge(
  fieldC,
  seedC,
  by.x = c(&quot;Easting&quot;, &quot;Northing&quot;),
  by.y = c(&quot;LonM&quot;, &quot;LatM&quot;)
  )
  colnames(dataC) &lt;- gsub(&quot;.x&quot;, &quot;&quot;, colnames(dataC))
  dataD &lt;-
  merge(
  fieldD,
  seedD,
  by.x = c(&quot;Easting&quot;, &quot;Northing&quot;),
  by.y = c(&quot;LonM&quot;, &quot;LatM&quot;)
  )
  colnames(dataD) &lt;- gsub(&quot;.x&quot;, &quot;&quot;, colnames(dataD))
  
  # Taking 1000 ControlRates interval, we will also compare different dates (i.e,
  # ControlRate and date pairs) for each merged seed and field data.
  
  ControlRateInterval &lt;- 1000
  # field A
  fieldA.data &lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataA, intervals = ControlRateInterval)
  # fieldA.data
  
  
  # field B
  fieldB.data &lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataB, intervals = ControlRateInterval)
  
  # fieldB.data
  
  # From Part 1 for fieldB, we saw significant difference between all Control Rates of
  # Seeding except between 28000 Vs 29000. The seeding rates of 28000 has the
  # highest Mean yield here.
  
  # field C
  fieldC.data &lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataC, intervals = ControlRateInterval)
  # fieldC.data
  
  # From Part 1 for fieldC, we saw all Control rates of seeding are significantly diffferent with the highest Mean yield for 28000.
  
  # field D
  fieldD.data &lt;-
  calculate_field_mean_SD_and_get_RR_EffSize(field = dataD, intervals = ControlRateInterval)
 # fieldD.data</code></pre>
<p>Based on my analysis from <code>Part 1</code>, we saw that it is better to compare all
four fields together. Also, I do not want to clutter this report with too many
figures and tables, I am analyzing 1000 Control Rate and all four fields
merged together. This way we can compare the skewed data points if we analyze
with date and ControlRate pairs. I have done this analysis below</p>
<p>We can now merge all four fields for SD, means and ControlRate level counts.</p>
<pre class="r"><code>  merged.SD.4.plots &lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$fieldSD,
  fieldB.data$fieldSD,
  fieldC.data$fieldSD,
  fieldD.data$fieldSD
  )
  )
  
  
  merged.Mean.4.plots &lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.mean,
  fieldB.data$field.mean,
  fieldC.data$field.mean,
  fieldD.data$field.mean
  )
  )
  
  
  merged.Count.4.plots &lt;-
  Reduce(
  function(x, y)
  merge(x, y, all = TRUE),
  list(
  fieldA.data$field.Count,
  fieldB.data$field.Count,
  fieldC.data$field.Count,
  fieldD.data$field.Count
  )
  )</code></pre>
<p>We need SD pooled for these levels so we can calculate Cohen’s d and Required
Replicates.</p>
<pre class="r"><code>  levels(merged.SD.4.plots$CR.Date.Levels)</code></pre>
<pre><code>##  [1] &quot;23000*2018-05-17&quot; &quot;24000*2018-05-17&quot; &quot;25000*2018-05-17&quot; &quot;26000*2018-05-17&quot;
##  [5] &quot;27000*2018-05-17&quot; &quot;28000*2018-05-17&quot; &quot;29000*2018-05-17&quot; &quot;24000*2018-05-20&quot;
##  [9] &quot;25000*2018-05-20&quot; &quot;25000*2018-05-21&quot; &quot;26000*2018-05-20&quot; &quot;26000*2018-05-21&quot;
## [13] &quot;27000*2018-05-20&quot; &quot;27000*2018-05-21&quot; &quot;28000*2018-05-20&quot; &quot;28000*2018-05-21&quot;
## [17] &quot;29000*2018-05-20&quot; &quot;29000*2018-05-21&quot; &quot;23000*2018-05-18&quot; &quot;24000*2018-05-18&quot;
## [21] &quot;25000*2018-05-18&quot; &quot;26000*2018-05-18&quot; &quot;26000*2018-05-19&quot; &quot;27000*2018-05-18&quot;
## [25] &quot;27000*2018-05-19&quot; &quot;28000*2018-05-18&quot; &quot;28000*2018-05-19&quot; &quot;23000*2018-05-20&quot;</code></pre>
<pre class="r"><code>  # First, we merged all merged.SD.4.plots, merged.Mean.4.plots and
  # merged.Count.4.plots from all fields.
  # Therefore, now merging all three dataframes from all four plots for SD, Mean
  # and counts by ControlRate column
  
  Mean_SD_Count.dat &lt;- Reduce(function(...)
  merge(..., by = c(&quot;CR.Date.Levels&quot;, &quot;grp&quot;), all.x = TRUE),
  lapply(
  list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots),
  transform,
  grp = ave(seq_along(CR.Date.Levels), CR.Date.Levels, FUN = seq_along)
  ))
  # head(Mean_SD_Count.dat)
  
  # Now, we can also do Anova on the merged data `Mean_SD_Count.dat`
  
  Mean_SD_Count_merged_for_all_four_plots &lt;- Mean_SD_Count.dat
  Mean_SD_Count_merged_for_all_four_plots$Yield &lt;-
  Mean_SD_Count_merged_for_all_four_plots$Mean
  Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels &lt;-
  Mean_SD_Count_merged_for_all_four_plots$ControlRate
  
  get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = &quot;all four fields&quot;)</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Based on this plot above, from all fields, we can tell that, Control Rates of
seeding of 23000-25000 has significantly less Mean yield as compared to
26000-29000. It looks like Control Rates of Seeding between 27000 and 28000
has the highest Mean yield. I also think that there is no effect on seeding
dates, I have done some analysis to show the seeding effect only in the last
part of this report and based on that as well, I do not think there is
significant effect of Dates on Yield.</p>
<p>We now calculate pooled SD for merged 4 plots <code>Mean_SD_Count.dat</code> to calculate
Effect Size and Required Replicates for all fields (combined analysis) using
pooled SD.</p>
<pre class="r"><code>pooled.dat &lt;- Mean_SD_Count.dat
# # Pooled sd can be calculated as:
pooled.dat$df &lt;- pooled.dat$Count - 1
# pooled SD is :
# pooledSD &lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) )
# We can calculate our SD pooled using this formula:
#  s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
# We will derrive this in steps as below:
pooled.dat$df &lt;- pooled.dat$Count - 1
pooled.dat$sd.square &lt;- pooled.dat$SD ^ 2
pooled.dat$ss &lt;- pooled.dat$sd.square * pooled.dat$df
# We can use convenience function (aggregate) for splitting and calculating
# the necessary sums.
ds &lt;- aggregate(ss ~ CR.Date.Levels, data = pooled.dat, sum)
# Two different built-in methods for split apply, we could use aggregate for
# both if we wanted. This calculates our degrees of freedom.
ds$df &lt;- tapply(pooled.dat$df, pooled.dat$CR.Date.Levels, sum)
# divide ss by df and then we get sd square
ds$sd.square &lt;- ds$ss / ds$df
# Finally, we can get our sd pooled
ds$SD_pooled &lt;- sqrt(ds$sd.square)
# ds
# However, we could also calculate our sd_pooled as below and get the same results :
# sd_pooled &lt;- lapply( split(Mean_SD_Count.dat, Mean_SD_Count.dat$CR.Date.Levels),
# function(dd) sqrt( sum( dd$SD^2 * (dd$Count-1) )/(sum(dd$Count-1)-nrow(dd)) ) )
# Now, we calculate Mean (Mean of Means) from the merged table
# `Mean_SD_Count.dat so we can calculate Cohens d and RequiredReplicates for all
# four field combined.
ds.Mean &lt;-
setNames(aggregate(
Mean_SD_Count.dat$Mean,
by = list(Mean_SD_Count.dat$CR.Date.Levels),
FUN = mean
),
c(&quot;CR.Date.Levels&quot;, &quot;Mean&quot;))
ds &lt;- merge(ds, ds.Mean, by.x = &quot;CR.Date.Levels&quot;)
# ds
# Now we calculate the Effect Size and Cohen&#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate
# Now we calculate the Effect Size and Cohen&#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate usinf our function
RequiredReplicates_for_all_fields &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(
field = ds,
intervals = ControlRateInterval,
Single.Field.Analysis = FALSE
)</code></pre>
<p>This table gives us the required replicates and effectSize for all ControlRates-Date pairs</p>
<pre class="r"><code>RequiredReplicates_for_all_fields</code></pre>
<pre><code>## $Req.Rep.table.field
##       Group                                  EffectSize          
##  [1,] &quot;23000*2018-05-17 Vs 23000*2018-05-18&quot; &quot;5.76937088213814&quot;  
##  [2,] &quot;23000*2018-05-18 Vs 23000*2018-05-20&quot; &quot;1.77467806243082&quot;  
##  [3,] &quot;23000*2018-05-20 Vs 24000*2018-05-17&quot; &quot;0.319085268435178&quot; 
##  [4,] &quot;24000*2018-05-17 Vs 24000*2018-05-18&quot; &quot;2.23695386227101&quot;  
##  [5,] &quot;24000*2018-05-18 Vs 24000*2018-05-20&quot; &quot;1.4793489792455&quot;   
##  [6,] &quot;24000*2018-05-20 Vs 25000*2018-05-17&quot; &quot;0.764820441835373&quot; 
##  [7,] &quot;25000*2018-05-17 Vs 25000*2018-05-18&quot; &quot;0.749715759825526&quot; 
##  [8,] &quot;25000*2018-05-18 Vs 25000*2018-05-20&quot; &quot;0.631404233619908&quot; 
##  [9,] &quot;25000*2018-05-20 Vs 25000*2018-05-21&quot; &quot;0.0122749797874684&quot;
## [10,] &quot;25000*2018-05-21 Vs 26000*2018-05-17&quot; &quot;1.86649999718639&quot;  
## [11,] &quot;26000*2018-05-17 Vs 26000*2018-05-18&quot; &quot;0.604237639607718&quot; 
## [12,] &quot;26000*2018-05-18 Vs 26000*2018-05-19&quot; &quot;0.335012013191533&quot; 
## [13,] &quot;26000*2018-05-19 Vs 26000*2018-05-20&quot; &quot;0.294694932900609&quot; 
## [14,] &quot;26000*2018-05-20 Vs 26000*2018-05-21&quot; &quot;0.0568859046090692&quot;
## [15,] &quot;26000*2018-05-21 Vs 27000*2018-05-17&quot; &quot;0.450821573333859&quot; 
## [16,] &quot;27000*2018-05-17 Vs 27000*2018-05-18&quot; &quot;0.227990733240137&quot; 
## [17,] &quot;27000*2018-05-18 Vs 27000*2018-05-19&quot; &quot;0.212920132008121&quot; 
## [18,] &quot;27000*2018-05-19 Vs 27000*2018-05-20&quot; &quot;0.186994200485925&quot; 
## [19,] &quot;27000*2018-05-20 Vs 27000*2018-05-21&quot; &quot;0.312513439276243&quot; 
## [20,] &quot;27000*2018-05-21 Vs 28000*2018-05-17&quot; &quot;0.296577835810804&quot; 
## [21,] &quot;28000*2018-05-17 Vs 28000*2018-05-18&quot; &quot;0.114094426452159&quot; 
## [22,] &quot;28000*2018-05-18 Vs 28000*2018-05-19&quot; &quot;2.35237654983648&quot;  
## [23,] &quot;28000*2018-05-19 Vs 28000*2018-05-20&quot; &quot;0.182678706582559&quot; 
## [24,] &quot;28000*2018-05-20 Vs 28000*2018-05-21&quot; &quot;0.703554320633904&quot; 
## [25,] &quot;28000*2018-05-21 Vs 29000*2018-05-17&quot; &quot;0.983655196454225&quot; 
## [26,] &quot;29000*2018-05-17 Vs 29000*2018-05-20&quot; &quot;0.927565354976892&quot; 
## [27,] &quot;29000*2018-05-20 Vs 29000*2018-05-21&quot; &quot;0.341504648673164&quot; 
##       RequiredReplicates
##  [1,] &quot;0&quot;               
##  [2,] &quot;5&quot;               
##  [3,] &quot;154&quot;             
##  [4,] &quot;3&quot;               
##  [5,] &quot;7&quot;               
##  [6,] &quot;27&quot;              
##  [7,] &quot;28&quot;              
##  [8,] &quot;39&quot;              
##  [9,] &quot;104183&quot;          
## [10,] &quot;5&quot;               
## [11,] &quot;43&quot;              
## [12,] &quot;140&quot;             
## [13,] &quot;181&quot;             
## [14,] &quot;4851&quot;            
## [15,] &quot;77&quot;              
## [16,] &quot;302&quot;             
## [17,] &quot;346&quot;             
## [18,] &quot;449&quot;             
## [19,] &quot;161&quot;             
## [20,] &quot;178&quot;             
## [21,] &quot;1206&quot;            
## [22,] &quot;3&quot;               
## [23,] &quot;470&quot;             
## [24,] &quot;32&quot;              
## [25,] &quot;16&quot;              
## [26,] &quot;18&quot;              
## [27,] &quot;135&quot;</code></pre>
<p>We can plot this results</p>
<pre class="r"><code>RequiredReplicates_for_all_fields &lt;-
  as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field)
  
  RequiredReplicates_for_all_fields$Group &lt;-
  factor(RequiredReplicates_for_all_fields$Group)
  
  RequiredReplicates_for_all_fields$RequiredReplicates &lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates))
  
  RequiredReplicates_for_all_fields$EffectSize &lt;-
  as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize))
  
  
  # There are one or two extreme Required replicates and extreme Effect size. We can
  # remove them to plot them, so we can properly visualize the datapoints
  
  RequiredReplicates_for_all_fields &lt;-
  RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$RequiredReplicates &lt; 4000, ]
  RequiredReplicates_for_all_fields &lt;-
  RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$EffectSize &lt; 1.5, ]
  
  my_levels &lt;-
  length(levels(RequiredReplicates_for_all_fields$Group))
  RequiredReplicates_for_all_fields$EffectSize &lt;-
  round(RequiredReplicates_for_all_fields$EffectSize, 2)
  
  # scaleFUN &lt;- function(x) sprintf(&quot;%.2f&quot;, x)
  # Plot RequiredReplicates_for_all_fields
  
  n &lt;- my_levels
  qual_col_pals = brewer.pal.info[brewer.pal.info$category == &#39;qual&#39;, ]
  col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
  
  
  ggplot(
  RequiredReplicates_for_all_fields,
  aes(
  x = EffectSize,
  y = RequiredReplicates,
  shape = as.factor(Group),
  group = 1
  )
  ) +
  ggtitle(&quot;Effect size Vs Required replicates \n for all fields data \n(Combined Effect)&quot;) +
  geom_point(alpha = 0.9,
  size = 3,
  stroke = 1)  +
  scale_shape_manual(values = rep(c(0:2, 5:6, 9:10, 11:12, 14), times =
  4)) +
  scale_color_manual(values = col_vector) +
  geom_line(color = &quot;black&quot;) +
  scale_x_continuous(&quot;EffectSize&quot;,
  breaks = c(0,
  0.2,
  0.5,
  1,
  max(
  as.numeric(RequiredReplicates_for_all_fields$EffectSize)
  ))) +
  theme_bw() +
  theme(
  axis.title = element_text(size = 14, face = &quot;bold&quot;),
  axis.text = element_text(size = 10),
  plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;),
  axis.line = element_line(colour = &quot;black&quot;),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank()
  ) +
  labs(shape = &quot;Comparison groups&quot;)</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Based on this plot above, we can tell that there is strong negative correlation
between EffectSize and RequiredReplicates.</p>
<p>So far, we analyzed our data for controlRates-Date pair, now we can also do the
same analysis for Date effect only.</p>
<pre class="r"><code>######################## Date Only Analysis !! ##############################
# If we want to do Date effect only without taking ControlRates into consideration, then we do the following:
  
# field A
fieldA.data &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataA,
intervals = ControlRateInterval,
Date.Only = TRUE)
# fieldA.data
# field B
fieldB.data &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataB,
intervals = ControlRateInterval,
Date.Only = TRUE)
# fieldB.data
# field C
fieldC.data &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataC,
intervals = ControlRateInterval,
Date.Only = TRUE)
# fieldC.data
# field D
fieldD.data &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(field = dataD,
intervals = ControlRateInterval,
Date.Only = TRUE)
# fieldD.data
# We can now merge all four fields  for SD, means and ControlRate level counts.
merged.SD.4.plots &lt;-
Reduce(
function(x, y)
merge(x, y, all = TRUE),
list(
fieldA.data$fieldSD,
fieldB.data$fieldSD,
fieldC.data$fieldSD,
fieldD.data$fieldSD
)
)
merged.Mean.4.plots &lt;-
Reduce(
function(x, y)
merge(x, y, all = TRUE),
list(
fieldA.data$field.mean,
fieldB.data$field.mean,
fieldC.data$field.mean,
fieldD.data$field.mean
)
)
merged.Count.4.plots &lt;-
Reduce(
function(x, y)
merge(x, y, all = TRUE),
list(
fieldA.data$field.Count,
fieldB.data$field.Count,
fieldC.data$field.Count,
fieldD.data$field.Count
)
)
# We need SD pooled for these levels so we can calculate Cohen&#39;s d and
# Required Replicates.
levels(merged.SD.4.plots$CR.Date.Levels)</code></pre>
<pre><code>## [1] &quot;2018-05-17&quot; &quot;2018-05-20&quot; &quot;2018-05-21&quot; &quot;2018-05-18&quot; &quot;2018-05-19&quot;</code></pre>
<pre class="r"><code># First, we merged all merged.SD.4.plots, merged.Mean.4.plots and
# merged.Count.4.plots from all fields.
# Therefore, now merging all three dataframes from all four plots for SD, Mean
# and counts by ControlRate column
Mean_SD_Count.dat &lt;- Reduce(function(...)
merge(..., by = c(&quot;CR.Date.Levels&quot;, &quot;grp&quot;), all.x = TRUE),
lapply(
list(merged.Mean.4.plots, merged.SD.4.plots, merged.Count.4.plots),
transform,
grp = ave(seq_along(CR.Date.Levels), CR.Date.Levels, FUN = seq_along)
))
head(Mean_SD_Count.dat)</code></pre>
<pre><code>##   CR.Date.Levels grp     Mean       SD Count
## 1     2018-05-17   1 234.0993 40.74205  6710
## 2     2018-05-18   1 222.4764 20.50457  3880
## 3     2018-05-19   1 229.3088 10.26760  6524
## 4     2018-05-20   1 229.8301 26.90213  7747
## 5     2018-05-20   2 230.8569 30.22076 12654
## 6     2018-05-21   1 226.4394 19.87301  1574</code></pre>
<pre class="r"><code># Now, we can also do Anova on the merged data `Mean_SD_Count.dat`
Mean_SD_Count_merged_for_all_four_plots &lt;- Mean_SD_Count.dat
Mean_SD_Count_merged_for_all_four_plots$Yield &lt;-
Mean_SD_Count_merged_for_all_four_plots$Mean
Mean_SD_Count_merged_for_all_four_plots$ControlRate.Levels &lt;-
Mean_SD_Count_merged_for_all_four_plots$ControlRate
Mean_SD_Count_merged_for_all_four_plots$CR.Date.Levels &lt;-
as.factor(gsub(
&quot;-&quot;,
&quot;_&quot;,
Mean_SD_Count_merged_for_all_four_plots$CR.Date.Levels
))
get_my_box_plot(Mean_SD_Count_merged_for_all_four_plots, plot_name = &quot;all four fields&quot;)</code></pre>
<pre><code>##                      Df Sum Sq Mean Sq F value Pr(&gt;F)
## field$CR.Date.Levels  4  78.66  19.665    37.3  0.122
## Residuals             1   0.53   0.527</code></pre>
<p>From this result above, since the residuals from ANOVA model has only 1 degree
of freedom, and the global p-value is not significant, we don’t have to
plot the TUKEY pairwise test. Instead, we only show the Summary anova. This
shows that the date alone doesn’t affect our mean Yield.</p>
<p>We now calculate pooled SD for merged 4 plots <code>Mean_SD_Count.dat</code> to calculate
Effect Size and Required Replicates for all fields (combined analysis) using
pooled SD.</p>
<pre class="r"><code>pooled.dat &lt;- Mean_SD_Count.dat
# # Pooled sd can be calculated as:
pooled.dat$df &lt;- pooled.dat$Count - 1
# pooled SD is :
# pooledSD &lt;- sqrt( sum(pooled.dat$sd^2 * pooled.dat$df) / sum(pooled.dat$df) )
# We can calculate our SD pooled using this formula:
#  s_{pooled} = \sqrt{\frac{\sum_i (n_i-1)s_i^2}{N-k}}
# We will derrive this in steps as below:
pooled.dat$df &lt;- pooled.dat$Count - 1
pooled.dat$sd.square &lt;- pooled.dat$SD ^ 2
pooled.dat$ss &lt;- pooled.dat$sd.square * pooled.dat$df
# We can use convenience function (aggregate) for splitting and calculating
# the necessary sums.
ds &lt;- aggregate(ss ~ CR.Date.Levels, data = pooled.dat, sum)
# Two different built in methods for split apply, we could use aggregate for
# both if we wanted. This calculates our degrees of freedom.
ds$df &lt;- tapply(pooled.dat$df, pooled.dat$CR.Date.Levels, sum)
# divide ss by df and then we get sd square
ds$sd.square &lt;- ds$ss / ds$df
# Finally, we can get our sd pooled
ds$SD_pooled &lt;- sqrt(ds$sd.square)
ds</code></pre>
<pre><code>##   CR.Date.Levels         ss    df sd.square SD_pooled
## 1     2018-05-17 11136365.7  6709 1659.9144  40.74205
## 2     2018-05-20 17161878.7 20399  841.3098  29.00534
## 3     2018-05-21   621235.0  1573  394.9364  19.87301
## 4     2018-05-18  1630876.2  3879  420.4373  20.50457
## 5     2018-05-19   687678.2  6523  105.4236  10.26760</code></pre>
<pre class="r"><code># However, we could also calculate our sd_pooled as below and get the same results :
sd_pooled &lt;-
lapply(split(Mean_SD_Count.dat, Mean_SD_Count.dat$CR.Date.Levels),
function(dd)
sqrt(sum(dd$SD ^ 2 * (dd$Count - 1)) / (sum(dd$Count - 1) - nrow(dd))))
# Now, we calculate Mean (Mean of Means) from the merged table
# `Mean_SD_Count.dat`, so we can calculate Cohens d and RequiredReplicates for
# all four field combined.
ds.Mean &lt;-
setNames(aggregate(
Mean_SD_Count.dat$Mean,
by = list(Mean_SD_Count.dat$CR.Date.Levels),
FUN = mean
),
c(&quot;CR.Date.Levels&quot;, &quot;Mean&quot;))
ds &lt;- merge(ds, ds.Mean, by.x = &quot;CR.Date.Levels&quot;)
ds</code></pre>
<pre><code>##   CR.Date.Levels         ss    df sd.square SD_pooled     Mean
## 1     2018-05-17 11136365.7  6709 1659.9144  40.74205 234.0993
## 2     2018-05-18  1630876.2  3879  420.4373  20.50457 222.4764
## 3     2018-05-19   687678.2  6523  105.4236  10.26760 229.3088
## 4     2018-05-20 17161878.7 20399  841.3098  29.00534 230.3435
## 5     2018-05-21   621235.0  1573  394.9364  19.87301 226.4394</code></pre>
<pre class="r"><code># Now we calculate the Effect Size and Cohen&#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate
# Now we calculate the Effect Size and Cohen&#39;s D for the combined 4 plots using
# mean yield and sd pooled for different ControlRate usinf our function
# `calculate_field_mean_SD_and_get_RR_EffSize`.
RequiredReplicates_for_all_fields &lt;-
calculate_field_mean_SD_and_get_RR_EffSize(
field = ds,
intervals = ControlRateInterval,
Single.Field.Analysis = FALSE
)
RequiredReplicates_for_all_fields</code></pre>
<pre><code>## $Req.Rep.table.field
##      Group                      EffectSize          RequiredReplicates
## [1,] &quot;2018-05-17 Vs 2018-05-18&quot; &quot;0.360381603784739&quot; &quot;121&quot;             
## [2,] &quot;2018-05-18 Vs 2018-05-19&quot; &quot;0.421358834058387&quot; &quot;88&quot;              
## [3,] &quot;2018-05-19 Vs 2018-05-20&quot; &quot;0.047558010269331&quot; &quot;6940&quot;            
## [4,] &quot;2018-05-20 Vs 2018-05-21&quot; &quot;0.157027441353185&quot; &quot;637&quot;</code></pre>
<pre class="r"><code># We can plot this results for better visualization of pattern.
RequiredReplicates_for_all_fields &lt;-
as.data.frame(RequiredReplicates_for_all_fields$Req.Rep.table.field)
RequiredReplicates_for_all_fields$Group &lt;-
factor(RequiredReplicates_for_all_fields$Group)
RequiredReplicates_for_all_fields$RequiredReplicates &lt;-
as.numeric(as.character(RequiredReplicates_for_all_fields$RequiredReplicates))
RequiredReplicates_for_all_fields$EffectSize &lt;-
as.numeric(as.character(RequiredReplicates_for_all_fields$EffectSize))
# There are one or two extreme Required replicates and extreme Effect size. We can
# remove them to plot them, so we can properly visualize the datapoints
RequiredReplicates_for_all_fields &lt;-
RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$RequiredReplicates &lt; 4000, ]
RequiredReplicates_for_all_fields &lt;-
RequiredReplicates_for_all_fields[RequiredReplicates_for_all_fields$EffectSize &lt; 1.5, ]
my_levels &lt;-
length(levels(RequiredReplicates_for_all_fields$Group))
RequiredReplicates_for_all_fields$EffectSize &lt;-
round(RequiredReplicates_for_all_fields$EffectSize, 2)
# Plot RequiredReplicates_for_all_fields
n &lt;- my_levels
qual_col_pals = brewer.pal.info[brewer.pal.info$category == &#39;qual&#39;,]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
ggplot(
RequiredReplicates_for_all_fields,
aes(
x = EffectSize,
y = RequiredReplicates,
shape = as.factor(Group),
group = 1
)
) +
ggtitle(&quot;Effect size Vs Required replicates for all fields data \n(Combined Effect)&quot;) +
geom_point(alpha = 0.9,
size = 3,
stroke = 1)  +
scale_shape_manual(values = rep(c(0:2, 5:6, 9:10, 11:12, 14), times =
4)) +
scale_color_manual(values = col_vector) +
geom_line(color = &quot;black&quot;) +
scale_x_continuous(&quot;EffectSize&quot;,
breaks = c(0,
0.2,
0.5,
1,
max(
as.numeric(RequiredReplicates_for_all_fields$EffectSize)
))) +
theme_bw() +
theme(
axis.title = element_text(size = 14, face = &quot;bold&quot;),
axis.text = element_text(size = 10),
plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;),
axis.line = element_line(colour = &quot;black&quot;),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
labs(shape = &quot;Comparison groups&quot;)</code></pre>
<p><img src="/post/Multivariate_statistical_analysis_with_agronomic_data/Multivariate_statistical_analysis_with_agronomic_data_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<pre class="r"><code>################################ End of codes !! ################################</code></pre>
<p>Discussion: Based on these results, I think that the fields were planted within
24 hours. seedB and seedC were planted on different dates.
Similarly, harvestB was harvested within 24 hours and on the same date, but rest
of the fields were harvested/planted for multiple dates. Additionally, I also
performed ANOVA for ControlRates paired with dates and were not significantly
different from what I found in <code>Part 1</code>. I analyzed these data for Date
effect and seems there is no Date effect on Mean Yield. I thought
the skewness in 28000 interval from my analysis in <code>Part 1</code> would be addressed by Date effect,
but I could not find significant effect of date here. As in the <code>Part 1</code>, I also
calculated the required replicated for ControlRate-Date pairs as well as Date
only observations, and in both cases, the EffectSize was inversely proportional
to RequiredReplicates.
© 2022 GitHub, Inc.
Terms
Privacy
Security
Status
Docs
Contact GitHub
Pricing
API
Training
Blog
About
Loading complete</p>
</div>
