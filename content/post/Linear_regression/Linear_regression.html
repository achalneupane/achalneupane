---
Authors: ["**Achal Neupane**"]
title: "Linear regression"
date: 2021-10-18T17:26:23-05:00
draft: false
output: html_document
tags:
- R
- Statistics
- Machine Learning
summary: Statistics series
---



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<p>Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.</p>
<p>Linear regression models are often fitted using the <code>least squares</code> approach, but they may also be fitted in other ways, such as by minimizing the “lack of fit” in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (<span class="math inline">\({L2-norm}\)</span> penalty) and lasso (<span class="math inline">\({L1-norm}\)</span> penalty). Conversely, the <code>least squares</code> approach can be used to fit models that are not linear models. Thus, although the terms “least squares” and “linear model” are closely linked, they are not synonymous.</p>
<p>In this exercise series of linear regression, we will look at some problems to better understand the use case of linear regression.</p>
<div id="part-1" class="section level2">
<h2>Part 1</h2>
<p>Consider the fitted values that result from performing linear regression without an intercept. In this setting, the <span class="math inline">\(i-th\)</span> fitted value takes the form <span class="math inline">\(\hat{y_i} = x_i\hat{\beta}\)</span>, where
<span class="math display">\[\hat{\beta} = \frac{\sum_{i=1}^n x_iy_i}{\sum_{i&#39;=1}^nx_{i&#39;}^2}.\]</span></p>
<p>Show that we can write:</p>
<p><span class="math display">\[\hat{y}_i = \sum_{i=1}^na_iy_i.\]</span></p>
<p>What is <span class="math inline">\(a_{i&#39;}\)</span> ?</p>
<p>Note: we interpret this result by saying that the fitted values from
linear regression are linear combinations of the response values.</p>
<p>Here, we can write: <span class="math display">\[\hat{y}_i = x_i\frac{\sum_{i=1}^n x_iy_i}{\sum_{i&#39;=1}^nx_{i&#39;}^2}\]</span></p>
<p>Where <span class="math display">\[\frac{x_i}{\sum_{i&#39;=1}^nx_{i&#39;}^2}\]</span> is a constant value for each <span class="math inline">\(i^{th}\)</span> which can be represented as <span class="math inline">\(c_{i&#39;}\)</span>. If we have this on the equation above, we get:</p>
<p><span class="math display">\[\hat{y}_i = c_{i&#39;}{\sum_{i=1}^n x_iy_i} = {\sum_{i=1}^n c_{i&#39;}x_iy_i}\]</span></p>
<p>When we combine <span class="math inline">\(c_{i&#39;}\)</span> with <span class="math inline">\(x_i\)</span>, we get <span class="math inline">\(a_{i&#39;}\)</span>. We could determine <span class="math inline">\(a_{i&#39;}\)</span> as weight function over the <span class="math inline">\(Y\)</span> set to estimate the <span class="math inline">\(y_i\)</span> element.</p>
<p>This question should be answered using the Carseats dataset.
First, we will fit a multiple regression model to predict Sales using Price, Urban, and US.</p>
<pre class="r"><code>### Q 2 a
library(ISLR)
library(knitr)
library(caret)
data(Carseats)
head(Carseats)</code></pre>
<pre><code>##   Sales CompPrice Income Advertising Population Price ShelveLoc Age Education
## 1  9.50       138     73          11        276   120       Bad  42        17
## 2 11.22       111     48          16        260    83      Good  65        10
## 3 10.06       113     35          10        269    80    Medium  59        12
## 4  7.40       117    100           4        466    97    Medium  55        14
## 5  4.15       141     64           3        340   128       Bad  38        13
## 6 10.81       124    113          13        501    72       Bad  78        16
##   Urban  US
## 1   Yes Yes
## 2   Yes Yes
## 3   Yes Yes
## 4   Yes Yes
## 5   Yes  No
## 6    No Yes</code></pre>
<pre class="r"><code>## make the regression model
model_3.7.10.a=lm(Sales~Price+Urban+US,data=Carseats)
summary(model_3.7.10.a)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sales ~ Price + Urban + US, data = Carseats)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9206 -1.6220 -0.0564  1.5786  7.0581 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 13.043469   0.651012  20.036  &lt; 2e-16 ***
## Price       -0.054459   0.005242 -10.389  &lt; 2e-16 ***
## UrbanYes    -0.021916   0.271650  -0.081    0.936    
## USYes        1.200573   0.259042   4.635 4.86e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.472 on 396 degrees of freedom
## Multiple R-squared:  0.2393, Adjusted R-squared:  0.2335 
## F-statistic: 41.52 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>layout(matrix(1:4,nrow=2))
plot(model_3.7.10.a,pch=16,col=4,cex=0.8)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Then we will provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!</p>
<p>Given the Unit sales (which is in thousands) at each location, we can interpret the result as follows: if there is 1 dollar change in price, 54.4588492 units in sales will go down keeping all other predictors fixed. We can also say that the unit sales in urban location are 21.9161508 units less than in rural location keeping all other predictors fixed. Likewise, we can also say that on average the unit sales in a US store are 1200.5726978 units more than in a non US store keeping other predictors fixed.</p>
<p>Now, we write out the model in equation form, being careful to handle the qualitative variables properly.</p>
<p><span class="math display">\[Sales = 13.0434689 + (-0.0544588)\times Price + (-0.0219162)\times Urban + (1.2005727)\times US + \varepsilon\]</span></p>
<p>with <span class="math inline">\(Urban = 1\)</span> if the store is in an urban location and <span class="math inline">\(0\)</span> if not, and <span class="math inline">\(US = 1\)</span> if the store is in the US and <span class="math inline">\(0\)</span> if not.</p>
<p>Now, we will try to find out for which of the predictors we can reject the null hypothesis
<span class="math inline">\(H_0 : \beta_j = 0\)</span> ?</p>
<p><strong>Answer:</strong></p>
<p>For Price and US, we can reject the Null Hypothesis at any significance level. The p-value (P&lt;0.05) is the threshold value at which we can reject the Null hypothesis.</p>
<p>On the basis of our response to the previous question, we will fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.</p>
<pre class="r"><code>model_3.7.10.e=lm(Sales~Price+US,data=Carseats)
summary(model_3.7.10.e)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sales ~ Price + US, data = Carseats)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9269 -1.6286 -0.0574  1.5766  7.0515 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 13.03079    0.63098  20.652  &lt; 2e-16 ***
## Price       -0.05448    0.00523 -10.416  &lt; 2e-16 ***
## USYes        1.19964    0.25846   4.641 4.71e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.469 on 397 degrees of freedom
## Multiple R-squared:  0.2393, Adjusted R-squared:  0.2354 
## F-statistic: 62.43 on 2 and 397 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>layout(matrix(1:4,nrow=2))
plot(model_3.7.10.e,pch=16,col=4,cex=0.8)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In this previous model, we removed the Urban variable. Urban variable did not have enough evidence of linear association with Sales. The summary statistics from this model show that both Price and US have nearly zero p-values, suggesting good linear association. We can also tell that the estimates of the coefficients are almost the same as the previous model, but the adjusted R-squared value now has slightly gone up.</p>
<p>Now, see how well the models fit the data.</p>
<pre class="r"><code>## plots:
data=Carseats[order(Carseats$Sales),]
model_3.7.10.a=lm(Sales~Price+Urban+US,data=data)
model_3.7.10.e=lm(Sales~Price+US,data=data)
layout(matrix(1:2,nrow=1))
## model A
plot(data$Sales, col=1, cex=0.5,pch=16,ylim = c(-5,20), ylab=&quot;sales&quot;,
     main=&quot;Sales~Price+Urban+US&quot;)
points(model_3.7.10.a$fitted.values, col=4,cex=0.5,pch=16)
legend(&quot;topright&quot;,bty=&quot;n&quot;,c(&quot;Actual Sales&quot;,&quot;Estimates Sales&quot;),
        col=c(1,4),pch=16)

## model E
plot(data$Sales, col=1, cex=0.5,pch=16,ylim = c(-5,20), ylab=&quot;sales&quot;,
     main=&quot;Sales~Price+US&quot;)
points(model_3.7.10.e$fitted.values, col=4,cex=0.5,pch=16)
legend(&quot;topright&quot;,bty=&quot;n&quot;,c(&quot;Actual Sales&quot;,&quot;Estimates Sales&quot;),
       col=c(1,4),pch=16)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>cat (&quot;Est std err of the error of model_3.7.10.a&quot;)</code></pre>
<pre><code>## Est std err of the error of model_3.7.10.a</code></pre>
<pre class="r"><code>Aerror=sqrt(sum((model_3.7.10.a$residuals)^2)/model_3.7.10.a$df.residual)
Aerror</code></pre>
<pre><code>## [1] 2.472492</code></pre>
<pre class="r"><code>cat (&quot;Est std err of the error of model_3.7.10.e&quot;)</code></pre>
<pre><code>## Est std err of the error of model_3.7.10.e</code></pre>
<pre class="r"><code>Eerror=sqrt(sum((model_3.7.10.e$residuals)^2)/model_3.7.10.e$df.residual)
Eerror</code></pre>
<pre><code>## [1] 2.469397</code></pre>
<pre class="r"><code>cat(&quot;Mean sales: &quot;)</code></pre>
<pre><code>## Mean sales:</code></pre>
<pre class="r"><code>mean(data$Sales)</code></pre>
<pre><code>## [1] 7.496325</code></pre>
<pre class="r"><code>layout(matrix(1))
hist(data$Sales)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code>cat(&quot;Anova of model_3.7.10.a and model_3.7.10.e&quot;)</code></pre>
<pre><code>## Anova of model_3.7.10.a and model_3.7.10.e</code></pre>
<pre class="r"><code>kable(anova(model_3.7.10.a,model_3.7.10.e), caption = &quot;Anova of model_3.7.10.a and model_3.7.10.e&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 1: </span>Anova of model_3.7.10.a and model_3.7.10.e</caption>
<thead>
<tr class="header">
<th align="right">Res.Df</th>
<th align="right">RSS</th>
<th align="right">Df</th>
<th align="right">Sum of Sq</th>
<th align="right">F</th>
<th align="right">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">396</td>
<td align="right">2420.835</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">397</td>
<td align="right">2420.874</td>
<td align="right">-1</td>
<td align="right">-0.0397904</td>
<td align="right">0.0065089</td>
<td align="right">0.9357389</td>
</tr>
</tbody>
</table>
<p>The adjusted R-squared values from the two models are 0.2335 (larger model) and 0.2354 (smaller model). So, i.e., [<code>summary(model_3.7.10.a)$r.sq</code> * <code>100</code>], which is roughly 23.9262888% (for smaller model) and 23.9275392% (for larger model) of the variability of the sales is explained by the models which means it doesn’t explain a lot.</p>
<p>In this figure above, we can visualize the estimated sales of the models (blue dots). Actual sales are represented by black dots. The estimated values are clustered roughly in between 5 and 10, underestimating the high Sales and overestimating the low Sales. Also the estimated std error of the error are computed to be 2.47 for both the models, which is relatively high for a response having a mean value of 7.5. The response is approximately normally distributed. Conducting anova with both models gave p-value 0.94 (which is high) suggesting they are not different.</p>
<p>Next, using the model built previously, obtain 95% confidence intervals for the coefficient(s).</p>
<pre class="r"><code>cat(&quot;For beta0: &quot;)</code></pre>
<pre><code>## For beta0:</code></pre>
<pre class="r"><code>model_3.7.10.e$coeff[1]-0.63098*qt(0.95, 397)</code></pre>
<pre><code>## (Intercept) 
##     11.9905</code></pre>
<pre class="r"><code>model_3.7.10.e$coeff[1]+0.63098*qt(0.975, 397)</code></pre>
<pre><code>## (Intercept) 
##    14.27127</code></pre>
<pre class="r"><code>cat(&quot;For beta1: &quot;)</code></pre>
<pre><code>## For beta1:</code></pre>
<pre class="r"><code>model_3.7.10.e$coeff[2]-0.00523*qt(0.975, 397)</code></pre>
<pre><code>##       Price 
## -0.06475959</code></pre>
<pre class="r"><code>model_3.7.10.e$coeff[2]+0.00523*qt(0.975, 397)</code></pre>
<pre><code>##       Price 
## -0.04419568</code></pre>
<pre class="r"><code>cat(&quot;For beta2: &quot;)</code></pre>
<pre><code>## For beta2:</code></pre>
<pre class="r"><code>model_3.7.10.e$coeff[3]-0.25846*qt(0.975, 397)</code></pre>
<pre><code>##     USYes 
## 0.6915216</code></pre>
<pre class="r"><code>model_3.7.10.e$coeff[3]+0.25846*qt(0.975, 397)</code></pre>
<pre><code>##    USYes 
## 1.707764</code></pre>
<pre class="r"><code>cat(&quot;Or we can also simply use confint function: &quot;)</code></pre>
<pre><code>## Or we can also simply use confint function:</code></pre>
<pre class="r"><code>kable(confint(model_3.7.10.e, level = 0.95), caption = &quot;95% confidence intervals for the coefficient(s)&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-5">Table 2: </span>95% confidence intervals for the coefficient(s)</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">2.5 %</th>
<th align="right">97.5 %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">11.7903202</td>
<td align="right">14.2712653</td>
</tr>
<tr class="even">
<td align="left">Price</td>
<td align="right">-0.0647598</td>
<td align="right">-0.0441954</td>
</tr>
<tr class="odd">
<td align="left">USYes</td>
<td align="right">0.6915196</td>
<td align="right">1.7077663</td>
</tr>
</tbody>
</table>
<p>Check if there is any evidence of outliers or high leverage observations in the model?</p>
<pre class="r"><code># par(mfrow = c(2, 2))
# plot(model_3.7.10.e)
layout(matrix(1:4,nrow=2))
plot(model_3.7.10.e,pch=16,col=4,cex=0.8)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>## Looks like 69 and 377 are the y outliers


stdres=rstudent(model_3.7.10.e)
hist(stdres, main = &quot;Studentized Residuals&quot;)
#min(stdres)

cat(&quot;Additionally, we can point out the outliers: &quot;)</code></pre>
<pre><code>## Additionally, we can point out the outliers:</code></pre>
<pre class="r"><code>print(which(abs(stdres)&gt;qt(0.995,397)))</code></pre>
<pre><code>##  51  69  26 377 
##   6 393 398 400</code></pre>
<pre class="r"><code>## leverage
hii=lm.influence(model_3.7.10.e)$hat
layout(matrix(1))</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<pre class="r"><code>sum(hii)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="r"><code>barplot(hii,border = NA,col=&quot;gray40&quot;,ylim=c(0,0.05),xlab=&quot;Index&quot;,ylab=&quot;Leverage Statisitcs&quot;)
abline(h=2*mean(hii),lwd=2,col=2)
legend(&quot;topright&quot;,&quot;Threshold&quot;,bty=&quot;n&quot;,col=2, lwd=2, lty=1)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-6-3.png" width="672" /></p>
<pre class="r"><code>which(hii&gt;2*mean(hii))</code></pre>
<pre><code>## 175 166 204 357 270 387 316 366 192 157 156 209 160 314 126 384  43 172 273 368 
##   1   3   8  27  78  96 145 157 165 200 218 224 299 302 303 304 336 382 389 397</code></pre>
<p><em>The plot of standardized residuals versus leverage indicates the presence of a few outliers (at threshold of higher than 2 or lower than -2) which are . We can also use studentized residuals to determine any outliers. It also indicates some high leverage observations because some points exceed (p + 1)/n i.e., (0.01)</em></p>
<p>This problem involves the Boston dataset. We will now try to predict per capita crime rate using the other variables in this dataset. In other words, per capita
crime rate is the response, and the other variables are the predictors.</p>
<p>For each predictor, we will fit a simple linear regression model to predict the response. We will then find out in which of the models there is a statistically significant association between the predictor and the response. For this, we will create some plots to back up our assertions.</p>
<pre class="r"><code>library(MASS)
attach(Boston)


library(MASS)
data(Boston)

# We will loop over each variable
layout(matrix(1:4,nrow=2))
for(i in 2:(dim(Boston)[2])){
  message(&quot;Variable: &quot;, names(Boston)[i])
  model=lm(Boston$crim~Boston[,i])
  print(summary(model))
  for(j in 1:2){
    plot(model, which=j,main=paste(&quot;Variable: &quot;, names(Boston)[i]),
         pch=16, col=j+2,cex=0.6,lwd=2)
  }
}</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.429 -4.222 -2.620  1.250 84.523 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.45369    0.41722  10.675  &lt; 2e-16 ***
## Boston[, i] -0.07393    0.01609  -4.594 5.51e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.435 on 504 degrees of freedom
## Multiple R-squared:  0.04019,    Adjusted R-squared:  0.03828 
## F-statistic:  21.1 on 1 and 504 DF,  p-value: 5.506e-06</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.972  -2.698  -0.736   0.712  81.813 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.06374    0.66723  -3.093  0.00209 ** 
## Boston[, i]  0.50978    0.05102   9.991  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.866 on 504 degrees of freedom
## Multiple R-squared:  0.1653, Adjusted R-squared:  0.1637 
## F-statistic: 99.82 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.738 -3.661 -3.435  0.018 85.232 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.7444     0.3961   9.453   &lt;2e-16 ***
## Boston[, i]  -1.8928     1.5061  -1.257    0.209    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.597 on 504 degrees of freedom
## Multiple R-squared:  0.003124,   Adjusted R-squared:  0.001146 
## F-statistic: 1.579 on 1 and 504 DF,  p-value: 0.2094</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.371  -2.738  -0.974   0.559  81.728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -13.720      1.699  -8.073 5.08e-15 ***
## Boston[, i]   31.249      2.999  10.419  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.81 on 504 degrees of freedom
## Multiple R-squared:  0.1772, Adjusted R-squared:  0.1756 
## F-statistic: 108.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.604 -3.952 -2.654  0.989 87.197 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   20.482      3.365   6.088 2.27e-09 ***
## Boston[, i]   -2.684      0.532  -5.045 6.35e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.401 on 504 degrees of freedom
## Multiple R-squared:  0.04807,    Adjusted R-squared:  0.04618 
## F-statistic: 25.45 on 1 and 504 DF,  p-value: 6.347e-07</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.789 -4.257 -1.230  1.527 82.849 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.77791    0.94398  -4.002 7.22e-05 ***
## Boston[, i]  0.10779    0.01274   8.463 2.85e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.057 on 504 degrees of freedom
## Multiple R-squared:  0.1244, Adjusted R-squared:  0.1227 
## F-statistic: 71.62 on 1 and 504 DF,  p-value: 2.855e-16</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.708 -4.134 -1.527  1.516 81.674 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   9.4993     0.7304  13.006   &lt;2e-16 ***
## Boston[, i]  -1.5509     0.1683  -9.213   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.965 on 504 degrees of freedom
## Multiple R-squared:  0.1441, Adjusted R-squared:  0.1425 
## F-statistic: 84.89 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.164  -1.381  -0.141   0.660  76.433 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.28716    0.44348  -5.157 3.61e-07 ***
## Boston[, i]  0.61791    0.03433  17.998  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.718 on 504 degrees of freedom
## Multiple R-squared:  0.3913, Adjusted R-squared:   0.39 
## F-statistic: 323.9 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-4.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.513  -2.738  -0.194   1.065  77.696 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -8.528369   0.815809  -10.45   &lt;2e-16 ***
## Boston[, i]  0.029742   0.001847   16.10   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.997 on 504 degrees of freedom
## Multiple R-squared:  0.3396, Adjusted R-squared:  0.3383 
## F-statistic: 259.2 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -7.654 -3.985 -1.912  1.825 83.353 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -17.6469     3.1473  -5.607 3.40e-08 ***
## Boston[, i]   1.1520     0.1694   6.801 2.94e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.24 on 504 degrees of freedom
## Multiple R-squared:  0.08407,    Adjusted R-squared:  0.08225 
## F-statistic: 46.26 on 1 and 504 DF,  p-value: 2.943e-11</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-5.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.756  -2.299  -2.095  -1.296  86.822 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 16.553529   1.425903  11.609   &lt;2e-16 ***
## Boston[, i] -0.036280   0.003873  -9.367   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.946 on 504 degrees of freedom
## Multiple R-squared:  0.1483, Adjusted R-squared:  0.1466 
## F-statistic: 87.74 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.925  -2.822  -0.664   1.079  82.862 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.33054    0.69376  -4.801 2.09e-06 ***
## Boston[, i]  0.54880    0.04776  11.491  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.664 on 504 degrees of freedom
## Multiple R-squared:  0.2076, Adjusted R-squared:  0.206 
## F-statistic:   132 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-6.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = Boston$crim ~ Boston[, i])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.071 -4.022 -2.343  1.298 80.957 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 11.79654    0.93419   12.63   &lt;2e-16 ***
## Boston[, i] -0.36316    0.03839   -9.46   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.934 on 504 degrees of freedom
## Multiple R-squared:  0.1508, Adjusted R-squared:  0.1491 
## F-statistic: 89.49 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>## Brown Forsythe test
library(lawstat)
for(i in 2:(dim(Boston)[2])){
  model=lm(Boston$crim~Boston[,i])
  # if (levene.test(model$resid,Boston[,i],trim.alpha=0.5)$p.value &lt;0.05){
  message(&quot;BF test p-value for the variable: &quot;,names(Boston)[i])
  # }
  print(levene.test(model$resid,Boston[,i],trim.alpha=0.5)$p.value)
}</code></pre>
<pre><code>## [1] 0.2281022
## [1] 0.00854712
## [1] 0.1850185
## [1] 1.248128e-12
## [1] 1.437096e-11
## [1] 1
## [1] 4.047298e-81
## [1] 5.868249e-20
## [1] 0.0006281896
## [1] 7.930461e-08
## [1] 1
## [1] 0.000370113
## [1] 1.116128e-30</code></pre>
<pre class="r"><code>plot(crim ~ . - crim, data = Boston)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-7.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-8.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-9.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-7-10.png" width="672" /></p>
<p><em>Statistically significant results were observed between the predictor and the response for every variable except the Charles River Dummy(chas). When looking at the response variables and crime in simple scatter plots, one can see how a general linear regression with these variables would allow for a better prediction of crime than simply using the mean of crime. That is, the data seems to have some slight shape sloping up or down, and isn’t a random cloud of data.</em></p>
<p><em>It is also important to note that although almost every variable is statistically significant, R-squared value is very low indicating that these predictors describe only a small amount of the variation in the response.</em></p>
<p><em>Additionally, The formal Brown Forsythe test produced evidence of homoscedasticity (meaning they all have same variance at every X) for the following 8 variables out of all 13 variables: indus, nox, rm, dis, rad, tax, ptratio, lstat, medv</em></p>
<p>Here, we will fit a multiple regression model to predict the response using
all of the predictors. We wil find out for which predictors
we can reject the null hypothesis <span class="math inline">\(H_0 : \beta_j = 0\)</span>.</p>
<pre class="r"><code>library(MASS)
attach(Boston)
fullmodel=lm(crim~.,data=Boston)
summary(fullmodel)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ ., data = Boston)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.924 -2.120 -0.353  1.019 75.051 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  17.033228   7.234903   2.354 0.018949 *  
## zn            0.044855   0.018734   2.394 0.017025 *  
## indus        -0.063855   0.083407  -0.766 0.444294    
## chas         -0.749134   1.180147  -0.635 0.525867    
## nox         -10.313535   5.275536  -1.955 0.051152 .  
## rm            0.430131   0.612830   0.702 0.483089    
## age           0.001452   0.017925   0.081 0.935488    
## dis          -0.987176   0.281817  -3.503 0.000502 ***
## rad           0.588209   0.088049   6.680 6.46e-11 ***
## tax          -0.003780   0.005156  -0.733 0.463793    
## ptratio      -0.271081   0.186450  -1.454 0.146611    
## black        -0.007538   0.003673  -2.052 0.040702 *  
## lstat         0.126211   0.075725   1.667 0.096208 .  
## medv         -0.198887   0.060516  -3.287 0.001087 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.439 on 492 degrees of freedom
## Multiple R-squared:  0.454,  Adjusted R-squared:  0.4396 
## F-statistic: 31.47 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>layout(matrix(1:4,nrow=2))
plot(fullmodel, pch=16, col=j+2,cex=0.6,lwd=2)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>It looks like we can reject the null hypothesis for predictors zn and black at P&lt;0.05; medv at P&lt;0.01; dis and rad at p&lt;0.001. For the rest of the predictors we fail to reject the null hypothesis.</p>
<p>We will also create a plot displaying the univariate regression coefficients on the x-axis, and the multiple regression coefficients on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.</p>
<pre class="r"><code>fullmodel=lm(crim~.,data=Boston)
cat(&quot;Create a dummy dataframe first with multiple and univariate variables&quot;)</code></pre>
<pre><code>## Create a dummy dataframe first with multiple and univariate variables</code></pre>
<pre class="r"><code>multiple=rep(1, 13)
univariate=rep(1, 13)


coeffs=as.data.frame(cbind(multiple,univariate))
for(i in 1:13){
  coeffs$multiple[i]=fullmodel$coeff[i+1]
}


for(i in 2:14){
  model=lm(Boston$crim~Boston[,i])
  coeffs$univariate[i-1]=model$coeff[2]
}
layout(matrix(1))
plot(multiple~univariate,data=coeffs,pch=16,col=2,cex=0.8,ylim=c(-20,10),
     xlab=&quot;Univariate Coefficients&quot;,
     ylab=&quot;Multiple Regression Coefficinets&quot;)
grid(col = &quot;lightgray&quot;, lty = &quot;dotted&quot;)
text(coeffs$univariate, coeffs$multiple, labels = names(Boston)[1:13],
      pos = 3, offset = 0.7, col = 1)
abline(0,1,col=3, lty=2, lwd=2)
cmodel=(lm(multiple~univariate,data=coeffs))
abline(cmodel$coeff[1],cmodel$coeff[2],col=4,lty=2, lwd=2)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><em>Here in this plot each red dot represents a predictor. The x value is the estimated coefficient with the predictor when separate models were created. The y value is the estimated coefficient of multiple linear regression. The blue dotted line represents a regression line of the points. So, if both models were to return same estimation, then these points would follow a line with slope 1 passing through the origin. This line is shown by the green dotted line. We see a severe difference between these lines. When separate regression performed, some are larger and some are smaller than the estimated values from the full regression model.</em></p>
<p>Now, check if there is any evidence of non-linear association between any of the
predictors and the response. In this case, we will need to fit a model of the form below for each predictor X.</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \varepsilon.\]</span></p>
<pre class="r"><code>attach(Boston)
mod.fit.zn2 &lt;- lm(crim ~ poly(zn, 3))
summary(mod.fit.zn2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(zn, 3))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.821 -4.614 -1.294  0.473 84.130 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    3.6135     0.3722   9.709  &lt; 2e-16 ***
## poly(zn, 3)1 -38.7498     8.3722  -4.628  4.7e-06 ***
## poly(zn, 3)2  23.9398     8.3722   2.859  0.00442 ** 
## poly(zn, 3)3 -10.0719     8.3722  -1.203  0.22954    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.372 on 502 degrees of freedom
## Multiple R-squared:  0.05824,    Adjusted R-squared:  0.05261 
## F-statistic: 10.35 on 3 and 502 DF,  p-value: 1.281e-06</code></pre>
<pre class="r"><code>mod.fit.indus2 &lt;- lm(crim ~ poly(indus, 3))
summary(mod.fit.indus2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(indus, 3))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.278 -2.514  0.054  0.764 79.713 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        3.614      0.330  10.950  &lt; 2e-16 ***
## poly(indus, 3)1   78.591      7.423  10.587  &lt; 2e-16 ***
## poly(indus, 3)2  -24.395      7.423  -3.286  0.00109 ** 
## poly(indus, 3)3  -54.130      7.423  -7.292  1.2e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.423 on 502 degrees of freedom
## Multiple R-squared:  0.2597, Adjusted R-squared:  0.2552 
## F-statistic: 58.69 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.nox2 &lt;- lm(crim ~ poly(nox, 3))
summary(mod.fit.nox2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(nox, 3))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.110 -2.068 -0.255  0.739 78.302 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     3.6135     0.3216  11.237  &lt; 2e-16 ***
## poly(nox, 3)1  81.3720     7.2336  11.249  &lt; 2e-16 ***
## poly(nox, 3)2 -28.8286     7.2336  -3.985 7.74e-05 ***
## poly(nox, 3)3 -60.3619     7.2336  -8.345 6.96e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.234 on 502 degrees of freedom
## Multiple R-squared:  0.297,  Adjusted R-squared:  0.2928 
## F-statistic: 70.69 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.rm2 &lt;- lm(crim ~ poly(rm, 3))
summary(mod.fit.rm2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(rm, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -18.485  -3.468  -2.221  -0.015  87.219 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    3.6135     0.3703   9.758  &lt; 2e-16 ***
## poly(rm, 3)1 -42.3794     8.3297  -5.088 5.13e-07 ***
## poly(rm, 3)2  26.5768     8.3297   3.191  0.00151 ** 
## poly(rm, 3)3  -5.5103     8.3297  -0.662  0.50858    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.33 on 502 degrees of freedom
## Multiple R-squared:  0.06779,    Adjusted R-squared:  0.06222 
## F-statistic: 12.17 on 3 and 502 DF,  p-value: 1.067e-07</code></pre>
<pre class="r"><code>mod.fit.age2 &lt;- lm(crim ~ poly(age, 3))
summary(mod.fit.age2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(age, 3))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.762 -2.673 -0.516  0.019 82.842 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     3.6135     0.3485  10.368  &lt; 2e-16 ***
## poly(age, 3)1  68.1820     7.8397   8.697  &lt; 2e-16 ***
## poly(age, 3)2  37.4845     7.8397   4.781 2.29e-06 ***
## poly(age, 3)3  21.3532     7.8397   2.724  0.00668 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.84 on 502 degrees of freedom
## Multiple R-squared:  0.1742, Adjusted R-squared:  0.1693 
## F-statistic: 35.31 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.dis2 &lt;- lm(crim ~ poly(dis, 3))
summary(mod.fit.dis2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(dis, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.757  -2.588   0.031   1.267  76.378 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     3.6135     0.3259  11.087  &lt; 2e-16 ***
## poly(dis, 3)1 -73.3886     7.3315 -10.010  &lt; 2e-16 ***
## poly(dis, 3)2  56.3730     7.3315   7.689 7.87e-14 ***
## poly(dis, 3)3 -42.6219     7.3315  -5.814 1.09e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.331 on 502 degrees of freedom
## Multiple R-squared:  0.2778, Adjusted R-squared:  0.2735 
## F-statistic: 64.37 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.rad2 &lt;- lm(crim ~ poly(rad, 3))
summary(mod.fit.rad2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(rad, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.381  -0.412  -0.269   0.179  76.217 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     3.6135     0.2971  12.164  &lt; 2e-16 ***
## poly(rad, 3)1 120.9074     6.6824  18.093  &lt; 2e-16 ***
## poly(rad, 3)2  17.4923     6.6824   2.618  0.00912 ** 
## poly(rad, 3)3   4.6985     6.6824   0.703  0.48231    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.682 on 502 degrees of freedom
## Multiple R-squared:    0.4,  Adjusted R-squared:  0.3965 
## F-statistic: 111.6 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.tax2 &lt;- lm(crim ~ poly(tax, 3))
summary(mod.fit.tax2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(tax, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.273  -1.389   0.046   0.536  76.950 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     3.6135     0.3047  11.860  &lt; 2e-16 ***
## poly(tax, 3)1 112.6458     6.8537  16.436  &lt; 2e-16 ***
## poly(tax, 3)2  32.0873     6.8537   4.682 3.67e-06 ***
## poly(tax, 3)3  -7.9968     6.8537  -1.167    0.244    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.854 on 502 degrees of freedom
## Multiple R-squared:  0.3689, Adjusted R-squared:  0.3651 
## F-statistic:  97.8 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.ptratio2 &lt;- lm(crim ~ poly(ptratio, 3))
summary(mod.fit.ptratio2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(ptratio, 3))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.833 -4.146 -1.655  1.408 82.697 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          3.614      0.361  10.008  &lt; 2e-16 ***
## poly(ptratio, 3)1   56.045      8.122   6.901 1.57e-11 ***
## poly(ptratio, 3)2   24.775      8.122   3.050  0.00241 ** 
## poly(ptratio, 3)3  -22.280      8.122  -2.743  0.00630 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.122 on 502 degrees of freedom
## Multiple R-squared:  0.1138, Adjusted R-squared:  0.1085 
## F-statistic: 21.48 on 3 and 502 DF,  p-value: 4.171e-13</code></pre>
<pre class="r"><code>mod.fit.black2 &lt;- lm(crim ~ poly(black, 3))
summary(mod.fit.black2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(black, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.096  -2.343  -2.128  -1.439  86.790 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.6135     0.3536  10.218   &lt;2e-16 ***
## poly(black, 3)1 -74.4312     7.9546  -9.357   &lt;2e-16 ***
## poly(black, 3)2   5.9264     7.9546   0.745    0.457    
## poly(black, 3)3  -4.8346     7.9546  -0.608    0.544    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.955 on 502 degrees of freedom
## Multiple R-squared:  0.1498, Adjusted R-squared:  0.1448 
## F-statistic: 29.49 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.lstat2 &lt;- lm(crim ~ poly(lstat, 3))
summary(mod.fit.lstat2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(lstat, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.234  -2.151  -0.486   0.066  83.353 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.6135     0.3392  10.654   &lt;2e-16 ***
## poly(lstat, 3)1  88.0697     7.6294  11.543   &lt;2e-16 ***
## poly(lstat, 3)2  15.8882     7.6294   2.082   0.0378 *  
## poly(lstat, 3)3 -11.5740     7.6294  -1.517   0.1299    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.629 on 502 degrees of freedom
## Multiple R-squared:  0.2179, Adjusted R-squared:  0.2133 
## F-statistic: 46.63 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>mod.fit.medv2 &lt;- lm(crim ~ poly(medv, 3))
summary(mod.fit.medv2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = crim ~ poly(medv, 3))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.427  -1.976  -0.437   0.439  73.655 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.614      0.292  12.374  &lt; 2e-16 ***
## poly(medv, 3)1  -75.058      6.569 -11.426  &lt; 2e-16 ***
## poly(medv, 3)2   88.086      6.569  13.409  &lt; 2e-16 ***
## poly(medv, 3)3  -48.033      6.569  -7.312 1.05e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.569 on 502 degrees of freedom
## Multiple R-squared:  0.4202, Adjusted R-squared:  0.4167 
## F-statistic: 121.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><em>For predictor variables zn, rm, rad, tax and lstat, the p-values suggest that the cubic coefficient is not statistically significant. For variables “indus”, “nox”, “age”, “dis”, “ptratio” and “medv” as predictor, the p-values suggest the adequacy of the cubic fit. For variables “black” as predictor, the p-values suggest that the quadratic and cubic coefficients are not statistically significant, so in this latter case no non-linear effect is visible.</em></p>
</div>
<div id="part-2" class="section level2">
<h2>Part 2</h2>
<p>Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In
other words, the logistic function representation and logit representation
for the logistic regression model are equivalent.</p>
<p>4.2 is:</p>
<p><span class="math display">\[p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span></p>
<p>and 4.3 is :</p>
<p><span class="math display">\[\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}\]</span></p>
<p>Solution:</p>
<p>Suppose we have, <span class="math inline">\(Y(X) = e^{\beta_0 + \beta_1 X}\)</span> in equation 4.2. Then <span class="math inline">\(p(X) = Y(X) / 1 + Y(X)\)</span> .</p>
<p>So, we have <span class="math inline">\(p(X) \left(1 + Y(X)\right) = Y(X)\)</span> which is equivalent to :</p>
<p><span class="math display">\[p(X) = Y(X) -  p(X) Y(X) = \left(1 - p(X)\right) Y(X)\]</span></p>
<p>If we then divide both sides by <span class="math inline">\(1 - p(X)\)</span> and replace the original value of <span class="math inline">\(Y(X)\)</span>, we then get equation 4.3.</p>
<p><span class="math display">\[\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}\]</span></p>
<p>This next question should be answered using the <span class="math inline">\({Weekly}\)</span> dataset, which
is part of the <span class="math inline">\({ISLR}\)</span> package. This data is similar in nature to the
<span class="math inline">\({Smarket}\)</span> data, except that it contains <span class="math inline">\({1, 089}\)</span> weekly returns for 21 years, from the beginning of 1990 to the end of 2010.</p>
<p>So, we will first produce some numerical and graphical summaries of the Weekly
data to if any pattern exists.</p>
<pre class="r"><code>library(ISLR)

data(Weekly)
head(Weekly)</code></pre>
<pre><code>##   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction
## 1 1990  0.816  1.572 -3.936 -0.229 -3.484 0.1549760 -0.270      Down
## 2 1990 -0.270  0.816  1.572 -3.936 -0.229 0.1485740 -2.576      Down
## 3 1990 -2.576 -0.270  0.816  1.572 -3.936 0.1598375  3.514        Up
## 4 1990  3.514 -2.576 -0.270  0.816  1.572 0.1616300  0.712        Up
## 5 1990  0.712  3.514 -2.576 -0.270  0.816 0.1537280  1.178        Up
## 6 1990  1.178  0.712  3.514 -2.576 -0.270 0.1544440 -1.372      Down</code></pre>
<pre class="r"><code>dim(Weekly)</code></pre>
<pre><code>## [1] 1089    9</code></pre>
<pre class="r"><code>summary(Weekly)</code></pre>
<pre><code>##       Year           Lag1               Lag2               Lag3         
##  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  
##  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  
##  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  
##  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  
##  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  
##  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  
##       Lag4               Lag5              Volume            Today         
##  Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747   Min.   :-18.1950  
##  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202   1st Qu.: -1.1540  
##  Median :  0.2380   Median :  0.2340   Median :1.00268   Median :  0.2410  
##  Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462   Mean   :  0.1499  
##  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373   3rd Qu.:  1.4050  
##  Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821   Max.   : 12.0260  
##  Direction 
##  Down:484  
##  Up  :605  
##            
##            
##            
## </code></pre>
<pre class="r"><code>cor(Weekly[,-9])</code></pre>
<pre><code>##               Year         Lag1        Lag2        Lag3         Lag4
## Year    1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923
## Lag1   -0.03228927  1.000000000 -0.07485305  0.05863568 -0.071273876
## Lag2   -0.03339001 -0.074853051  1.00000000 -0.07572091  0.058381535
## Lag3   -0.03000649  0.058635682 -0.07572091  1.00000000 -0.075395865
## Lag4   -0.03112792 -0.071273876  0.05838153 -0.07539587  1.000000000
## Lag5   -0.03051910 -0.008183096 -0.07249948  0.06065717 -0.075675027
## Volume  0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617
## Today  -0.03245989 -0.075031842  0.05916672 -0.07124364 -0.007825873
##                Lag5      Volume        Today
## Year   -0.030519101  0.84194162 -0.032459894
## Lag1   -0.008183096 -0.06495131 -0.075031842
## Lag2   -0.072499482 -0.08551314  0.059166717
## Lag3    0.060657175 -0.06928771 -0.071243639
## Lag4   -0.075675027 -0.06107462 -0.007825873
## Lag5    1.000000000 -0.05851741  0.011012698
## Volume -0.058517414  1.00000000 -0.033077783
## Today   0.011012698 -0.03307778  1.000000000</code></pre>
<pre class="r"><code>dim(cor(Weekly[,-9]))</code></pre>
<pre><code>## [1] 8 8</code></pre>
<pre class="r"><code>class(cor(Weekly[,-9]))</code></pre>
<pre><code>## [1] &quot;matrix&quot; &quot;array&quot;</code></pre>
<pre class="r"><code>sort(unlist(list(cor(Weekly[,-9]))),decreasing = T)[-c(1:8)]</code></pre>
<pre><code>##  [1]  0.841941619  0.841941619  0.060657175  0.060657175  0.059166717
##  [6]  0.059166717  0.058635682  0.058635682  0.058381535  0.058381535
## [11]  0.011012698  0.011012698 -0.007825873 -0.007825873 -0.008183096
## [16] -0.008183096 -0.030006492 -0.030006492 -0.030519101 -0.030519101
## [21] -0.031127923 -0.031127923 -0.032289274 -0.032289274 -0.032459894
## [26] -0.032459894 -0.033077783 -0.033077783 -0.033390009 -0.033390009
## [31] -0.058517414 -0.058517414 -0.061074617 -0.061074617 -0.064951313
## [36] -0.064951313 -0.069287712 -0.069287712 -0.071243639 -0.071243639
## [41] -0.071273876 -0.071273876 -0.072499482 -0.072499482 -0.074853051
## [46] -0.074853051 -0.075031842 -0.075031842 -0.075395865 -0.075395865
## [51] -0.075675027 -0.075675027 -0.075720913 -0.075720913 -0.085513141
## [56] -0.085513141</code></pre>
<pre class="r"><code>cat(&quot;There is great postive linear association with volume and year (volume increases with the time). seeing the different nature now:&quot;)</code></pre>
<pre><code>## There is great postive linear association with volume and year (volume increases with the time). seeing the different nature now:</code></pre>
<pre class="r"><code>plot(Volume~Year,data=Weekly,pch=16,cex=0.7,col=2,main=&quot;Weekly Data Plot&quot;)
data=Weekly[,c(1,7)]
library(plyr)
group=ddply(data,~Year,summarise,mean=mean(Volume),sd=sd(Volume))
lines(mean~Year,data=group,col=3,lwd=2)
epsilon = 0.2
for(i in 1:dim(group)[1]){
  x=group$Year
  up = group$mean[i] + group$sd[i]
  low = group$mean[i] - group$sd[i]
  segments(x[i],low , x[i], up,lwd=2,col=4)
  segments(x[i]-epsilon, up , x[i]+epsilon, up,lwd=2,col=4)
  segments(x[i]-epsilon, low , x[i]+epsilon, low,lwd=2,col=4)
}</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>library(ggplot2)
library(GGally)
data(Weekly)
summary(Weekly)</code></pre>
<pre><code>##       Year           Lag1               Lag2               Lag3         
##  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  
##  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  
##  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  
##  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  
##  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  
##  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  
##       Lag4               Lag5              Volume            Today         
##  Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747   Min.   :-18.1950  
##  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202   1st Qu.: -1.1540  
##  Median :  0.2380   Median :  0.2340   Median :1.00268   Median :  0.2410  
##  Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462   Mean   :  0.1499  
##  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373   3rd Qu.:  1.4050  
##  Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821   Max.   : 12.0260  
##  Direction 
##  Down:484  
##  Up  :605  
##            
##            
##            
## </code></pre>
<pre class="r"><code>cor(Weekly[, -9])</code></pre>
<pre><code>##               Year         Lag1        Lag2        Lag3         Lag4
## Year    1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923
## Lag1   -0.03228927  1.000000000 -0.07485305  0.05863568 -0.071273876
## Lag2   -0.03339001 -0.074853051  1.00000000 -0.07572091  0.058381535
## Lag3   -0.03000649  0.058635682 -0.07572091  1.00000000 -0.075395865
## Lag4   -0.03112792 -0.071273876  0.05838153 -0.07539587  1.000000000
## Lag5   -0.03051910 -0.008183096 -0.07249948  0.06065717 -0.075675027
## Volume  0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617
## Today  -0.03245989 -0.075031842  0.05916672 -0.07124364 -0.007825873
##                Lag5      Volume        Today
## Year   -0.030519101  0.84194162 -0.032459894
## Lag1   -0.008183096 -0.06495131 -0.075031842
## Lag2   -0.072499482 -0.08551314  0.059166717
## Lag3    0.060657175 -0.06928771 -0.071243639
## Lag4   -0.075675027 -0.06107462 -0.007825873
## Lag5    1.000000000 -0.05851741  0.011012698
## Volume -0.058517414  1.00000000 -0.033077783
## Today   0.011012698 -0.03307778  1.000000000</code></pre>
<pre class="r"><code>pairs(Weekly)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code>ggpairs(Weekly)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
<pre class="r"><code>attach(Weekly)
plot(Volume)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-11-4.png" width="672" /></p>
<pre class="r"><code>qplot(Volume, main=&quot;Plot of Volume&quot;)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-11-5.png" width="672" /></p>
<pre class="r"><code>#ggplot(Volume,aes(x=Index, y=Volume))+geom_point(size=1.5)</code></pre>
<p>In the figure above (<code>Weekly Data Plot</code>), the green line indicates the mean Volume in each Year, red dots are the original observations, and the blue lines indicate the standard deviations of the Volumes for each Year.</p>
<p>This analysis indicates that Year and Volume are highly correlated particularly after Year 1995 until the curve starts to decline after year 2010. Also, the variance change with respect to the change in mean volumne showing some positive relation with the mean Volume.</p>
<p>Now, we will use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. We will use the summary function to print the results and also examine if any of the predictors appear to be statistically significant. If so, we will also find them below.</p>
<pre class="r"><code>model.4.7.10.b=glm(Direction~.-Year-Today,data=Weekly,family = binomial)
summary(model.4.7.10.b)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Direction ~ . - Year - Today, family = binomial, 
##     data = Weekly)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6949  -1.2565   0.9913   1.0849   1.4579  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.26686    0.08593   3.106   0.0019 **
## Lag1        -0.04127    0.02641  -1.563   0.1181   
## Lag2         0.05844    0.02686   2.175   0.0296 * 
## Lag3        -0.01606    0.02666  -0.602   0.5469   
## Lag4        -0.02779    0.02646  -1.050   0.2937   
## Lag5        -0.01447    0.02638  -0.549   0.5833   
## Volume      -0.02274    0.03690  -0.616   0.5377   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1496.2  on 1088  degrees of freedom
## Residual deviance: 1486.4  on 1082  degrees of freedom
## AIC: 1500.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Based on the summary of the model, only Lag2 variable is statistically significant (at P&lt;0.05). The estimated coefficient for this variable is 0.05844, which means we would expect mean increase of the log odd in favor of the market going up by a unit increase in Lag2 variable, keeping other predictors in the model constant.</p>
<p>Here, we will compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.</p>
<pre class="r"><code>computeIT=function(TH,model,data){
  preds=rep(&quot;Down&quot;,dim(data)[1])
  vals=predict(model,newdata=data,type=&quot;response&quot;)
  for(i in 1:dim(data)[1]){
    if(vals[i]&gt;=TH){
      preds[i]=&quot;Up&quot;
    }
  }
  ## Confusion matrix
  cat(&quot;Confusion Matrix:&quot;)
  con=table(preds,data$Direction)
  print(con)
  cat(&quot;Model Accuracy (Percentage):&quot;)
  print(round(sum(preds==data$Direction)/dim(data)[1]*100,2))
  cat(&quot;True Postive Rate, TPR (percentage):&quot;)
  print(round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  cat(&quot;False Postive Rate, FPR (percentage):&quot;)
  spec=con[1,1]/(con[1,1]+con[2,1])*100
  print(round((100-spec),2))
  
}
computeIT(0.5,model.4.7.10.b,Weekly)</code></pre>
<pre><code>## Confusion Matrix:      
## preds  Down  Up
##   Down   54  48
##   Up    430 557
## Model Accuracy (Percentage):[1] 56.11
## True Postive Rate, TPR (percentage):[1] 92.07
## False Postive Rate, FPR (percentage):[1] 88.84</code></pre>
<p>In a good model we would expect large TPR and low FPR. In this analysis, using a threshold value of 0.5, the overall accuracy of the model is only 56.11%. Based on the confusion matrix, the True Positive Rate (TPR) of the model was calculated to be 92.07%, which is very good. The model gives 92.07% of the time right prediction, when the market goes up. But the thing that makes model very bad is the False Positive Rate (FRP). FPR for the model was calculated to be 88.84% (Specificity=11.16%) which means when the market is going Down, 88.84% of the time the model is predicting market to be going up, falsely.</p>
<p>Now, we will fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Then compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).</p>
<pre class="r"><code>index=c(which(Weekly$Year==2009), which(Weekly$Year==2010))
train=Weekly[-index,]
test=Weekly[index,]
model.4.7.10.d=glm(Direction~Lag2,data=train,family = binomial)
summary(model.4.7.10.d)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Direction ~ Lag2, family = binomial, data = train)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.536  -1.264   1.021   1.091   1.368  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.20326    0.06428   3.162  0.00157 **
## Lag2         0.05810    0.02870   2.024  0.04298 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1354.7  on 984  degrees of freedom
## Residual deviance: 1350.5  on 983  degrees of freedom
## AIC: 1354.5
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>## Calculations for held out data
summary(test$Direction)</code></pre>
<pre><code>## Down   Up 
##   43   61</code></pre>
<pre class="r"><code>computeIT(0.5,model.4.7.10.d,test)</code></pre>
<pre><code>## Confusion Matrix:      
## preds  Down Up
##   Down    9  5
##   Up     34 56
## Model Accuracy (Percentage):[1] 62.5
## True Postive Rate, TPR (percentage):[1] 91.8
## False Postive Rate, FPR (percentage):[1] 79.07</code></pre>
<p>For the held out data (data from 2009 and 2010), the calculations were done as before. At a threshold value of 0.5, the model accuracy was 62.5%. There were 43 Down and 61 Up cases in the held out data. The model gave a decent TPR value of 91.8%. However, it was reasonably bad in the sense of FPR, with a value of 79.07%.</p>
<p>In this problem, we will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.</p>
<p>We will first, create a binary variable, mpg01, that contains a 1 if mpg contains
a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note we may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.</p>
<pre class="r"><code>library(ISLR)
data(Auto)
mpg01=rep(NA,dim(Auto)[1])
med=median(Auto$mpg)
mpg01=ifelse(Auto$mpg&lt;med,0,1)
df=as.data.frame(cbind(Auto, mpg01))</code></pre>
<p>Here, we will explore the data graphically in order to investigate the association
between mpg01 and the other features. We will try to find the features that seem most likely to be useful in predicting mpg01. Scatterplots and boxplots may be useful tools to answer this question.</p>
<pre class="r"><code>k=c(2,3,4,5,6,7,8) ## taking just the relevant variables
layout(matrix(1:4,nrow = 2))
for(i in k){
  boxplot(df[,i]~df$mpg01,col=rainbow(7),xlab=&quot;mpg01&quot;,ylab=names(df)[i])
}</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>newdf=df[,c(2,3,4,5,6,7,8,10)] ## mpg and names dropped
plot(newdf,pch=16,cex=0.8,col=2)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-16-2.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-16-3.png" width="672" /></p>
<pre class="r"><code>plot(horsepower~displacement,Auto,pch=16,cex=0.8,col=2)

library(ggplot2)
library(GGally)
pairs(newdf) #pairwise correlation</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-16-4.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-16-5.png" width="672" /></p>
<pre class="r"><code>ggpairs(newdf,cardinality_threshold = 304)#ggpairs</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-16-6.png" width="672" /></p>
<pre class="r"><code>str(newdf)</code></pre>
<pre><code>## &#39;data.frame&#39;:    392 obs. of  8 variables:
##  $ cylinders   : num  8 8 8 8 8 8 8 8 8 8 ...
##  $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...
##  $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ weight      : num  3504 3693 3436 3433 3449 ...
##  $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year        : num  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin      : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ mpg01       : num  0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<p>Based on the plots, we see that there is a clear distinction between the distribution of cylinders in two groups. So is the case for horsepower, acceleration, origin, displacement, weight and year. US based cars are mostly condensed at lower mpg, whereas European and Japanese cars tend to be well distributed. Also, older cars tend to have lower mpg, and modern cars tend to have higher. We also expect correlation between the mechanical features of the cars, so from scatter plots it seems that there is very high correlations between the physical quantities.</p>
<p>Now, we will split the data into a training set and a test set.</p>
<pre class="r"><code>lowmpg=subset(newdf,mpg01==0)
highmpg=subset(newdf,mpg01==1)

set.seed(1235)
index1=sample((1:dim(lowmpg)[1]),size=round(0.5*dim(lowmpg)[1]),replace=F)
index2=sample((1:dim(highmpg)[1]),size=round(0.5*dim(highmpg)[1]),replace=F)

train1=lowmpg[index1,];   train2=highmpg[index2,] # training dataset 50%
test1=lowmpg[-index1,];   test2=highmpg[-index2,] # test dataset 50%

train=rbind(train1,train2)
test=rbind(test1,test2)</code></pre>
<p>The test and training data were created in the ration of 50:50 from high mpg and low mpg groups. Now, we will perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01. We will then calculate the test errors of the model obtained.</p>
<pre class="r"><code>model.4.7.11.f.1=glm(mpg01~.,train,family=binomial)
summary(model.4.7.11.f.1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = mpg01 ~ ., family = binomial, data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.28068  -0.03509   0.00396   0.11401   2.23756  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -20.105957  10.246866  -1.962   0.0497 *  
## cylinders      0.309334   0.676704   0.457   0.6476    
## displacement  -0.026788   0.018133  -1.477   0.1396    
## horsepower    -0.073805   0.041585  -1.775   0.0759 .  
## weight        -0.003055   0.001628  -1.877   0.0605 .  
## acceleration   0.029016   0.244067   0.119   0.9054    
## year           0.499403   0.125860   3.968 7.25e-05 ***
## origin         0.097692   0.519729   0.188   0.8509    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 271.714  on 195  degrees of freedom
## Residual deviance:  63.159  on 188  degrees of freedom
## AIC: 79.159
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<pre class="r"><code>showStats=function(cutoff,model,test){
  preds=rep(0,dim(test)[1])
  probs=predict(model,newdata=test, type=&quot;response&quot;)
  for(i in 1:length(probs)){
    if(probs[i]&gt;=cutoff){
      preds[i]=1
    }
  }
  cm=table(preds, test$mpg01)
  cat(&quot;Confusion Matrix:&quot;);print(cm)
  ac=((cm[1,1]+cm[2,2])/sum(cm))*100
  cat(&quot;Overall test accuracy (percentage) : &quot;, round(ac,2))
  cat(&quot;Misclassification rate (percantage): &quot;,round((100-ac),2))
  cat(&quot;True Postive Rate, TPR/Sensitivity (percentage): &quot;,
          round(cm[2,2]/(cm[2,2]+cm[1,2])*100,2))
  spec=cm[1,1]/(cm[1,1]+cm[2,1])*100
  cat(&quot;Specificity (percantage): &quot;,round(spec,2))
  cat(&quot;False Postive Rate, FPR (percentage): &quot;,round((100-spec),2))
}

showStats(0.5,model.4.7.11.f.1, test)</code></pre>
<pre><code>## Confusion Matrix:     
## preds  0  1
##     0 83 10
##     1 15 88
## Overall test accuracy (percentage) :  87.24Misclassification rate (percantage):  12.76True Postive Rate, TPR/Sensitivity (percentage):  89.8Specificity (percantage):  84.69False Postive Rate, FPR (percentage):  15.31</code></pre>
<pre class="r"><code>cat(&quot;Looking for influential correlation&quot;)</code></pre>
<pre><code>## Looking for influential correlation</code></pre>
<pre class="r"><code>library(car)
kable(vif(model.4.7.11.f.1), caption = &quot;VIF of model.4.7.11.f.1&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-18">Table 3: </span>VIF of model.4.7.11.f.1</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">cylinders</td>
<td align="right">4.666565</td>
</tr>
<tr class="even">
<td align="left">displacement</td>
<td align="right">6.512828</td>
</tr>
<tr class="odd">
<td align="left">horsepower</td>
<td align="right">2.377756</td>
</tr>
<tr class="even">
<td align="left">weight</td>
<td align="right">3.312294</td>
</tr>
<tr class="odd">
<td align="left">acceleration</td>
<td align="right">2.372670</td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="right">1.897841</td>
</tr>
<tr class="odd">
<td align="left">origin</td>
<td align="right">1.689610</td>
</tr>
</tbody>
</table>
<pre class="r"><code>model.4.7.11.f.2=glm(mpg01~.-displacement-cylinders,train,family=binomial)
summary(model.4.7.11.f.2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = mpg01 ~ . - displacement - cylinders, family = binomial, 
##     data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.59165  -0.03499   0.00251   0.14172   2.19944  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -19.652677   9.481877  -2.073   0.0382 *  
## horsepower    -0.077406   0.038028  -2.035   0.0418 *  
## weight        -0.004826   0.001298  -3.720   0.0002 ***
## acceleration   0.058313   0.236611   0.246   0.8053    
## year           0.508412   0.119857   4.242 2.22e-05 ***
## origin         0.649451   0.427815   1.518   0.1290    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 271.714  on 195  degrees of freedom
## Residual deviance:  66.134  on 190  degrees of freedom
## AIC: 78.134
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<pre class="r"><code>kable(vif(model.4.7.11.f.2), caption = &quot;VIF of model.4.7.11.f.2&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-18">Table 3: </span>VIF of model.4.7.11.f.2</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">horsepower</td>
<td align="right">2.090098</td>
</tr>
<tr class="even">
<td align="left">weight</td>
<td align="right">2.210212</td>
</tr>
<tr class="odd">
<td align="left">acceleration</td>
<td align="right">2.502255</td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="right">1.717593</td>
</tr>
<tr class="odd">
<td align="left">origin</td>
<td align="right">1.102758</td>
</tr>
</tbody>
</table>
<pre class="r"><code>showStats(0.5,model.4.7.11.f.2, test)</code></pre>
<pre><code>## Confusion Matrix:     
## preds  0  1
##     0 85  9
##     1 13 89
## Overall test accuracy (percentage) :  88.78Misclassification rate (percantage):  11.22True Postive Rate, TPR/Sensitivity (percentage):  90.82Specificity (percantage):  86.73False Postive Rate, FPR (percentage):  13.27</code></pre>
<pre class="r"><code>cat(&quot;model with weight and year&quot;)</code></pre>
<pre><code>## model with weight and year</code></pre>
<pre class="r"><code>model.4.7.11.f.3=glm(mpg01~weight+year,train,family=binomial)
kable(vif(model.4.7.11.f.3), caption = &quot;VIF of model.4.7.11.f.3&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-18">Table 3: </span>VIF of model.4.7.11.f.3</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">weight</td>
<td align="right">1.589399</td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="right">1.589399</td>
</tr>
</tbody>
</table>
<pre class="r"><code>summary(model.4.7.11.f.3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = mpg01 ~ weight + year, family = binomial, data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.41257  -0.10759   0.00992   0.20392   2.26494  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.106e+01  6.859e+00  -3.071  0.00214 ** 
## weight      -5.589e-03  9.485e-04  -5.893 3.79e-09 ***
## year         4.849e-01  1.073e-01   4.519 6.21e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 271.714  on 195  degrees of freedom
## Residual deviance:  79.114  on 193  degrees of freedom
## AIC: 85.114
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>showStats(0.5,model.4.7.11.f.3, test)</code></pre>
<pre><code>## Confusion Matrix:     
## preds  0  1
##     0 87  8
##     1 11 90
## Overall test accuracy (percentage) :  90.31Misclassification rate (percantage):  9.69True Postive Rate, TPR/Sensitivity (percentage):  91.84Specificity (percantage):  88.78False Postive Rate, FPR (percentage):  11.22</code></pre>
<p>For first model (<code>model.4.7.11.f.1</code>, selecting all variables), the summary statistics showed statistical significance of just two variables, year (p-value nearly 0) and weight (p-value 0.007). No statistical significance was found for others at any default level. Still, an attempt was made to use this model for prediction on the test data. With a cutoff 0.5, the overall accuracy of the model on the test data was 90.31%. TPR and FPR were also reasonable, with values of 91.84% and 11.22% respectively. As seen earlier, there was correlation between the variables. To treat this, variance inflation factor (VIF) of the full model was examined. Using a cutoff of 10 for VIF, two variables, cylinders and displacement were deleted from the model because their corresponding VIFs were 6.05 and 8.94, respectively. Here as well (<code>model.4.7.11.f.1</code>), we see that year and weight variables were statistically significant. The VIF values looked okay this time. This model returned identical prediction as before. But there was decrease in the standard errors of the estimated coefficients as expected since we made collinearity treatment by deleting cylinders and displacement from the model.</p>
<p>The third model (<code>model.4.7.11.f.1</code>) was built, with only year and weight. Both were the only two variables significant in the previous two models. The estimated coefficients were not dramatically different from previous two, but there is a good improvement in the standard errors of the estimated coefficients. In this third model, we see some improvement in the overall prediction. Overall accuracy on the test data was found to be 91.84%, whereas previously it was 90.31%. TPR remained same. But FPR reduced to 8.16% from 11.22%.</p>
<p>Here, we will write a function in RMD that calculates the misclassification rate, sensitivity, and specificity. The inputs for this function are a cutoff point, predicted probabilities, and original binary response.</p>
<pre class="r"><code>cutoff &lt;- 0.5
pred_prob &lt;- predict(model.4.7.10.b,newdata=Weekly, type=&quot;response&quot;)
original &lt;- Weekly$Direction

Function.for.Q.4 = function(cutoff, pred_prob, original) {
  predicted=rep(&quot;Down&quot;,length(original))
  vals &lt;- pred_prob
  for(i in 1:length(original)){
    if(vals[i]&gt;=cutoff){
      predicted[i]=&quot;Up&quot;
    }
  }
  
  con.mat = table(predicted, original) # creating confusion matrix
  MCR = (con.mat[1, 2] + con.mat[2, 1]) / sum(con.mat) ## misclassification rate
  TPR = con.mat[2, 2] / (con.mat[2, 2] + con.mat[1, 2]) ## sensitivity= (yes,yes)/(true yes)
  SPEC = con.mat[1, 1] / (con.mat[1, 1] + con.mat[2, 1]) ## specificity= (no,no)/(true no)
  ## now return as a list
  return(list(
    Misclassification_Rate = MCR,
    Sensitivity = TPR,
    Specificity = SPEC
  ))
}


Function.for.Q.4(cutoff, pred_prob, original)</code></pre>
<pre><code>## $Misclassification_Rate
## [1] 0.4389348
## 
## $Sensitivity
## [1] 0.9206612
## 
## $Specificity
## [1] 0.1115702</code></pre>
</div>
<div id="part-3" class="section level2">
<h2>Part 3</h2>
<p>We now examine the differences between LDA and QDA.</p>
<p>If the Bayes decision boundary is linear, do we expect LDA or
QDA to perform better on the training set? On the test set?</p>
<p>Answer:</p>
<p>we expect QDA to perform better on the training set if the Bayes decision boundary is linear because its higher flexiblity may yield a closer fit. On the test set, we expect LDA to perform better than QDA, because QDA could overfit the linearity on the Bayes decision boundary.</p>
<p>If the Bayes decision boundary is non-linear, do we expect LDA
or QDA to perform better on the training set? On the test set?</p>
<p>Answer:</p>
<p>We should expect QDA to perform better both on the training and test sets if the Bayes decision boundary is non-linear.</p>
<p>In general, as the sample size n increases, do we expect the test
prediction accuracy of QDA relative to LDA to improve, decline,
or be unchanged? Why?</p>
<p>Answer:</p>
<p>Generally, QDA (which is more flexible than LDA and so has higher variance) is recommended if the training set is very large, so that the variance of the classifier is not a major concern.</p>
<p>Note: This exercise will need in-depth understanding of <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" class="uri">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></p>
<p>Next, we will compare the logistic regression we performed in Part 2 above with the LDA analysis below using <code>Lag2</code> as th only predictor.</p>
<p>Before we perform this analysis, there are few things we need to understand. Such as, discriminant analysis assumes a multivariate normal distribution because what we usually consider to be predictors are really a multivariate dependent variable, and the grouping variable is considered to be a predictor. This means that categorical variables that are to be treated as predictors in the sense you wish are not handled well. This is one reason that many, including myself, consider discriminant analysis to have been made obsolete by logistic regression. Logistic regression makes no distributional assumptions of any kind, on either the left hand or the right hand side of the model. Logistic regression is a direct probability model and doesn’t require one to use Bayes’ rule to convert results to probabilities as does discriminant analysis.</p>
<p>Sources:
(Dr. Harell: <a href="https://stats.stackexchange.com/questions/158772/can-we-use-categorical-independent-variable-in-discriminant-analysis" class="uri">https://stats.stackexchange.com/questions/158772/can-we-use-categorical-independent-variable-in-discriminant-analysis</a>)</p>
<p>Also, see: <a href="https://stats.stackexchange.com/questions/95247/logistic-regression-vs-lda-as-two-class-classifiers" class="uri">https://stats.stackexchange.com/questions/95247/logistic-regression-vs-lda-as-two-class-classifiers</a></p>
<p>Now fit the LDA using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).</p>
<pre class="r"><code>library(MASS)
library(ISLR)

data(Weekly)
index=c(which(Weekly$Year==2009), which(Weekly$Year==2010))
train=Weekly[-index,]
test=Weekly[index,]
model4.7.10.e=lda(Direction~Lag2,data=train)


computeLDA=function(model,data){
  preds=(predict(model,newdata=data,type=&quot;response&quot;))$class
  cat(&quot;Confusion matrix:&quot;)
  con=table(preds,data$Direction)
  print(con)
  cat(&quot;Model Accuracy (Percentage):&quot;)
  print(round(sum(preds==data$Direction)/dim(data)[1]*100,2))
  cat(&quot;True Positive Rate, TPR (percentage):&quot;)
  print(round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  cat(&quot;False Postive Rate, FPR (percentage):&quot;)
  spec=con[1,1]/(con[1,1]+con[2,1])*100
  print(round((100-spec),2))
  
}

computeLDA(model4.7.10.e,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    9  5
##   Up     34 56
## Model Accuracy (Percentage):[1] 62.5
## True Positive Rate, TPR (percentage):[1] 91.8
## False Postive Rate, FPR (percentage):[1] 79.07</code></pre>
<p>In this case, Just like in Part 2(d) with logistic regression, at a threshold value of 0.5, the model accuracy or the percentage of correct prediction was 62.5%. In other words 37.5% is the test error rate. We could also say that for weeks when the market goes up, the model is right 91.8032787% of the time. For weeks when the market goes down, the model is right only 20.9302326% of the time. These results are very close to those obtained with the logistic regression model which is not surpising.</p>
<p>Now we will repeat the analysis above, but with QDA.</p>
<pre class="r"><code>model.4.7.10.f=qda(Direction~Lag2,data=train)
computeLDA(model.4.7.10.f,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    0  0
##   Up     43 61
## Model Accuracy (Percentage):[1] 58.65
## True Positive Rate, TPR (percentage):[1] 100
## False Postive Rate, FPR (percentage):[1] 100</code></pre>
<pre class="r"><code># #check with caret function
# tt &lt;- predict(model.4.7.10.f,newdata=test,type=&quot;response&quot;)
# gg &lt;- confusionMatrix(tt$class, reference = test$Direction)
# gg</code></pre>
<p>In this case, the QDA model predicted all up, which is bad. The overall accuracy, TPR and FPR were 58.65%, 100% and 100%, respectively</p>
<p>Again, repeat using KNN with K = 1.</p>
<pre class="r"><code>computeKNN=function(model,trues){
  ## Confusion matrix
  print(&quot;Confusion Matrix:&quot;)
  con=table(model,trues)
  print(con)
  print(&quot;Model Accuracy (Percentage):&quot;)
  print(round(((con[1,1]+con[2,2])/sum(con))*100,2))
  print(&quot;True Positive Rate, TPR (percentage):&quot;)
  print(round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  print(&quot;False Postive Rate, FPR (percentage):&quot;)
  spec=con[1,1]/(con[1,1]+con[2,1])*100
  print(round((100-spec),2))
}

library(class)
attach(Weekly)
head(Weekly)</code></pre>
<pre><code>##   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction
## 1 1990  0.816  1.572 -3.936 -0.229 -3.484 0.1549760 -0.270      Down
## 2 1990 -0.270  0.816  1.572 -3.936 -0.229 0.1485740 -2.576      Down
## 3 1990 -2.576 -0.270  0.816  1.572 -3.936 0.1598375  3.514        Up
## 4 1990  3.514 -2.576 -0.270  0.816  1.572 0.1616300  0.712        Up
## 5 1990  0.712  3.514 -2.576 -0.270  0.816 0.1537280  1.178        Up
## 6 1990  1.178  0.712  3.514 -2.576 -0.270 0.1544440 -1.372      Down</code></pre>
<pre class="r"><code>k_train = (Year &lt; 2009)

knn_train = as.matrix(Lag2[k_train])
knn_test = as.matrix(Lag2[!k_train])
train_class = Direction[k_train]

model4.7.10.g=knn(knn_train, knn_test, cl=train_class, k = 1)
computeKNN(model4.7.10.g, test$Direction)</code></pre>
<pre><code>## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   21 29
##   Up     22 32
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 50.96
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 52.46
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 51.16</code></pre>
<pre class="r"><code># # #check with caret function
# gg &lt;- confusionMatrix(model4.7.10.g, reference = test$Direction)
# gg</code></pre>
<p>With K=1, the KNN classification did not do a good job in the test set. The overall accuracy, TPR and FPR were 50%, 50.82% and 51.16% respectively.</p>
<p>If we compare the test error rates, we see that logistic regression and LDA have the minimum error rates, followed by QDA and KNN.</p>
<p>Now, we will experiment with different combinations of predictors, including
possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for
K in the KNN classifier.</p>
<pre class="r"><code>## Since Lag1 and lag2 had the lowest p-vales, so trying with both
for(i in 1:7){
  plot(train$Direction~train[,i],xlab=names(train)[i],ylab=&quot;Direction&quot;,
       main=&quot;Train Data&quot;)
}</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-1.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-2.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-3.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-4.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-5.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-6.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-23-7.png" width="672" /></p>
<pre class="r"><code>## trying with different combinations
formula1=Direction~Lag1
formula2=Direction~Lag2
formula3=Direction~Lag1+Lag2
formula4=Direction~Lag1+Lag2+Lag1*Lag2
formula5=Direction~Lag2+Lag5
formula6=Direction~Lag2+Lag5+Volume
formula7=Direction~Lag2+Volume
formula8=Direction~Lag2+I(Lag2^2)


## All the LDA mdoels 
ldamodel1=lda(formula1,data=train)
ldamodel2=lda(formula2,data=train)
ldamodel3=lda(formula3,data=train)
ldamodel4=lda(formula4,data=train)
ldamodel5=lda(formula5,data=train)
ldamodel6=lda(formula6,data=train)
ldamodel7=lda(formula7,data=train)
ldamodel8=lda(formula8,data=train)
## Results
computeLDA(ldamodel1,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    4  6
##   Up     39 55
## Model Accuracy (Percentage):[1] 56.73
## True Positive Rate, TPR (percentage):[1] 90.16
## False Postive Rate, FPR (percentage):[1] 90.7</code></pre>
<pre class="r"><code>computeLDA(ldamodel2,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    9  5
##   Up     34 56
## Model Accuracy (Percentage):[1] 62.5
## True Positive Rate, TPR (percentage):[1] 91.8
## False Postive Rate, FPR (percentage):[1] 79.07</code></pre>
<pre class="r"><code>computeLDA(ldamodel3,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    7  8
##   Up     36 53
## Model Accuracy (Percentage):[1] 57.69
## True Positive Rate, TPR (percentage):[1] 86.89
## False Postive Rate, FPR (percentage):[1] 83.72</code></pre>
<pre class="r"><code>computeLDA(ldamodel4,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    7  8
##   Up     36 53
## Model Accuracy (Percentage):[1] 57.69
## True Positive Rate, TPR (percentage):[1] 86.89
## False Postive Rate, FPR (percentage):[1] 83.72</code></pre>
<pre class="r"><code>computeLDA(ldamodel5,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    6  5
##   Up     37 56
## Model Accuracy (Percentage):[1] 59.62
## True Positive Rate, TPR (percentage):[1] 91.8
## False Postive Rate, FPR (percentage):[1] 86.05</code></pre>
<pre class="r"><code>computeLDA(ldamodel6,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down   23 33
##   Up     20 28
## Model Accuracy (Percentage):[1] 49.04
## True Positive Rate, TPR (percentage):[1] 45.9
## False Postive Rate, FPR (percentage):[1] 46.51</code></pre>
<pre class="r"><code>computeLDA(ldamodel7,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down   20 25
##   Up     23 36
## Model Accuracy (Percentage):[1] 53.85
## True Positive Rate, TPR (percentage):[1] 59.02
## False Postive Rate, FPR (percentage):[1] 53.49</code></pre>
<pre class="r"><code>computeLDA(ldamodel8,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    7  4
##   Up     36 57
## Model Accuracy (Percentage):[1] 61.54
## True Positive Rate, TPR (percentage):[1] 93.44
## False Postive Rate, FPR (percentage):[1] 83.72</code></pre>
<pre class="r"><code>## All the QDA mdoels 
qdamodel1=qda(formula1,data=train)
qdamodel2=qda(formula2,data=train)
qdamodel3=qda(formula3,data=train)
qdamodel4=qda(formula4,data=train)
qdamodel5=qda(formula5,data=train)
qdamodel6=qda(formula6,data=train)
qdamodel7=qda(formula7,data=train)
qdamodel8=qda(formula8,data=train)
## Results (we can use the same function here)
computeLDA(qdamodel1,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    0  0
##   Up     43 61
## Model Accuracy (Percentage):[1] 58.65
## True Positive Rate, TPR (percentage):[1] 100
## False Postive Rate, FPR (percentage):[1] 100</code></pre>
<pre class="r"><code>computeLDA(qdamodel2,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    0  0
##   Up     43 61
## Model Accuracy (Percentage):[1] 58.65
## True Positive Rate, TPR (percentage):[1] 100
## False Postive Rate, FPR (percentage):[1] 100</code></pre>
<pre class="r"><code>computeLDA(qdamodel3,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    7 10
##   Up     36 51
## Model Accuracy (Percentage):[1] 55.77
## True Positive Rate, TPR (percentage):[1] 83.61
## False Postive Rate, FPR (percentage):[1] 83.72</code></pre>
<pre class="r"><code>computeLDA(qdamodel4,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down   23 36
##   Up     20 25
## Model Accuracy (Percentage):[1] 46.15
## True Positive Rate, TPR (percentage):[1] 40.98
## False Postive Rate, FPR (percentage):[1] 46.51</code></pre>
<pre class="r"><code>computeLDA(qdamodel5,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    3 11
##   Up     40 50
## Model Accuracy (Percentage):[1] 50.96
## True Positive Rate, TPR (percentage):[1] 81.97
## False Postive Rate, FPR (percentage):[1] 93.02</code></pre>
<pre class="r"><code>computeLDA(qdamodel6,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down   31 42
##   Up     12 19
## Model Accuracy (Percentage):[1] 48.08
## True Positive Rate, TPR (percentage):[1] 31.15
## False Postive Rate, FPR (percentage):[1] 27.91</code></pre>
<pre class="r"><code>computeLDA(qdamodel7,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down   32 44
##   Up     11 17
## Model Accuracy (Percentage):[1] 47.12
## True Positive Rate, TPR (percentage):[1] 27.87
## False Postive Rate, FPR (percentage):[1] 25.58</code></pre>
<pre class="r"><code>computeLDA(qdamodel8,test)</code></pre>
<pre><code>## Confusion matrix:      
## preds  Down Up
##   Down    7  3
##   Up     36 58
## Model Accuracy (Percentage):[1] 62.5
## True Positive Rate, TPR (percentage):[1] 95.08
## False Postive Rate, FPR (percentage):[1] 83.72</code></pre>
<pre class="r"><code>## KNN
i=1
ks=c(1,3,5,10,20,50,100)
for(i in ks){
  print(&quot;########################################&quot;)
  print(paste0(&quot;K = &quot;,i))
  computeKNN(knn(knn_train, knn_test, cl=train_class, k = i), test$Direction)
  print(&quot;########################################&quot;)
}</code></pre>
<pre><code>## [1] &quot;########################################&quot;
## [1] &quot;K = 1&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   21 29
##   Up     22 32
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 50.96
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 52.46
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 51.16
## [1] &quot;########################################&quot;
## [1] &quot;########################################&quot;
## [1] &quot;K = 3&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   15 20
##   Up     28 41
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 53.85
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 67.21
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 65.12
## [1] &quot;########################################&quot;
## [1] &quot;########################################&quot;
## [1] &quot;K = 5&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   16 21
##   Up     27 40
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 53.85
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 65.57
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 62.79
## [1] &quot;########################################&quot;
## [1] &quot;########################################&quot;
## [1] &quot;K = 10&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   19 18
##   Up     24 43
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 59.62
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 70.49
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 55.81
## [1] &quot;########################################&quot;
## [1] &quot;########################################&quot;
## [1] &quot;K = 20&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   19 21
##   Up     24 40
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 56.73
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 65.57
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 55.81
## [1] &quot;########################################&quot;
## [1] &quot;########################################&quot;
## [1] &quot;K = 50&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   20 22
##   Up     23 39
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 56.73
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 63.93
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 53.49
## [1] &quot;########################################&quot;
## [1] &quot;########################################&quot;
## [1] &quot;K = 100&quot;
## [1] &quot;Confusion Matrix:&quot;
##       trues
## model  Down Up
##   Down   10 12
##   Up     33 49
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 56.73
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 80.33
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 76.74
## [1] &quot;########################################&quot;</code></pre>
<p>Different combination of predictors with possible transformations and interactions were done. LDA and QDA model and associated confusion matrix was developed, and finally K in KNN classifier was done. The output is shown above with all combinations.</p>
<p>Next, we will perform LDA on the training data in order to predict mpg01
using the variables that seemed most associated with mpg01. We will then calculate the the test error of the model obtained.</p>
<pre class="r"><code>data(Auto)
mpg01=rep(NA,dim(Auto)[1])
med=median(Auto$mpg)
mpg01=ifelse(Auto$mpg&lt;med,0,1)
df=as.data.frame(cbind(Auto, mpg01))

k=c(2,3,4,5,6,7,8) ## taking just the relevant variables
layout(matrix(1:4,nrow = 2))
for(i in k){
  boxplot(df[,i]~df$mpg01,col=rainbow(7),xlab=&quot;mpg01&quot;,ylab=names(df)[i])
}</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>newdf=df[,c(2,3,4,5,6,7,8,10)] ## mpg and names dropped
plot(newdf,pch=16,cex=0.8,col=2)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-24-2.png" width="672" /><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-24-3.png" width="672" /></p>
<pre class="r"><code>plot(horsepower~displacement,Auto,pch=16,cex=0.8,col=2)


lowmpg=subset(newdf,mpg01==0)
highmpg=subset(newdf,mpg01==1)
set.seed(1235)
index1=sample((1:dim(lowmpg)[1]),size=round(0.5*dim(lowmpg)[1]),replace=F)
index2=sample((1:dim(highmpg)[1]),size=round(0.5*dim(highmpg)[1]),replace=F)
train1=lowmpg[index1,];   train2=highmpg[index2,]
test1=lowmpg[-index1,];   test2=highmpg[-index2,]
autotrain=rbind(train1,train2)
autotest=rbind(test1,test2)


auto_formula=mpg01~cylinders+horsepower+acceleration+origin+displacement+weight+year 
auto_model_lda=lda(auto_formula,data=autotrain)

computeForAutoDA=function(model,test){
  trues=test$mpg01
  preds=(predict(model,newdata=test,type=&quot;response&quot;))$class
  con=table(preds,trues)
  print(&quot;Confusion Matrix:&quot;)
  print(con)
  print(&quot;Model Accuracy (Percentage):&quot;)
  print(round((con[1,1]+con[2,2])/sum(con)*100,2))
  print(&quot;True Positive Rate, TPR (percentage):&quot;)
  print(round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  print(&quot;False Postive Rate, FPR (percentage):&quot;)
  spec=con[1,1]/(con[1,1]+con[2,1])*100
  print(round((100-spec),2))
}
computeForAutoDA(auto_model_lda,autotest)</code></pre>
<pre><code>## [1] &quot;Confusion Matrix:&quot;
##      trues
## preds  0  1
##     0 84  4
##     1 14 94
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 90.82
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 95.92
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 14.29</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-24-4.png" width="672" /></p>
<p>With the variables listed above, LDA was performed on the training set. Prediction was performed in the test data. The results are listed above. So, the test error rate is 100-91.84 = 8.16 %</p>
<p>Now we will perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01. We will then calculate the the test error of the model obtained.</p>
<pre class="r"><code>auto_model_qda=qda(auto_formula,data=autotrain)
computeForAutoDA(auto_model_qda,autotest)</code></pre>
<pre><code>## [1] &quot;Confusion Matrix:&quot;
##      trues
## preds  0  1
##     0 84  4
##     1 14 94
## [1] &quot;Model Accuracy (Percentage):&quot;
## [1] 90.82
## [1] &quot;True Positive Rate, TPR (percentage):&quot;
## [1] 95.92
## [1] &quot;False Postive Rate, FPR (percentage):&quot;
## [1] 14.29</code></pre>
<p>The test error rate is 100-90.31 = 9.69 % a little higher that the LDA.</p>
<p>Next, we will perform KNN on the training data, with several values of K, in
order to predict mpg01. Use only the variables that seemed most associated with mpg01. We will then calculate the test errors, and also see which value of <span class="math inline">\(K\)</span> seems to perform the best on this data set.</p>
<pre class="r"><code>library(class)

sel.variables &lt;- which(names(autotrain)%in%c(&quot;mpg01&quot;, &quot;displacement&quot;, &quot;horsepower&quot;, &quot;weight&quot;, &quot;acceleration&quot;, &quot;year&quot;, &quot;cylinders&quot;, &quot;origin&quot;))

set.seed(1)
accuracies &lt;- data.frame(&quot;k&quot;=1:10, acc=NA)
for(k in 1:10){
  knn.pred &lt;- knn(train=autotrain[, sel.variables], test=autotest[, sel.variables], cl=autotrain$mpg01, k=k)
  
  # test-error
  accuracies$acc[k]= round(sum(knn.pred!=autotest$mpg01)/nrow(autotest)*100,2)
}

accuracies</code></pre>
<pre><code>##     k   acc
## 1   1 10.20
## 2   2 12.24
## 3   3 11.22
## 4   4 13.27
## 5   5 11.73
## 6   6 12.24
## 7   7  9.69
## 8   8 11.73
## 9   9 11.73
## 10 10 10.71</code></pre>
<p>The error on the different k values are given on the output above.
Here k=1 and k=7 was the best response out of all having lowest error rate.</p>
<p>We we will explore this <a href="https://archive.ics.uci.edu/ml/datasets.html;%20https://archive.ics.uci.edu/ml/">website</a> that contains open datasets that are used in machine learning.</p>
<p>I am interested in <code>iris</code> dataset available on this website <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">website</a>. It is one the most popular datasets in data science and have been widely used in pattern recognition literature. It is a small dataset with 150 samples, 3 categories of species and 4 features with the sepal length, sepal width, petal length and petal width (in cm). The classification problem in this dataset could be about predicting whether the given flower comes from which particular species or belongs to which species. This classification problem can be solved by developing a model which could be used to predict the flower by first training our model with the features and the labels.</p>
<p>We will try to analyze this data using the KNN or k-nearest neighbors. We can first read our data and check some basic statistics:</p>
<pre class="r"><code># Read in `iris` data
iris &lt;- read.csv(url(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;), header = FALSE) 

str(iris)</code></pre>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ V1: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ V2: num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ V3: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ V4: num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ V5: chr  &quot;Iris-setosa&quot; &quot;Iris-setosa&quot; &quot;Iris-setosa&quot; &quot;Iris-setosa&quot; ...</code></pre>
<pre class="r"><code># Add column names
names(iris) &lt;- c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;, &quot;Species&quot;)


# Division of `Species`
table(iris$Species) </code></pre>
<pre><code>## 
##     Iris-setosa Iris-versicolor  Iris-virginica 
##              50              50              50</code></pre>
<pre class="r"><code># Percentual division of `Species`
round(prop.table(table(iris$Species)) * 100, digits = 1)</code></pre>
<pre><code>## 
##     Iris-setosa Iris-versicolor  Iris-virginica 
##            33.3            33.3            33.3</code></pre>
<pre class="r"><code># Check the result
# iris
head(iris)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width     Species
## 1          5.1         3.5          1.4         0.2 Iris-setosa
## 2          4.9         3.0          1.4         0.2 Iris-setosa
## 3          4.7         3.2          1.3         0.2 Iris-setosa
## 4          4.6         3.1          1.5         0.2 Iris-setosa
## 5          5.0         3.6          1.4         0.2 Iris-setosa
## 6          5.4         3.9          1.7         0.4 Iris-setosa</code></pre>
<pre class="r"><code>library(ggplot2)
# Iris scatter plot
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col = Species)) +
  geom_point()</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code>ggplot(iris, aes(x=Petal.Length, y=Petal.Width, col = Species)) +
  geom_point()</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-27-2.png" width="672" /></p>
<pre class="r"><code>ggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +
  geom_point() + 
  labs(title = &quot;Scatter plot of all variables for petals&quot;)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-27-3.png" width="672" /></p>
<pre class="r"><code># Overall correlation `Petal.Length` and `Petal.Width`
cor(iris$Petal.Length, iris$Petal.Width)</code></pre>
<pre><code>## [1] 0.9627571</code></pre>
<pre class="r"><code># Return values of `iris` levels 
x=levels(iris$Species)

cat(&quot;Print Setosa correlation matrix&quot;)</code></pre>
<pre><code>## Print Setosa correlation matrix</code></pre>
<pre class="r"><code>print(x[1])</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>cor(iris[iris$Species==x[1],1:4])</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length           NA          NA           NA          NA
## Sepal.Width            NA          NA           NA          NA
## Petal.Length           NA          NA           NA          NA
## Petal.Width            NA          NA           NA          NA</code></pre>
<pre class="r"><code>cat(&quot;Print Versicolor correlation matrix&quot;)</code></pre>
<pre><code>## Print Versicolor correlation matrix</code></pre>
<pre class="r"><code>print(x[2])</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>cor(iris[iris$Species==x[2],1:4])</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length           NA          NA           NA          NA
## Sepal.Width            NA          NA           NA          NA
## Petal.Length           NA          NA           NA          NA
## Petal.Width            NA          NA           NA          NA</code></pre>
<pre class="r"><code>cat(&quot;Print Virginica correlation matrix&quot;)</code></pre>
<pre><code>## Print Virginica correlation matrix</code></pre>
<pre class="r"><code>print(x[3])</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>cor(iris[iris$Species==x[3],1:4])</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length           NA          NA           NA          NA
## Sepal.Width            NA          NA           NA          NA
## Petal.Length           NA          NA           NA          NA
## Petal.Width            NA          NA           NA          NA</code></pre>
<p>We can see that there is a high correlation between the sepal length and the sepal width of the Iris-setosa flowers, however Virginica and Versicolor flowers are somewhat less correlated. Based on the second plot, we can tell that the petal length and petal width are more correlated for all three Species. When we further analyze this data with all three species together, we can see that the correlation gets a bit stronger than it is when we examine different species separately (R = 0.96).</p>
<p>Now, we can use this iris dataset to predict the species variable using KNN regression model. We can also normalize our dataset before doing the analysis.</p>
<pre class="r"><code># create Normalize() function
normalize &lt;- function(x) {
num &lt;- x - min(x)
denom &lt;- max(x) - min(x)
return (num/denom)
}

# Normalize the `iris` data
iris_norm &lt;- as.data.frame(lapply(iris[1:4], normalize))

# Summarize 
kable(summary(iris_norm),caption = &quot;Summary of normalized iris data&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-28">Table 4: </span>Summary of normalized iris data</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Sepal.Length</th>
<th align="left">Sepal.Width</th>
<th align="left">Petal.Length</th>
<th align="left">Petal.Width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left">Min. :0.0000</td>
<td align="left">Min. :0.0000</td>
<td align="left">Min. :0.0000</td>
<td align="left">Min. :0.00000</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">1st Qu.:0.2222</td>
<td align="left">1st Qu.:0.3333</td>
<td align="left">1st Qu.:0.1017</td>
<td align="left">1st Qu.:0.08333</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">Median :0.4167</td>
<td align="left">Median :0.4167</td>
<td align="left">Median :0.5678</td>
<td align="left">Median :0.50000</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Mean :0.4287</td>
<td align="left">Mean :0.4392</td>
<td align="left">Mean :0.4676</td>
<td align="left">Mean :0.45778</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">3rd Qu.:0.5833</td>
<td align="left">3rd Qu.:0.5417</td>
<td align="left">3rd Qu.:0.6949</td>
<td align="left">3rd Qu.:0.70833</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Max. :1.0000</td>
<td align="left">Max. :1.0000</td>
<td align="left">Max. :1.0000</td>
<td align="left">Max. :1.00000</td>
</tr>
</tbody>
</table>
<p>Now, we partition our data into test (33.33%) and training sets(66.66%).</p>
<pre class="r"><code>set.seed(54321)
ind &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))


# Create training labels
iris.trainLabels &lt;- iris[ind==1,5]

# Compose `iris` test labels
iris.testLabels &lt;- iris[ind==2, 5]

# Now we can create the training and test datasets
# create training set
iris.training &lt;- iris[ind==1, 1:4]

cat(&quot;Top 6 rows of training set&quot;)</code></pre>
<pre><code>## Top 6 rows of training set</code></pre>
<pre class="r"><code>head(iris.training)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
## 2          4.9         3.0          1.4         0.2
## 3          4.7         3.2          1.3         0.2
## 4          4.6         3.1          1.5         0.2
## 5          5.0         3.6          1.4         0.2
## 7          4.6         3.4          1.4         0.3</code></pre>
<pre class="r"><code># Compose test set
iris.test &lt;- iris[ind==2, 1:4]

cat(&quot;Top 6 rows of test set&quot;)</code></pre>
<pre><code>## Top 6 rows of test set</code></pre>
<pre class="r"><code>head(iris.test)</code></pre>
<pre><code>##    Sepal.Length Sepal.Width Petal.Length Petal.Width
## 6           5.4         3.9          1.7         0.4
## 12          4.8         3.4          1.6         0.2
## 15          5.8         4.0          1.2         0.2
## 16          5.7         4.4          1.5         0.4
## 17          5.4         3.9          1.3         0.4
## 23          4.6         3.6          1.0         0.2</code></pre>
<pre class="r"><code>library(gmodels)
library(class)
# Build the model
iris_pred &lt;- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)

# Put `iris.testLabels` in a data frame
irisTestLabels &lt;- data.frame(iris.testLabels)

# Merge `iris_pred` and `iris.testLabels` 
merge &lt;- data.frame(iris_pred, iris.testLabels)

# Specify column names for `merge`
names(merge) &lt;- c(&quot;Predicted Species&quot;, &quot;Observed Species&quot;)</code></pre>
<p>From the above analysis we can now check predicted and observed species:</p>
<pre class="r"><code>cat(&quot;First 6 rows of predicted and observed species using KNN&quot;)</code></pre>
<pre><code>## First 6 rows of predicted and observed species using KNN</code></pre>
<pre class="r"><code>head(merge)</code></pre>
<pre><code>##   Predicted Species Observed Species
## 1       Iris-setosa      Iris-setosa
## 2       Iris-setosa      Iris-setosa
## 3       Iris-setosa      Iris-setosa
## 4       Iris-setosa      Iris-setosa
## 5       Iris-setosa      Iris-setosa
## 6       Iris-setosa      Iris-setosa</code></pre>
<p>We can also use caret package to check the accuracy of this model:</p>
<pre class="r"><code>cm &lt;- confusionMatrix(merge$`Predicted Species`, reference = as.factor(merge$`Observed Species`))
cm</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##                  Reference
## Prediction        Iris-setosa Iris-versicolor Iris-virginica
##   Iris-setosa              21               0              0
##   Iris-versicolor           0              15              1
##   Iris-virginica            0               0             16
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9811          
##                  95% CI : (0.8993, 0.9995)
##     No Information Rate : 0.3962          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9714          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: Iris-setosa Class: Iris-versicolor
## Sensitivity                      1.0000                 1.0000
## Specificity                      1.0000                 0.9737
## Pos Pred Value                   1.0000                 0.9375
## Neg Pred Value                   1.0000                 1.0000
## Prevalence                       0.3962                 0.2830
## Detection Rate                   0.3962                 0.2830
## Detection Prevalence             0.3962                 0.3019
## Balanced Accuracy                1.0000                 0.9868
##                      Class: Iris-virginica
## Sensitivity                         0.9412
## Specificity                         1.0000
## Pos Pred Value                      1.0000
## Neg Pred Value                      0.9730
## Prevalence                          0.3208
## Detection Rate                      0.3019
## Detection Prevalence                0.3019
## Balanced Accuracy                   0.9706</code></pre>
<p>We can see that the model accuracy is pretty high here.</p>
</div>
<div id="part-4" class="section level2">
<h2>Part 4</h2>
<p>Suppose we collect data for a group of students in a statistics class
with variables <span class="math inline">\({X1 =hours}\)</span> studied, <span class="math inline">\({X2 =undergrad GPA}\)</span>, and <span class="math inline">\({Y =received A}\)</span>. We fit a logistic regression and produce estimated
coefficient, <span class="math inline">\(\hat{\beta}_0 = -6\)</span>, <span class="math inline">\(\hat{\beta}_1 = 0.05\)</span>, <span class="math inline">\(\hat{\beta}_2 = 1\)</span>.</p>
<p>Now, we will estimate the probability that a student who studies for 40 h and
has an undergrad GPA of 3.5 gets an A in the class. For this question, we can simply plug in the beta values in this equation below:</p>
<p><span class="math display">\[\hat{p}(X) = \frac{e^{\hat\beta_0 + \hat\beta_1 {X_1}}}{(1 + e^{\hat\beta_0 + \hat\beta_1 {X_1}})}\]</span></p>
<p>As:</p>
<p><span class="math display">\[\hat{p}(X) = \frac{e^{-6 + 0.05X_1 + X_2}}{(1 + e^{-6 + 0.05X_1 + X_2})} = \frac{e^{-6 + 0.05*40 + 1*3.5}}{(1 + e^{-6 + 0.05*40 + 1*3.5})} = 0.3775.\]</span></p>
<pre class="r"><code>cat(&quot;We can also write a function to calculate this and get the answer&quot;)</code></pre>
<pre><code>## We can also write a function to calculate this and get the answer</code></pre>
<pre class="r"><code>calculate_prob &lt;- function(x1,x2){ z &lt;- exp(-6 + 0.05*x1 + 1*x2); return( round(z/(1+z),4))}
calculate_prob(40,3.5)</code></pre>
<pre><code>## [1] 0.3775</code></pre>
<p>Next, we will check how many hours would the student need to study to
have a 50% chance of getting an A in the class. The equation for predicted probability gives us:</p>
<p><span class="math display">\[\frac{e^{-6 + 0.05X_1 + 3.5}}{(1 + e^{-6 + 0.05X_1 + 3.5})} = 0.5\]</span></p>
<p>from which, we get:</p>
<p><span class="math display">\[e^{-6 + 0.05X_1 + 3.5} = 1\]</span></p>
<p>If we take log of both sides, we get:</p>
<p><span class="math display">\[X_1 = \frac{2.5}{0.05} = 50\]</span></p>
<pre class="r"><code>cat(&quot;We can also use the function to calculate this and get the answer&quot;)</code></pre>
<pre><code>## We can also use the function to calculate this and get the answer</code></pre>
<pre class="r"><code>hours &lt;- seq(40,60,1)
probs &lt;- mapply(hours, 3.5, FUN = calculate_prob)
names(probs) &lt;- paste0(hours,&quot;_hours&quot;)
probs</code></pre>
<pre><code>## 40_hours 41_hours 42_hours 43_hours 44_hours 45_hours 46_hours 47_hours 
##   0.3775   0.3894   0.4013   0.4134   0.4256   0.4378   0.4502   0.4626 
## 48_hours 49_hours 50_hours 51_hours 52_hours 53_hours 54_hours 55_hours 
##   0.4750   0.4875   0.5000   0.5125   0.5250   0.5374   0.5498   0.5622 
## 56_hours 57_hours 58_hours 59_hours 60_hours 
##   0.5744   0.5866   0.5987   0.6106   0.6225</code></pre>
<p>Hence, to have 50 percent chance for securing A, the student has to study for 50 hours.</p>
<p>Suppose, we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on <span class="math inline">\(X\)</span>, last year’s percent profit. We examine a large number of companies and discover that the mean value of <span class="math inline">\(X\)</span> for companies that issued a dividend was <span class="math inline">\(\overline{X} = 10\)</span>, while the mean for those that didn’t was <span class="math inline">\(\overline{X} = 0\)</span>. In addition, the variance of X for these two sets of companies was <span class="math inline">\(\hat{\sigma}^2 = 36\)</span>. Finally, 80% of companies issued dividends. Assuming that <span class="math inline">\(X\)</span> follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was <span class="math inline">\(X = 4\)</span> last year.</p>
<p>Recall that the density function for a normal random variable
is
<span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}\]</span>. You will need to use Bayes’ theorem.</p>
<p>For this question, we plugin normal density into Bayes’ theorem, prior probability for ‘yes’ is given, and next do math as shown in the code below:</p>
<p>Based on pdf and Bayes theorem, we have:</p>
<p><span class="math display">\[p_k(X) = \frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma} exp \bigg( {-\frac{1}{2\sigma^2}(x - \mu_k)^2} \bigg)}
{\sum _{l=1}^{k}\pi_l\frac{1}{\sqrt{2\pi}\sigma} exp \bigg( {-\frac{1}{2\sigma^2}(x - \mu_l)^2} \bigg)}\]</span></p>
<p>If we just plug in the values for:</p>
<p><span class="math display">\[\pi _{YES} = 0.8, \pi _{NO} = 0.2, \mu _{YES} = 10, \mu _{N0} = 0, \widehat{\sigma^2} = 36\]</span></p>
<p>We have,</p>
<p><span class="math display">\[f _{YES}(4) = 0.04033, \ f _{NO}(4) = 0.05324\]</span></p>
<p>and if we use this value to caluclate the probablity that a company will issue
a dividend, we get (0.752%):</p>
<p><span class="math display">\[pYES(4) = \frac{0.8 \times 0.04033}{0.8 \times 0.04033 + 0.2 \times 0.05324} = 0.75186\]</span></p>
<p>The same can be done in R to get the result:</p>
<pre class="r"><code># pdf function
pdf &lt;- function(x, mu_k, sigma){((sqrt(2*pi)*sigma)^-1)*(exp(-((2*sigma^2)^-1)*(x-mu_k)^2))}

sigma &lt;- 6 # for both classes

# Type 1 where companies issued dividend
pi_1 &lt;- .8
mu_1 &lt;- 10

# Type 2 where companies  did not issue dividend
pi_2 &lt;- .2
mu_2 &lt;- 0

# Calculate probabilities based on Bayes
x &lt;- 4
p_1 &lt;- (pi_1*pdf(4,mu_1,sigma))/(pi_1*pdf(4,mu_1,sigma) + pi_2*pdf(4,mu_2,sigma))
p_2 &lt;- (pi_2*pdf(4,mu_2,sigma))/(pi_1*pdf(4,mu_1,sigma) + pi_2*pdf(4,mu_2,sigma))

# rounding the numbers
p_1 &lt;- round(p_1,4)
p_2 &lt;- round(p_2,4)

# prediction
prediction &lt;- data.frame(cbind(c(&quot;Dividend&quot;, &quot;Non-Dividend&quot;), c(p_1, p_2)))
colnames(prediction) &lt;- c(&quot;Types&quot;, &quot;prediction&quot;)
prediction</code></pre>
<pre><code>##          Types prediction
## 1     Dividend     0.7519
## 2 Non-Dividend     0.2481</code></pre>
<p>Based on this analysis, 75.19 percent of the companies will issue dividend this year and 24.81% of the companies will not.</p>
<p>Now, we will fit a model using the predictors (chosen previously in this exercise) for classification using the <code>MclustDA</code> function from the <code>mclust-package</code>. We will first get the summary of our model. We will also find the best model selected by BIC and report the Model Name and the BIC. (See <a href="https://www.rdocumentation.org/packages/mclust/versions/5.4/topics/mclustModelNames" class="uri">https://www.rdocumentation.org/packages/mclust/versions/5.4/topics/mclustModelNames</a>)</p>
<p>We will also report on the training error, test error, and true positive and true negative rates.</p>
<pre class="r"><code>library(ISLR)
library(mclust)
data(Weekly)
cat(&quot;## Omitting the `Today` variable&quot;)</code></pre>
<pre><code>## ## Omitting the `Today` variable</code></pre>
<pre class="r"><code>wk=Weekly[,-8]
index=c(which(Weekly$Year==2009), which(Weekly$Year==2010))
train=wk[-index,]
test=wk[index,]

cat(&quot;## Just with lag 2&quot;)</code></pre>
<pre><code>## ## Just with lag 2</code></pre>
<pre class="r"><code>X=as.data.frame(train[,3]) ## keeping only Lag2
class=train$Direction

cat(&quot;MclustDA&quot;)</code></pre>
<pre><code>## MclustDA</code></pre>
<pre class="r"><code>mod1 = MclustDA(X, class)
summary(mod1)</code></pre>
<pre><code>## ------------------------------------------------ 
## Gaussian finite mixture model for classification 
## ------------------------------------------------ 
## 
## MclustDA model summary: 
## 
##  log-likelihood   n df       BIC
##       -2129.439 985 10 -4327.804
##        
## Classes   n     % Model G
##    Down 441 44.77     V 2
##    Up   544 55.23     V 2
## 
## Training confusion matrix:
##       Predicted
## Class  Down  Up
##   Down   76 365
##   Up     70 474
## Classification error = 0.4416 
## Brier score          = 0.2452</code></pre>
<pre class="r"><code>cat(&quot;train error&quot;)</code></pre>
<pre><code>## train error</code></pre>
<pre class="r"><code>predstrain=predict(mod1, newdata=train[,3])$classification
###################################################################
## function to get overall accuracy, TPR and TNR
overall_accuracy_TPR_TNR=function(con){
  OA=round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100-OA
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  return(list(overall_accuracy = OA,True_positive_rate = TPR,True_negative_rate = TNR))
}
###################################################################

overall_accuracy_TPR_TNR(table(predstrain,class))</code></pre>
<pre><code>## $overall_accuracy
## [1] 55.84
## 
## $True_positive_rate
## [1] 87.13
## 
## $True_negative_rate
## [1] 17.23</code></pre>
<pre class="r"><code>cat(&quot;test error&quot;)</code></pre>
<pre><code>## test error</code></pre>
<pre class="r"><code>predstest=predict(mod1, newdata=test[,3])$classification
trueClass=test[,8]
overall_accuracy_TPR_TNR(table(predstest,trueClass))</code></pre>
<pre><code>## $overall_accuracy
## [1] 54.81
## 
## $True_positive_rate
## [1] 85.25
## 
## $True_negative_rate
## [1] 11.63</code></pre>
<pre class="r"><code>cat(&quot;With all the variables&quot;)</code></pre>
<pre><code>## With all the variables</code></pre>
<pre class="r"><code>X=as.data.frame(train[,-8]) ## keeping only Lag2
mod11 = MclustDA(X, class)
summary(mod11)</code></pre>
<pre><code>## ------------------------------------------------ 
## Gaussian finite mixture model for classification 
## ------------------------------------------------ 
## 
## MclustDA model summary: 
## 
##  log-likelihood   n  df       BIC
##       -12477.54 985 286 -26926.37
##        
## Classes   n     % Model G
##    Down 441 44.77   VVV 4
##    Up   544 55.23   VVV 4
## 
## Training confusion matrix:
##       Predicted
## Class  Down  Up
##   Down  251 190
##   Up    160 384
## Classification error = 0.3553 
## Brier score          = 0.2177</code></pre>
<pre class="r"><code>predstrain=predict(mod11, newdata=train[,-8])$classification
overall_accuracy_TPR_TNR(table(predstrain,class))</code></pre>
<pre><code>## $overall_accuracy
## [1] 64.47
## 
## $True_positive_rate
## [1] 70.59
## 
## $True_negative_rate
## [1] 56.92</code></pre>
<pre class="r"><code>predstest=predict(mod11, newdata=test[,-8])$classification
trueClass=test[,8]
overall_accuracy_TPR_TNR(table(predstest,trueClass))</code></pre>
<pre><code>## $overall_accuracy
## [1] 53.85
## 
## $True_positive_rate
## [1] 85.25
## 
## $True_negative_rate
## [1] 9.3</code></pre>
<p>As we wanted to test, I fitted a model with the <code>MclustDA</code> function. The dataset was partitioned into training and testing sets using the same criterion as before:</p>
<p>Two models were fitted. One with just the variable <code>Lag2</code>, and in the other one, <code>all</code> the variables were used. As we can see from the above table, in the univariate case (with lag2 variable), the best model was a two-component model in each class, having variable variance.</p>
<p>Now, we will specify <code>modelType="EDDA"</code> and run <code>MclustDA</code> again. We will get a summary of our model to report on the best model selected by BIC, training and test error, and the true positive and true Negative rates.</p>
<pre class="r"><code>cat(&quot;EDDA type&quot;)</code></pre>
<pre><code>## EDDA type</code></pre>
<pre class="r"><code>X=as.data.frame(train[,-8]) ## keeping only Lag2
mod2=MclustDA(X, class, modelType = &quot;EDDA&quot;)
summary(mod2)</code></pre>
<pre><code>## ------------------------------------------------ 
## Gaussian finite mixture model for classification 
## ------------------------------------------------ 
## 
## EDDA model summary: 
## 
##  log-likelihood   n df       BIC
##       -15029.33 985 49 -30396.39
##        
## Classes   n     % Model G
##    Down 441 44.77   VVE 1
##    Up   544 55.23   VVE 1
## 
## Training confusion matrix:
##       Predicted
## Class  Down  Up
##   Down   98 343
##   Up     90 454
## Classification error = 0.4396 
## Brier score          = 0.2497</code></pre>
<pre class="r"><code>## train error
predstrain=predict(mod2, newdata=train[,-8])$classification
overall_accuracy_TPR_TNR(table(predstrain,class))</code></pre>
<pre><code>## $overall_accuracy
## [1] 56.04
## 
## $True_positive_rate
## [1] 83.46
## 
## $True_negative_rate
## [1] 22.22</code></pre>
<pre class="r"><code>## test error
predstest=predict(mod2, newdata=test[,-8])$classification
trueClass=test[,8]
overall_accuracy_TPR_TNR(table(predstest,trueClass))</code></pre>
<pre><code>## $overall_accuracy
## [1] 46.15
## 
## $True_positive_rate
## [1] 14.75
## 
## $True_negative_rate
## [1] 90.7</code></pre>
<p>With the <code>modelType=”EDDA”</code>, a model was fitted using <code>all</code> the variables (which in our case did not converge with only one variable).</p>
<p>From the documentation of <code>mclust</code>, it was found that specifying <code>“EDDA”</code> as the model type, we force the model to have a single component in each class with same covariance structure. We see that the single component had ellipsoidal, equal orientation (VVE) structure.</p>
<p>Next, we will use the <span class="math inline">\({\textbf{Auto}}\)</span> dataset. Fit a classification model (using the predictors chosen for previously) using the <code>MclustDA</code> function from the <code>mclust-package</code>. Then, use the same training and test sets to get a summary of our model. We will find the best model selected by BIC and report the model name and BIC, the training and test errors, and the true positive and the true negative rates.</p>
<pre class="r"><code>library(ISLR)
data(Auto)
mpg01=rep(NA,dim(Auto)[1])
med=median(Auto$mpg)
mpg01=ifelse(Auto$mpg&lt;med,0,1)
df=as.data.frame(cbind(Auto, mpg01))

newdf=df[,c(2,3,4,5,6,7,8,10)] ## mpg and names dropped
lowmpg=subset(newdf,mpg01==0)
highmpg=subset(newdf,mpg01==1)
set.seed(1235)
index1=sample((1:dim(lowmpg)[1]),size=round(0.5*dim(lowmpg)[1]),replace=F)
set.seed(54321)
index2=sample((1:dim(highmpg)[1]),size=round(0.5*dim(highmpg)[1]),replace=F)
train1=lowmpg[index1,];   train2=highmpg[index2,]
test1=lowmpg[-index1,];   test2=highmpg[-index2,]
autotrain=rbind(train1,train2)
autotest=rbind(test1,test2)

autotrainX=autotrain[,-8]
autotrainClass=autotrain[,8]
autotestX=autotest[,-8]
autotestClass=autotest[,8]

auto_mclust_DA=MclustDA(autotrainX, autotrainClass)
summary(auto_mclust_DA)</code></pre>
<pre><code>## ------------------------------------------------ 
## Gaussian finite mixture model for classification 
## ------------------------------------------------ 
## 
## MclustDA model summary: 
## 
##  log-likelihood   n  df       BIC
##       -3696.388 196 215 -8527.571
##        
## Classes  n  % Model G
##       0 98 50   EEV 3
##       1 98 50   EEV 4
## 
## Training confusion matrix:
##      Predicted
## Class  0  1
##     0 89  9
##     1  5 93
## Classification error = 0.0714 
## Brier score          = 0.0586</code></pre>
<pre class="r"><code>autopredstrain=predict(auto_mclust_DA,newdata=autotrainX)$classification
## train summaries
overall_accuracy_TPR_TNR(table(autopredstrain, autotrainClass)) ##94.9, 94.9, 94.9</code></pre>
<pre><code>## $overall_accuracy
## [1] 92.86
## 
## $True_positive_rate
## [1] 94.9
## 
## $True_negative_rate
## [1] 90.82</code></pre>
<pre class="r"><code>## test summaries
autopredstest=predict(auto_mclust_DA,newdata=autotestX)$classification
overall_accuracy_TPR_TNR(table(autopredstest, autotestClass)) ## 88.27, 84.69, 91.84</code></pre>
<pre><code>## $overall_accuracy
## [1] 90.82
## 
## $True_positive_rate
## [1] 90.82
## 
## $True_negative_rate
## [1] 90.82</code></pre>
<p>Using the same training and testing dataset as before, <code>MclustDA</code> was performed in the Auto data. The response was the binary coded MPG (whether greater or equal to median mpg). Same predictors as previous were used as previous.</p>
<p>As we can see from the table above, the best model picked up by the BIC was a 5-component mixture in both classes, having the similar covariance structure diagonal, equal volume and shape (EEI).</p>
<p>Now, we will specify <code>modelType="EDDA"</code> and run <code>MclustDA</code> again to report on the summary of our model, the best model selected by BIC, training and test error rates, and the true Positive and true Negative rates.</p>
<pre class="r"><code>autoDA2=MclustDA(autotrainX, autotrainClass, modelType = &quot;EDDA&quot;)
summary(autoDA2)</code></pre>
<pre><code>## ------------------------------------------------ 
## Gaussian finite mixture model for classification 
## ------------------------------------------------ 
## 
## EDDA model summary: 
## 
##  log-likelihood   n df       BIC
##       -4457.887 196 49 -9174.401
##        
## Classes  n  % Model G
##       0 98 50   VVE 1
##       1 98 50   VVE 1
## 
## Training confusion matrix:
##      Predicted
## Class  0  1
##     0 90  8
##     1  8 90
## Classification error = 0.0816 
## Brier score          = 0.0774</code></pre>
<pre class="r"><code>autopredstrain=predict(autoDA2,newdata=autotrainX)$classification
cat(&quot;## train summaries&quot;)</code></pre>
<pre><code>## ## train summaries</code></pre>
<pre class="r"><code>overall_accuracy_TPR_TNR(table(autopredstrain, autotrainClass)) ##92.86, 94.9, 90.82</code></pre>
<pre><code>## $overall_accuracy
## [1] 91.84
## 
## $True_positive_rate
## [1] 91.84
## 
## $True_negative_rate
## [1] 91.84</code></pre>
<pre class="r"><code>cat(&quot;## test summaries&quot;)</code></pre>
<pre><code>## ## test summaries</code></pre>
<pre class="r"><code>autopredstest=predict(autoDA2,newdata=autotestX)$classification
overall_accuracy_TPR_TNR(table(autopredstest, autotestClass)) ## 89.29, 88.78, 89.8</code></pre>
<pre><code>## $overall_accuracy
## [1] 89.8
## 
## $True_positive_rate
## [1] 89.8
## 
## $True_negative_rate
## [1] 89.8</code></pre>
<p>As expected, the best model was a 1 component mixture in each class having the same covariance structure. The covariance structure was single component had ellipsoidal, equal orientation (VVE).</p>
<p>Next, we will choose a dataset from <a href="http://archive.ics.uci.edu/ml">this website</a> and do some initial exploration of the dataset. We will report the analysis and dicuss the challenges with analyzing the data. <span class="math inline">\({\textit{We will strictly use ggplot for this}}\)</span></p>
<p>So, for this exercise, I am going to use <a href="https://archive.ics.uci.edu/ml/datasets/adult">this data:</a>.</p>
<p>Attribute Information:</p>
<p>Listing of attributes:</p>
<p><span class="math inline">\(&gt;50K, &lt;=50K\)</span></p>
<p>age: continuous.
workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
fnlwgt: continuous.
education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
education-num: continuous.
marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
sex: Female, Male.
capital-gain: continuous.
capital-loss: continuous.
hours-per-week: continuous.
native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands.</p>
<pre class="r"><code># https://archive.ics.uci.edu/ml/machine-learning-databases/adult/
set.seed(123)
library(rpart)
# install.packages(&quot;MLmetrics&quot;)
library(MLmetrics)
library(caTools)
library(e1071)
# install.packages(&quot;rpart.plot&quot;)
library(rpart.plot)
library(ggplot2)
library(Hmisc)
# install.packages(&quot;usdm&quot;)
library(usdm)

##Load Data##
df &lt;- read.csv(&quot;https://raw.githubusercontent.com/achalneupane/data/master/adult.data&quot;, header = T)
#Rename features for simplicity
colnames(df) &lt;- c(&quot;Age&quot;, &quot;Work_Class&quot;, &quot;Sampling_Weight&quot;, &quot;Highest_Education&quot;, &quot;Education_Num&quot;, 
                  &quot;Martial_Status&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;Cap_Gain&quot;, &quot;Cap_Loss&quot;,
                  &quot;Hours_Per_Week&quot;, &quot;Native_Country&quot;, &quot;Income&quot;)



cat(&quot;Data exploration&quot;)</code></pre>
<pre><code>## Data exploration</code></pre>
<pre class="r"><code>cat(&quot;is.na Age?&quot;)</code></pre>
<pre><code>## is.na Age?</code></pre>
<pre class="r"><code>sum(is.na(df$Age)) #none</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>cat (&quot;Summary of Age of adult population&quot;)</code></pre>
<pre><code>## Summary of Age of adult population</code></pre>
<pre class="r"><code>summary(df$Age)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   17.00   28.00   37.00   38.58   48.00   90.00</code></pre>
<pre class="r"><code>hist(df$Age)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>This data is skewed slightly right, 75% of adults in the dataset are younger than 48yrs old 50% of adults in data set lie between 28yrs old and 48yrs old age data has a range of 73 years</p>
<pre class="r"><code>cat(&quot;Types or Work Class&quot;)</code></pre>
<pre><code>## Types or Work Class</code></pre>
<pre class="r"><code>str(df$Work_Class)</code></pre>
<pre><code>##  chr [1:32560] &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; &quot; Private&quot; ...</code></pre>
<pre class="r"><code>summary(df$Work_Class)</code></pre>
<pre><code>##    Length     Class      Mode 
##     32560 character character</code></pre>
<pre class="r"><code>ggplot(data = df) +
  geom_bar(mapping = aes(x = df$Work_Class), col = &quot;blue&quot;, fill = &quot;blue&quot;)+
  coord_flip() + 
  labs(x = &quot;Work Class&quot;)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<pre class="r"><code># Never-worked: 7
# Without-pay: 14
# Might be able to leave those 21 datapoints out
# Most people work for private corporations, followed by self employed (not incorporated), then
# local government, then unknown, then state government, then self employed (incorporated), then
# federal gov, and lastly by without pay and never worked
#

cat(&quot;Race&quot;)</code></pre>
<pre><code>## Race</code></pre>
<pre class="r"><code>summary(df$Race)</code></pre>
<pre><code>##    Length     Class      Mode 
##     32560 character character</code></pre>
<pre class="r"><code>ggplot(data = df) +
  geom_bar(mapping = aes(x = df$Race), col = &quot;blue&quot;, fill = &quot;blue&quot;) +
  coord_flip() + 
  labs(x = &quot;Race&quot;)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-40-2.png" width="672" /></p>
<pre class="r"><code>ggplot(df, aes(x = df$Race)) +  
  geom_bar(aes(y = (..count..)/sum(..count..)), col = &quot;blue&quot;, fill = &quot;blue&quot;) +
  coord_flip() + 
  labs(x = &quot;Race&quot;, y = &quot;Percent&quot;)</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-40-3.png" width="672" /></p>
<pre class="r"><code>cat(&quot;# White: ~85.43%&quot;)</code></pre>
<pre><code>## # White: ~85.43%</code></pre>
<pre class="r"><code>27816/32561</code></pre>
<pre><code>## [1] 0.8542735</code></pre>
<pre class="r"><code>cat(&quot;# Black: ~9.6% &quot;)</code></pre>
<pre><code>## # Black: ~9.6%</code></pre>
<pre class="r"><code>3124/32561</code></pre>
<pre><code>## [1] 0.095943</code></pre>
<pre class="r"><code>cat(&quot;# American-Indian-Eskimo: ~0.96%&quot;)</code></pre>
<pre><code>## # American-Indian-Eskimo: ~0.96%</code></pre>
<pre class="r"><code>311/32561</code></pre>
<pre><code>## [1] 0.009551304</code></pre>
<pre class="r"><code>cat(&quot;# Asian-Pacific-Island: ~3.2%&quot;)</code></pre>
<pre><code>## # Asian-Pacific-Island: ~3.2%</code></pre>
<pre class="r"><code>1039/32561</code></pre>
<pre><code>## [1] 0.03190934</code></pre>
<pre class="r"><code>cat(&quot;# Other: ~0.8%&quot;)</code></pre>
<pre><code>## # Other: ~0.8%</code></pre>
<pre class="r"><code>271/32561</code></pre>
<pre><code>## [1] 0.00832284</code></pre>
<pre class="r"><code># Later generator: https://www.tablesgenerator.com/latex_tables</code></pre>
<p>This data looks accurate representative of the US population (using 2010 Census data):</p>

<pre class="r"><code>cat(&quot;## Sex&quot;)</code></pre>
<pre><code>## ## Sex</code></pre>
<pre class="r"><code>summary(df$Sex)</code></pre>
<pre><code>##    Length     Class      Mode 
##     32560 character character</code></pre>
<pre class="r"><code>ggplot(data = df) +
  geom_bar(mapping = aes(x = df$Sex))</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<pre class="r"><code>ggplot(df, aes(x = df$Sex)) +
  geom_bar(aes(y = (..count..)/sum(..count..)))</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-41-2.png" width="672" /></p>
<pre class="r"><code>#female: ~33.1%
10771/32561</code></pre>
<pre><code>## [1] 0.3307945</code></pre>
<pre class="r"><code>#male: ~66.9%
21790/32561</code></pre>
<pre><code>## [1] 0.6692055</code></pre>
<p>The data is disproportionately male. According to the Bureau of Labor Statistics, the workforce is 47% female and 53% male.</p>
<pre class="r"><code>###Hours
summary(df$Hours_Per_Week)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.00   40.00   40.00   40.44   45.00   99.00</code></pre>
<pre class="r"><code>ggplot() +
  geom_histogram(mapping = aes(x = df$Hours_Per_Week), data = df, stat = &quot;bin&quot;,
                 binwidth = 10, bins = 6
  )</code></pre>
<p><img src="/post/Linear_regression/Linear_regression_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>Worker’s Native Country information:</p>
<pre class="r"><code>summary(df$Native_Country)</code></pre>
<pre><code>##    Length     Class      Mode 
##     32560 character character</code></pre>
<pre class="r"><code>country_count &lt;- data.frame(Name = df$Native_Country)
summary(country_count)</code></pre>
<pre><code>##      Name          
##  Length:32560      
##  Class :character  
##  Mode  :character</code></pre>
<pre class="r"><code>cat(&quot;# Top three countries by frequency:&quot;)</code></pre>
<pre><code>## # Top three countries by frequency:</code></pre>
<pre class="r"><code>#   USA     :      29170 (~89.6%)
#   Mexico  :        643 (`2.00%)
#   Unknown :        582 (~1.8%)
#   Other   :       2165 (~6.65%)


cat(&quot;Summary of data USING DESCRIB(), HEAD(), SUMMARY(), AND STR()&quot;)</code></pre>
<pre><code>## Summary of data USING DESCRIB(), HEAD(), SUMMARY(), AND STR()</code></pre>
<pre class="r"><code>head(df)</code></pre>
<pre><code>##   Age        Work_Class Sampling_Weight Highest_Education Education_Num
## 1  50  Self-emp-not-inc           83311         Bachelors            13
## 2  38           Private          215646           HS-grad             9
## 3  53           Private          234721              11th             7
## 4  28           Private          338409         Bachelors            13
## 5  37           Private          284582           Masters            14
## 6  49           Private          160187               9th             5
##           Martial_Status         Occupation   Relationship   Race     Sex
## 1     Married-civ-spouse    Exec-managerial        Husband  White    Male
## 2               Divorced  Handlers-cleaners  Not-in-family  White    Male
## 3     Married-civ-spouse  Handlers-cleaners        Husband  Black    Male
## 4     Married-civ-spouse     Prof-specialty           Wife  Black  Female
## 5     Married-civ-spouse    Exec-managerial           Wife  White  Female
## 6  Married-spouse-absent      Other-service  Not-in-family  Black  Female
##   Cap_Gain Cap_Loss Hours_Per_Week Native_Country Income
## 1        0        0             13  United-States  &lt;=50K
## 2        0        0             40  United-States  &lt;=50K
## 3        0        0             40  United-States  &lt;=50K
## 4        0        0             40           Cuba  &lt;=50K
## 5        0        0             40  United-States  &lt;=50K
## 6        0        0             16        Jamaica  &lt;=50K</code></pre>
<pre class="r"><code>str(df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    32560 obs. of  15 variables:
##  $ Age              : int  50 38 53 28 37 49 52 31 42 37 ...
##  $ Work_Class       : chr  &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; &quot; Private&quot; ...
##  $ Sampling_Weight  : int  83311 215646 234721 338409 284582 160187 209642 45781 159449 280464 ...
##  $ Highest_Education: chr  &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; &quot; Bachelors&quot; ...
##  $ Education_Num    : int  13 9 7 13 14 5 9 14 13 10 ...
##  $ Martial_Status   : chr  &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; &quot; Married-civ-spouse&quot; ...
##  $ Occupation       : chr  &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; &quot; Prof-specialty&quot; ...
##  $ Relationship     : chr  &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Wife&quot; ...
##  $ Race             : chr  &quot; White&quot; &quot; White&quot; &quot; Black&quot; &quot; Black&quot; ...
##  $ Sex              : chr  &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Female&quot; ...
##  $ Cap_Gain         : int  0 0 0 0 0 0 0 14084 5178 0 ...
##  $ Cap_Loss         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Hours_Per_Week   : int  13 40 40 40 40 16 45 50 40 80 ...
##  $ Native_Country   : chr  &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; Cuba&quot; ...
##  $ Income           : chr  &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ...</code></pre>
<pre class="r"><code>summary(df)</code></pre>
<pre><code>##       Age         Work_Class        Sampling_Weight   Highest_Education 
##  Min.   :17.00   Length:32560       Min.   :  12285   Length:32560      
##  1st Qu.:28.00   Class :character   1st Qu.: 117832   Class :character  
##  Median :37.00   Mode  :character   Median : 178363   Mode  :character  
##  Mean   :38.58                      Mean   : 189782                     
##  3rd Qu.:48.00                      3rd Qu.: 237055                     
##  Max.   :90.00                      Max.   :1484705                     
##  Education_Num   Martial_Status      Occupation        Relationship      
##  Min.   : 1.00   Length:32560       Length:32560       Length:32560      
##  1st Qu.: 9.00   Class :character   Class :character   Class :character  
##  Median :10.00   Mode  :character   Mode  :character   Mode  :character  
##  Mean   :10.08                                                           
##  3rd Qu.:12.00                                                           
##  Max.   :16.00                                                           
##      Race               Sex               Cap_Gain        Cap_Loss      
##  Length:32560       Length:32560       Min.   :    0   Min.   :   0.00  
##  Class :character   Class :character   1st Qu.:    0   1st Qu.:   0.00  
##  Mode  :character   Mode  :character   Median :    0   Median :   0.00  
##                                        Mean   : 1078   Mean   :  87.31  
##                                        3rd Qu.:    0   3rd Qu.:   0.00  
##                                        Max.   :99999   Max.   :4356.00  
##  Hours_Per_Week  Native_Country        Income         
##  Min.   : 1.00   Length:32560       Length:32560      
##  1st Qu.:40.00   Class :character   Class :character  
##  Median :40.00   Mode  :character   Mode  :character  
##  Mean   :40.44                                        
##  3rd Qu.:45.00                                        
##  Max.   :99.00</code></pre>
<pre class="r"><code>describe(df)</code></pre>
<pre><code>## df 
## 
##  15  Variables      32560  Observations
## --------------------------------------------------------------------------------
## Age 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##    32560        0       73        1    38.58     15.4       19       22 
##      .25      .50      .75      .90      .95 
##       28       37       48       58       63 
## 
## lowest : 17 18 19 20 21, highest: 85 86 87 88 90
## --------------------------------------------------------------------------------
## Work_Class 
##        n  missing distinct 
##    32560        0        9 
## 
## lowest :  ?                 Federal-gov       Local-gov         Never-worked      Private         
## highest:  Private           Self-emp-inc      Self-emp-not-inc  State-gov         Without-pay     
## 
## ? (1836, 0.056), Federal-gov (960, 0.029), Local-gov (2093, 0.064),
## Never-worked (7, 0.000), Private (22696, 0.697), Self-emp-inc (1116, 0.034),
## Self-emp-not-inc (2541, 0.078), State-gov (1297, 0.040), Without-pay (14,
## 0.000)
## --------------------------------------------------------------------------------
## Sampling_Weight 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##    32560        0    21647        1   189782   112347    39459    65715 
##      .25      .50      .75      .90      .95 
##   117832   178363   237055   329055   379686 
## 
## lowest :   12285   13769   14878   18827   19214
## highest: 1226583 1268339 1366120 1455435 1484705
## --------------------------------------------------------------------------------
## Highest_Education 
##        n  missing distinct 
##    32560        0       16 
## 
## lowest :  10th          11th          12th          1st-4th       5th-6th     
## highest:  HS-grad       Masters       Preschool     Prof-school   Some-college
## 
## 10th (933, 0.029), 11th (1175, 0.036), 12th (433, 0.013), 1st-4th (168, 0.005),
## 5th-6th (333, 0.010), 7th-8th (646, 0.020), 9th (514, 0.016), Assoc-acdm (1067,
## 0.033), Assoc-voc (1382, 0.042), Bachelors (5354, 0.164), Doctorate (413,
## 0.013), HS-grad (10501, 0.323), Masters (1723, 0.053), Preschool (51, 0.002),
## Prof-school (576, 0.018), Some-college (7291, 0.224)
## --------------------------------------------------------------------------------
## Education_Num 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##    32560        0       16     0.95    10.08     2.75        5        7 
##      .25      .50      .75      .90      .95 
##        9       10       12       13       14 
## 
## lowest :  1  2  3  4  5, highest: 12 13 14 15 16
##                                                                             
## Value          1     2     3     4     5     6     7     8     9    10    11
## Frequency     51   168   333   646   514   933  1175   433 10501  7291  1382
## Proportion 0.002 0.005 0.010 0.020 0.016 0.029 0.036 0.013 0.323 0.224 0.042
##                                         
## Value         12    13    14    15    16
## Frequency   1067  5354  1723   576   413
## Proportion 0.033 0.164 0.053 0.018 0.013
## --------------------------------------------------------------------------------
## Martial_Status 
##        n  missing distinct 
##    32560        0        7 
## 
## lowest :  Divorced               Married-AF-spouse      Married-civ-spouse     Married-spouse-absent  Never-married        
## highest:  Married-civ-spouse     Married-spouse-absent  Never-married          Separated              Widowed              
## 
## Divorced (4443, 0.136), Married-AF-spouse (23, 0.001), Married-civ-spouse
## (14976, 0.460), Married-spouse-absent (418, 0.013), Never-married (10682,
## 0.328), Separated (1025, 0.031), Widowed (993, 0.030)
## --------------------------------------------------------------------------------
## Occupation 
##        n  missing distinct 
##    32560        0       15 
## 
## lowest :  ?                 Adm-clerical      Armed-Forces      Craft-repair      Exec-managerial 
## highest:  Prof-specialty    Protective-serv   Sales             Tech-support      Transport-moving
## --------------------------------------------------------------------------------
## Relationship 
##        n  missing distinct 
##    32560        0        6 
## 
## lowest :  Husband         Not-in-family   Other-relative  Own-child       Unmarried     
## highest:  Not-in-family   Other-relative  Own-child       Unmarried       Wife          
##                                                                       
## Value             Husband  Not-in-family Other-relative      Own-child
## Frequency           13193           8304            981           5068
## Proportion          0.405          0.255          0.030          0.156
##                                         
## Value           Unmarried           Wife
## Frequency            3446           1568
## Proportion          0.106          0.048
## --------------------------------------------------------------------------------
## Race 
##        n  missing distinct 
##    32560        0        5 
## 
## lowest :  Amer-Indian-Eskimo  Asian-Pac-Islander  Black               Other               White             
## highest:  Amer-Indian-Eskimo  Asian-Pac-Islander  Black               Other               White             
##                                                                    
## Value      Amer-Indian-Eskimo Asian-Pac-Islander              Black
## Frequency                 311               1039               3124
## Proportion              0.010              0.032              0.096
##                                                 
## Value                   Other              White
## Frequency                 271              27815
## Proportion              0.008              0.854
## --------------------------------------------------------------------------------
## Sex 
##        n  missing distinct 
##    32560        0        2 
##                         
## Value      Female   Male
## Frequency   10771  21789
## Proportion  0.331  0.669
## --------------------------------------------------------------------------------
## Cap_Gain 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##    32560        0      119     0.23     1078     2082        0        0 
##      .25      .50      .75      .90      .95 
##        0        0        0        0     5013 
## 
## lowest :     0   114   401   594   914, highest: 25236 27828 34095 41310 99999
## --------------------------------------------------------------------------------
## Cap_Loss 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##    32560        0       92    0.134    87.31    167.3        0        0 
##      .25      .50      .75      .90      .95 
##        0        0        0        0        0 
## 
## lowest :    0  155  213  323  419, highest: 3004 3683 3770 3900 4356
## --------------------------------------------------------------------------------
## Hours_Per_Week 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##    32560        0       94    0.897    40.44    12.28       18       24 
##      .25      .50      .75      .90      .95 
##       40       40       45       55       60 
## 
## lowest :  1  2  3  4  5, highest: 95 96 97 98 99
## --------------------------------------------------------------------------------
## Native_Country 
##        n  missing distinct 
##    32560        0       42 
## 
## lowest :  ?                Cambodia         Canada           China            Columbia       
## highest:  Thailand         Trinadad&amp;Tobago  United-States    Vietnam          Yugoslavia     
## --------------------------------------------------------------------------------
## Income 
##        n  missing distinct 
##    32560        0        2 
##                       
## Value      &lt;=50K  &gt;50K
## Frequency  24719  7841
## Proportion 0.759 0.241
## --------------------------------------------------------------------------------</code></pre>
<pre class="r"><code>cat(&quot;#It shows that 24.1% of adults make more than 50K USD&quot;)</code></pre>
<pre><code>## #It shows that 24.1% of adults make more than 50K USD</code></pre>
<pre class="r"><code>cat(&quot;Most people (75%) make less than or equal to 50K USD annually&quot;)</code></pre>
<pre><code>## Most people (75%) make less than or equal to 50K USD annually</code></pre>
<pre class="r"><code>cat(&quot;Let&#39;s look at the relationships between the variables&quot;)</code></pre>
<pre><code>## Let&#39;s look at the relationships between the variables</code></pre>
<pre class="r"><code>cat(&quot; Totals individuals making more than 50K&quot;)</code></pre>
<pre><code>##  Totals individuals making more than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot;) #7841</code></pre>
<pre><code>## [1] 7841</code></pre>
<pre class="r"><code>cat(&quot;Total White:&quot;)</code></pre>
<pre><code>## Total White:</code></pre>
<pre class="r"><code>sum(df$Race==&quot; White&quot;) #27816</code></pre>
<pre><code>## [1] 27815</code></pre>
<pre class="r"><code>cat(&quot;Total White making more than 50K&quot;)</code></pre>
<pre><code>## Total White making more than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; White&quot;) #7117</code></pre>
<pre><code>## [1] 7117</code></pre>
<pre class="r"><code>cat(&quot;Total Non-White making more than 50K&quot;)</code></pre>
<pre><code>## Total Non-White making more than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race!=&quot; White&quot;) #724</code></pre>
<pre><code>## [1] 724</code></pre>
<pre class="r"><code>cat(&quot;Total White making less than 50K&quot;)</code></pre>
<pre><code>## Total White making less than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &lt;=50K&quot; &amp; df$Race==&quot; White&quot;) #20699</code></pre>
<pre><code>## [1] 20698</code></pre>
<pre class="r"><code>cat (&quot;Percent of adults making more than 50K and are White:&quot;)</code></pre>
<pre><code>## Percent of adults making more than 50K and are White:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; White&quot;)/sum(df$Income==&quot; &gt;50K&quot;) *100</code></pre>
<pre><code>## [1] 90.76648</code></pre>
<pre class="r"><code>cat (&quot;Percent of White adults making more than 50K:&quot;)</code></pre>
<pre><code>## Percent of White adults making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; White&quot;)/sum(df$Race==&quot; White&quot;) *100</code></pre>
<pre><code>## [1] 25.58691</code></pre>
<p>White adults make up 90.77% of adults making more than 50K USD Non-white adults make up 9.23% of adults making more than 50K USD 25.6% of White adults make more than 50K USD</p>
<p>We can repeat the same analysis for Black population:</p>
<pre class="r"><code>cat(&quot;Race -- Black&quot;)</code></pre>
<pre><code>## Race -- Black</code></pre>
<pre class="r"><code>cat(&quot;Total black individuals&quot;)</code></pre>
<pre><code>## Total black individuals</code></pre>
<pre class="r"><code>sum(df$Race==&quot; Black&quot;) #3124</code></pre>
<pre><code>## [1] 3124</code></pre>
<pre class="r"><code>cat(&quot;Totals individuals making more than 50K&quot;)</code></pre>
<pre><code>## Totals individuals making more than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Black&quot;) #387</code></pre>
<pre><code>## [1] 387</code></pre>
<pre class="r"><code>cat(&quot;Total Non-Black making more than 50K&quot;)</code></pre>
<pre><code>## Total Non-Black making more than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race!=&quot; Black&quot;) #7454</code></pre>
<pre><code>## [1] 7454</code></pre>
<pre class="r"><code>cat(&quot;Total Black making less than 50K&quot;)</code></pre>
<pre><code>## Total Black making less than 50K</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &lt;=50K&quot; &amp; df$Race==&quot; Black&quot;) #2737</code></pre>
<pre><code>## [1] 2737</code></pre>
<pre class="r"><code>cat (&quot;percent of adults making more than 50K and are Black:&quot;)</code></pre>
<pre><code>## percent of adults making more than 50K and are Black:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Black&quot;)/sum(df$Income==&quot; &gt;50K&quot;) *100</code></pre>
<pre><code>## [1] 4.935595</code></pre>
<pre class="r"><code>cat (&quot;percent of blacks adults making more than 50K:&quot;)</code></pre>
<pre><code>## percent of blacks adults making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Black&quot;)/sum(df$Race==&quot; Black&quot;) *100</code></pre>
<pre><code>## [1] 12.38796</code></pre>
<p>Black adults make up 4.935% of adults making more than 50K USD; 12.38% of Black adults make more than 50K USD</p>
<p>We repeat the same analysis for American Indian:</p>
<pre class="r"><code>cat(&quot;Total Race -- Amer-Indian-Eskimo&quot;)</code></pre>
<pre><code>## Total Race -- Amer-Indian-Eskimo</code></pre>
<pre class="r"><code>sum(df$Race==&quot; Amer-Indian-Eskimo&quot;) #311</code></pre>
<pre><code>## [1] 311</code></pre>
<pre class="r"><code>cat(&quot;Amer-Indian-Eskimo making more than 50K: &quot;)</code></pre>
<pre><code>## Amer-Indian-Eskimo making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Amer-Indian-Eskimo&quot;) #36</code></pre>
<pre><code>## [1] 36</code></pre>
<pre class="r"><code>cat(&quot;Total non-Amer-Indian-Eskimo making more than 50K: &quot;)</code></pre>
<pre><code>## Total non-Amer-Indian-Eskimo making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race!=&quot; Amer-Indian-Eskimo&quot;) #7805</code></pre>
<pre><code>## [1] 7805</code></pre>
<pre class="r"><code>cat(&quot;Amer-Indian-Eskimo making less than 50K: &quot;)</code></pre>
<pre><code>## Amer-Indian-Eskimo making less than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &lt;=50K&quot; &amp; df$Race==&quot; Amer-Indian-Eskimo&quot;) #275</code></pre>
<pre><code>## [1] 275</code></pre>
<pre class="r"><code>cat (&quot;percent of adults making more than 50K and are Amer-Indian-Eskimo:&quot;)</code></pre>
<pre><code>## percent of adults making more than 50K and are Amer-Indian-Eskimo:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Amer-Indian-Eskimo&quot;)/sum(df$Income==&quot; &gt;50K&quot;) *100</code></pre>
<pre><code>## [1] 0.4591251</code></pre>
<pre class="r"><code>cat (&quot;percent of Amer-Indian-Eskimo adults making more than 50K:&quot;)</code></pre>
<pre><code>## percent of Amer-Indian-Eskimo adults making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Amer-Indian-Eskimo&quot;)/sum(df$Race==&quot; Amer-Indian-Eskimo&quot;) *100</code></pre>
<pre><code>## [1] 11.57556</code></pre>
<p>Amer-Indian-Eskimo adults make up 0.5% of adults making more than 50K USD
11.6% of Amer-Indian-Eskimo adults make more than 50K USD</p>
<p>We can repeat this analysis for Asian-Pac-Islander</p>
<pre class="r"><code>cat(&quot;Race -- Asian-Pac-Islander&quot;)</code></pre>
<pre><code>## Race -- Asian-Pac-Islander</code></pre>
<pre class="r"><code>cat(&quot;Total Race -- Asian-Pac-Islander&quot;)</code></pre>
<pre><code>## Total Race -- Asian-Pac-Islander</code></pre>
<pre class="r"><code>sum(df$Race==&quot; Asian-Pac-Islander&quot;) #1039</code></pre>
<pre><code>## [1] 1039</code></pre>
<pre class="r"><code>cat(&quot;Asian-Pac-Islander making more than 50K: &quot;)</code></pre>
<pre><code>## Asian-Pac-Islander making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Asian-Pac-Islander&quot;) #276</code></pre>
<pre><code>## [1] 276</code></pre>
<pre class="r"><code>cat(&quot;Total non-Asian-Pac-Islander making more than 50K: &quot;)</code></pre>
<pre><code>## Total non-Asian-Pac-Islander making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race!=&quot; Asian-Pac-Islander&quot;) #7565</code></pre>
<pre><code>## [1] 7565</code></pre>
<pre class="r"><code>cat(&quot;Asian-Pac-Islander making less than 50K: &quot;)</code></pre>
<pre><code>## Asian-Pac-Islander making less than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &lt;=50K&quot; &amp; df$Race==&quot; Asian-Pac-Islander&quot;) #763</code></pre>
<pre><code>## [1] 763</code></pre>
<pre class="r"><code>cat (&quot;percent of adults making more than 50K and are Asian-Pac-Islander:&quot;)</code></pre>
<pre><code>## percent of adults making more than 50K and are Asian-Pac-Islander:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Asian-Pac-Islander&quot;)/sum(df$Income==&quot; &gt;50K&quot;) *100</code></pre>
<pre><code>## [1] 3.519959</code></pre>
<pre class="r"><code>cat (&quot;percent of Asian-Pac-Islander adults making more than 50K:&quot;)</code></pre>
<pre><code>## percent of Asian-Pac-Islander adults making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Asian-Pac-Islander&quot;)/sum(df$Race==&quot; Asian-Pac-Islander&quot;) *100</code></pre>
<pre><code>## [1] 26.564</code></pre>
<p>Asian-Pac-Islander adults make up 3.5% of adults making more than 50K USD 26.56% of Asian-Pac-Islander adults make more than 50K USD</p>
<p>We can repeat this for all other race:</p>
<pre class="r"><code>cat(&quot;Race -- Other&quot;)</code></pre>
<pre><code>## Race -- Other</code></pre>
<pre class="r"><code>cat(&quot;Total Race -- Other&quot;)</code></pre>
<pre><code>## Total Race -- Other</code></pre>
<pre class="r"><code>sum(df$Race==&quot; Other&quot;) #271</code></pre>
<pre><code>## [1] 271</code></pre>
<pre class="r"><code>cat(&quot;Other-race making more than 50K: &quot;)</code></pre>
<pre><code>## Other-race making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Other&quot;) #25</code></pre>
<pre><code>## [1] 25</code></pre>
<pre class="r"><code>cat(&quot;Non-Other-race making more than 50K: &quot;)</code></pre>
<pre><code>## Non-Other-race making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race!=&quot; Other&quot;) #7816</code></pre>
<pre><code>## [1] 7816</code></pre>
<pre class="r"><code>cat(&quot;Other-race making less than 50K: &quot;)</code></pre>
<pre><code>## Other-race making less than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &lt;=50K&quot; &amp; df$Race==&quot; Other&quot;) #246</code></pre>
<pre><code>## [1] 246</code></pre>
<pre class="r"><code>cat (&quot;percent of adults making more than 50K and are Other-race:&quot;)</code></pre>
<pre><code>## percent of adults making more than 50K and are Other-race:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Other&quot;)/sum(df$Income==&quot; &gt;50K&quot;) *100</code></pre>
<pre><code>## [1] 0.3188369</code></pre>
<pre class="r"><code>cat (&quot;percent of Other-race adults making more than 50K:&quot;)</code></pre>
<pre><code>## percent of Other-race adults making more than 50K:</code></pre>
<pre class="r"><code>sum(df$Income==&quot; &gt;50K&quot; &amp; df$Race==&quot; Other&quot;)/sum(df$Race==&quot; Other&quot;) *100</code></pre>
<pre><code>## [1] 9.225092</code></pre>
<p>Adults of Other race make up 0.03% of adults making more than 50K USD. 9.22% of Other adults make more than 50K USD.</p>
<p>Now we can also compare other variables like Gender</p>
<pre class="r"><code>#Sex -- Male
sum(df$Sex==&quot; Male&quot;) #21790</code></pre>
<pre><code>## [1] 21789</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Male&quot; &amp; df$Income==&quot; &gt;50K&quot;) #6662</code></pre>
<pre><code>## [1] 6662</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Male&quot; &amp; df$Income!=&quot; &gt;50K&quot;) #15128</code></pre>
<pre><code>## [1] 15127</code></pre>
<pre class="r"><code>cat(&quot;30.6% of Male adults make more than 50K USD annually; 85% of adults making more than 50K USD annually are Male&quot;)</code></pre>
<pre><code>## 30.6% of Male adults make more than 50K USD annually; 85% of adults making more than 50K USD annually are Male</code></pre>
<pre class="r"><code>#Sex == Female
sum(df$Sex==&quot; Female&quot;) #10771</code></pre>
<pre><code>## [1] 10771</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Female&quot; &amp; df$Income==&quot; &gt;50K&quot;) #1179</code></pre>
<pre><code>## [1] 1179</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Female&quot; &amp; df$Income!=&quot; &gt;50K&quot;) #9592</code></pre>
<pre><code>## [1] 9592</code></pre>
<pre class="r"><code>cat(&quot;11% of Female adults make more than 50K USD annually; 15% of adults making more than 50K USD annually are Female&quot;)</code></pre>
<pre><code>## 11% of Female adults make more than 50K USD annually; 15% of adults making more than 50K USD annually are Female</code></pre>
<pre class="r"><code>#Sex -- Male AND Race -- White AND
sum(df$Sex==&quot; Female&quot; &amp; df$Race==&quot; White&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #1028</code></pre>
<pre><code>## [1] 1028</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Male&quot; &amp; df$Race==&quot; White&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #6089</code></pre>
<pre><code>## [1] 6089</code></pre>
<pre class="r"><code>cat(&quot;13.11% of adults making more than 50K USD annually are White Females; 77.66% of adults making more than 50K USD annually are White Males; 85.6% of White adults making more than 50K USD annually are Males&quot;)</code></pre>
<pre><code>## 13.11% of adults making more than 50K USD annually are White Females; 77.66% of adults making more than 50K USD annually are White Males; 85.6% of White adults making more than 50K USD annually are Males</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Female&quot; &amp; df$Race==&quot; Black&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #90</code></pre>
<pre><code>## [1] 90</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Male&quot; &amp; df$Race==&quot; Black&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #297</code></pre>
<pre><code>## [1] 297</code></pre>
<pre class="r"><code>cat (&quot;1.15% of adults making more than 50K USD annually are Black Females; 3.79% of adults making more than 50K USD annually are Black Males; 76.75% of Black adults making more than 50K USD annually are Males&quot;)</code></pre>
<pre><code>## 1.15% of adults making more than 50K USD annually are Black Females; 3.79% of adults making more than 50K USD annually are Black Males; 76.75% of Black adults making more than 50K USD annually are Males</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Female&quot; &amp; df$Race==&quot; Asian-Pac-Islander&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #43</code></pre>
<pre><code>## [1] 43</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Male&quot; &amp; df$Race==&quot; Asian-Pac-Islander&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #233</code></pre>
<pre><code>## [1] 233</code></pre>
<pre class="r"><code>cat(&quot;0.5% of adults making more than 50K USD annually are Asian/Pacific Islander Females; 2.97% of adults making more than 50K USD annually are Asian/Pacific Islander Males; 84.42% of Asian adults making more than 50K USD annually are Male&quot;)</code></pre>
<pre><code>## 0.5% of adults making more than 50K USD annually are Asian/Pacific Islander Females; 2.97% of adults making more than 50K USD annually are Asian/Pacific Islander Males; 84.42% of Asian adults making more than 50K USD annually are Male</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Female&quot; &amp; df$Race==&quot; Amer-Indian-Eskimo&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #12</code></pre>
<pre><code>## [1] 12</code></pre>
<pre class="r"><code>sum(df$Sex==&quot; Male&quot; &amp; df$Race==&quot; Amer-Indian-Eskimo&quot; &amp; df$Income ==&quot; &gt;50K&quot;) #24</code></pre>
<pre><code>## [1] 24</code></pre>
<pre class="r"><code>cat(&quot;0.15% of adults making more than 50K USD annually are Female American-Indian/Eskimo; 0.30% of adults making more than 50K USD annually are Male American-Indian/Eskimo; 92.31% of American-Indian/Eskimo adults making more than 50K USD annually are Male&quot;)</code></pre>
<pre><code>## 0.15% of adults making more than 50K USD annually are Female American-Indian/Eskimo; 0.30% of adults making more than 50K USD annually are Male American-Indian/Eskimo; 92.31% of American-Indian/Eskimo adults making more than 50K USD annually are Male</code></pre>
<pre class="r"><code># Workclass -- Private
sum(df$Income==&quot; &gt;50K&quot; &amp; df$Work_Class==&quot; Private&quot;) #4963</code></pre>
<pre><code>## [1] 4963</code></pre>
<pre class="r"><code>cat(&quot;63.29% of adults making more than 50K USD annually are in the private workclass&quot;)</code></pre>
<pre><code>## 63.29% of adults making more than 50K USD annually are in the private workclass</code></pre>
<pre class="r"><code># Workclass -- State-gov
sum(df$Income==&quot; &gt;50K&quot; &amp; df$Work_Class==&quot; State-gov&quot;) #353</code></pre>
<pre><code>## [1] 353</code></pre>
<pre class="r"><code>cat(&quot;4.5% of adults making more than 50K USD annually work for the state government&quot;)</code></pre>
<pre><code>## 4.5% of adults making more than 50K USD annually work for the state government</code></pre>
<pre class="r"><code># Workclass -- Self-emp-not-inc
sum(df$Income==&quot; &gt;50K&quot; &amp; df$Work_Class==&quot; Self-emp-not-inc&quot;) #724</code></pre>
<pre><code>## [1] 724</code></pre>
<pre class="r"><code>cat(&quot;9.23% of adults making more than 50K USD annually are self-employed (not incorporated&quot;)</code></pre>
<pre><code>## 9.23% of adults making more than 50K USD annually are self-employed (not incorporated</code></pre>
<pre class="r"><code># Workclass -- Self-emp-inc
sum(df$Income==&quot; &gt;50K&quot; &amp; df$Work_Class==&quot; Self-emp-inc&quot;) #622</code></pre>
<pre><code>## [1] 622</code></pre>
<pre class="r"><code>cat(&quot;8% of adults making more than 50K USD annually are self-employed (incorporated)&quot;)</code></pre>
<pre><code>## 8% of adults making more than 50K USD annually are self-employed (incorporated)</code></pre>
<pre class="r"><code># Workclass -- Federal
sum(df$Income==&quot; &gt;50K&quot; &amp; df$Work_Class==&quot; Federal-gov&quot;) #371</code></pre>
<pre><code>## [1] 371</code></pre>
<pre class="r"><code>cat(&quot;4.73% of adults making more than 50K USD annually work for the federal government&quot;)</code></pre>
<pre><code>## 4.73% of adults making more than 50K USD annually work for the federal government</code></pre>
<pre class="r"><code># Workclass -- ?
sum(df$Income==&quot; &gt;50K&quot; &amp; df$Work_Class==&quot; ?&quot;) #191</code></pre>
<pre><code>## [1] 191</code></pre>
<pre class="r"><code>cat(&quot;2.44% of adults making more than 50K USD annually work in unknown workclasses&quot;)</code></pre>
<pre><code>## 2.44% of adults making more than 50K USD annually work in unknown workclasses</code></pre>
<pre class="r"><code>cat(&quot;#QUICK RECAP USING XTABS()&quot;)</code></pre>
<pre><code>## #QUICK RECAP USING XTABS()</code></pre>
<pre class="r"><code>cat(&quot;# two-way contingency tables using Income and Workclass&quot;)</code></pre>
<pre><code>## # two-way contingency tables using Income and Workclass</code></pre>
<pre class="r"><code>xtabs( ~df$Income + df$Work_Class, data = df)</code></pre>
<pre><code>##          df$Work_Class
## df$Income     ?  Federal-gov  Local-gov  Never-worked  Private  Self-emp-inc
##     &lt;=50K  1645          589       1476             7    17733           494
##     &gt;50K    191          371        617             0     4963           622
##          df$Work_Class
## df$Income  Self-emp-not-inc  State-gov  Without-pay
##     &lt;=50K              1817        944           14
##     &gt;50K                724        353            0</code></pre>
<pre class="r"><code>cat(&quot;More self-employed-inc adults make greater than 50K USD, than those that do not&quot;)</code></pre>
<pre><code>## More self-employed-inc adults make greater than 50K USD, than those that do not</code></pre>
<pre class="r"><code>cat(&quot;# two-way contigency table using Income and Sex&quot;)</code></pre>
<pre><code>## # two-way contigency table using Income and Sex</code></pre>
<pre class="r"><code>xtabs( ~df$Income + df$Sex, data = df)</code></pre>
<pre><code>##          df$Sex
## df$Income  Female  Male
##     &lt;=50K    9592 15127
##     &gt;50K     1179  6662</code></pre>
<pre class="r"><code>cat(&quot;More Males make greater than 50K USD than Females&quot;)</code></pre>
<pre><code>## More Males make greater than 50K USD than Females</code></pre>
<pre class="r"><code>xtabs( ~df$Income + df$Highest_Education, data = df)</code></pre>
<pre><code>##          df$Highest_Education
## df$Income  10th  11th  12th  1st-4th  5th-6th  7th-8th  9th  Assoc-acdm
##     &lt;=50K   871  1115   400      162      317      606  487         802
##     &gt;50K     62    60    33        6       16       40   27         265
##          df$Highest_Education
## df$Income  Assoc-voc  Bachelors  Doctorate  HS-grad  Masters  Preschool
##     &lt;=50K       1021       3133        107     8826      764         51
##     &gt;50K         361       2221        306     1675      959          0
##          df$Highest_Education
## df$Income  Prof-school  Some-college
##     &lt;=50K          153          5904
##     &gt;50K           423          1387</code></pre>
<p>Adults with higher levels of education correspond with more making greater than 50K USD annually.</p>
<p>Next, we will clean and separate the data: separate the response variable from the explanatory variables. We will then split our data into test and training datasets for Recursive Partitioning and Regression Trees.</p>
<pre class="r"><code># Factor as numeric
df_clean &lt;- df
df_clean_numeric &lt;- df

# First, we will convert character variables as factor
for (i in 1:15) {
  if(class(df_clean_numeric[,i])==&quot;character&quot;){
    df_clean_numeric[,i] &lt;- as.factor(df_clean_numeric[,i])
  }
}

#Then converting variables to numerics
for (i in 1:15) {
  if(class(df_clean_numeric[,i])==&quot;factor&quot;){
    df_clean_numeric[,i] &lt;- as.numeric(df_clean_numeric[,i])
  }
}

#encoding the target feature as factor of 0 or 1
df_clean_numeric$Income &lt;- df_clean_numeric$Income - 1

# df_clean_numeric$Income &lt;- as.numeric(as.factor(df_clean_numeric$Income))-1

split = sample.split(df_clean$Income, SplitRatio = 0.75)
train_set &lt;- subset(df_clean, split == TRUE)
test_set &lt;- subset(df_clean, split == FALSE)

split = sample.split(df_clean_numeric$Income, SplitRatio = 0.75)
train_numeric &lt;- subset(df_clean_numeric, split == TRUE)
test_numeric &lt;- subset(df_clean_numeric, split == FALSE)

y &lt;- as.integer(train_numeric$Income)

cat(&quot;Model Building&quot;)</code></pre>
<pre><code>## Model Building</code></pre>
<pre class="r"><code>cat(&quot;Outlier detection by quantile analysis&quot;)</code></pre>
<pre><code>## Outlier detection by quantile analysis</code></pre>
<pre class="r"><code>sapply(train_numeric[,], function(x) quantile(x, c(.01,.05,.25,.5,.75,.90,.95, .99, 1),na.rm=TRUE) )</code></pre>
<pre><code>##      Age Work_Class Sampling_Weight Highest_Education Education_Num
## 1%    17          1         27242.0                 1             3
## 5%    19          1         40021.0                 2             5
## 25%   28          5        117795.8                10             9
## 50%   37          5        178272.0                12            10
## 75%   48          5        236396.0                13            12
## 90%   58          7        328571.1                16            13
## 95%   63          7        378724.2                16            14
## 99%   73          8        509629.0                16            16
## 100%  90          9       1484705.0                16            16
##      Martial_Status Occupation Relationship Race Sex Cap_Gain Cap_Loss
## 1%                1          1            1    2   1        0        0
## 5%                1          1            1    3   1        0        0
## 25%               3          4            1    5   1        0        0
## 50%               3          8            2    5   2        0        0
## 75%               5         11            4    5   2        0        0
## 90%               5         13            5    5   2        0        0
## 95%               6         14            5    5   2     5013        0
## 99%               7         15            6    5   2    15024     1980
## 100%              7         15            6    5   2    99999     4356
##      Hours_Per_Week Native_Country Income
## 1%                8              1      0
## 5%               18             20      0
## 25%              40             40      0
## 50%              40             40      0
## 75%              45             40      0
## 90%              55             40      1
## 95%              60             40      1
## 99%              80             40      1
## 100%             99             42      1</code></pre>
<pre class="r"><code>cat(&quot;We see no significant effect from outliers&quot;)</code></pre>
<pre><code>## We see no significant effect from outliers</code></pre>
<pre class="r"><code>cat(&quot;Missing value detection&quot;)</code></pre>
<pre><code>## Missing value detection</code></pre>
<pre class="r"><code>sapply(train_set, function(x) sum(is.na(x)) )</code></pre>
<pre><code>##               Age        Work_Class   Sampling_Weight Highest_Education 
##                 0                 0                 0                 0 
##     Education_Num    Martial_Status        Occupation      Relationship 
##                 0                 0                 0                 0 
##              Race               Sex          Cap_Gain          Cap_Loss 
##                 0                 0                 0                 0 
##    Hours_Per_Week    Native_Country            Income 
##                 0                 0                 0</code></pre>
<pre class="r"><code>cat(&quot;No NA values&quot;)</code></pre>
<pre><code>## No NA values</code></pre>
<pre class="r"><code>cat(&quot;correlation and VIF&quot;)</code></pre>
<pre><code>## correlation and VIF</code></pre>
<pre class="r"><code>correlation_table &lt;- cor(train_numeric[,1:14])
correlation_table2 &lt;- cor(train_numeric[,1:15])
correlation_table &lt;- (correlation_table*100)
correlation_tableDF &lt;- as.data.frame(correlation_table)
correlation_tableDFLogical &lt;- correlation_tableDF &gt; 5 | correlation_tableDF &lt; -5

cat(&quot;Correlation table:&quot;)</code></pre>
<pre><code>## Correlation table:</code></pre>
<pre class="r"><code>correlation_tableDF</code></pre>
<pre><code>##                            Age  Work_Class Sampling_Weight Highest_Education
## Age               100.00000000   0.3251212     -7.66944071         -1.486800
## Work_Class          0.32512121 100.0000000     -1.61517197          1.961213
## Sampling_Weight    -7.66944071  -1.6151720    100.00000000         -2.980889
## Highest_Education  -1.48679983   1.9612127     -2.98088950        100.000000
## Education_Num       3.46173798   4.7727016     -4.47341082         35.695005
## Martial_Status    -26.19488730  -6.4001468      3.31419524         -4.088912
## Occupation         -1.92241812  25.8024801      0.05946222         -2.261009
## Relationship      -26.32903221  -9.3824196      0.89553492         -1.094974
## Race                2.77498249   5.3188904     -1.67026574          1.420897
## Sex                 8.70487489   9.8977561      2.47346530         -3.132079
## Cap_Gain            7.76084425   3.5383148      0.31262977          2.767140
## Cap_Loss            5.46927062   0.6030259     -0.38300071          1.565373
## Hours_Per_Week      6.57180065  13.8076836     -1.71838683          5.168429
## Native_Country      0.04694584  -1.0055274     -5.38124360          6.323913
##                   Education_Num Martial_Status   Occupation Relationship
## Age                    3.461738     -26.194887  -1.92241812  -26.3290322
## Work_Class             4.772702      -6.400147  25.80248015   -9.3824196
## Sampling_Weight       -4.473411       3.314195   0.05946222    0.8955349
## Highest_Education     35.695005      -4.088912  -2.26100918   -1.0949742
## Education_Num        100.000000      -7.540081  10.92508359   -9.9838623
## Martial_Status        -7.540081     100.000000  -1.27165165   18.9835609
## Occupation            10.925084      -1.271652 100.00000000   -7.5904094
## Relationship          -9.983862      18.983561  -7.59040945  100.0000000
## Race                   3.684630      -7.139909   0.74522070  -11.1069229
## Sex                    1.611366     -13.212822   8.22371153  -58.1226027
## Cap_Gain              11.961446      -4.200839   2.41305676   -6.0372924
## Cap_Loss               7.647272      -3.374877   1.84884864   -6.0943717
## Hours_Per_Week        14.818123     -18.978607   7.88399987  -24.8906840
## Native_Country         4.812547      -2.671368  -1.54899986   -0.5558843
##                          Race         Sex    Cap_Gain    Cap_Loss
## Age                 2.7749825   8.7048749   7.7608443   5.4692706
## Work_Class          5.3188904   9.8977561   3.5383148   0.6030259
## Sampling_Weight    -1.6702657   2.4734653   0.3126298  -0.3830007
## Highest_Education   1.4208973  -3.1320793   2.7671399   1.5653728
## Education_Num       3.6846299   1.6113656  11.9614455   7.6472720
## Martial_Status     -7.1399092 -13.2128224  -4.2008391  -3.3748770
## Occupation          0.7452207   8.2237115   2.4130568   1.8488486
## Relationship      -11.1069229 -58.1226027  -6.0372924  -6.0943717
## Race              100.0000000   8.1164911   1.0834055   2.4038925
## Sex                 8.1164911 100.0000000   5.0290680   4.4357622
## Cap_Gain            1.0834055   5.0290680 100.0000000  -3.1160442
## Cap_Loss            2.4038925   4.4357622  -3.1160442 100.0000000
## Hours_Per_Week      4.3860109  23.0892702   7.7637843   5.5098707
## Native_Country     14.2261966  -0.6339393  -0.1424823  -0.1790446
##                   Hours_Per_Week Native_Country
## Age                    6.5718007     0.04694584
## Work_Class            13.8076836    -1.00552742
## Sampling_Weight       -1.7183868    -5.38124360
## Highest_Education      5.1684290     6.32391275
## Education_Num         14.8181234     4.81254734
## Martial_Status       -18.9786074    -2.67136800
## Occupation             7.8839999    -1.54899986
## Relationship         -24.8906840    -0.55588434
## Race                   4.3860109    14.22619663
## Sex                   23.0892702    -0.63393931
## Cap_Gain               7.7637843    -0.14248226
## Cap_Loss               5.5098707    -0.17904459
## Hours_Per_Week       100.0000000    -0.68190278
## Native_Country        -0.6819028   100.00000000</code></pre>
<pre class="r"><code>cat(&quot;Variables that are most strongly correlated with income: sex, education num, age, hours per week, cap gain, cap loss&quot;)</code></pre>
<pre><code>## Variables that are most strongly correlated with income: sex, education num, age, hours per week, cap gain, cap loss</code></pre>
<pre class="r"><code>cat(&quot;VIF&quot;)</code></pre>
<pre><code>## VIF</code></pre>
<pre class="r"><code>vifDF &lt;- as.data.frame(vif(train_numeric[,1:14]))
vifDF</code></pre>
<pre><code>##            Variables      VIF
## 1                Age 1.153218
## 2         Work_Class 1.119094
## 3    Sampling_Weight 1.012908
## 4  Highest_Education 1.176039
## 5      Education_Num 1.236533
## 6     Martial_Status 1.114775
## 7         Occupation 1.116620
## 8       Relationship 1.685945
## 9               Race 1.043802
## 10               Sex 1.564920
## 11          Cap_Gain 1.031509
## 12          Cap_Loss 1.021400
## 13    Hours_Per_Week 1.142152
## 14    Native_Country 1.032442</code></pre>
<pre class="r"><code>cat(&quot;Highest variance-inflation factors:&quot;)</code></pre>
<pre><code>## Highest variance-inflation factors:</code></pre>
<pre class="r"><code>cat(&quot;Relationship: 1.720860&quot;)</code></pre>
<pre><code>## Relationship: 1.720860</code></pre>
<pre class="r"><code>cat(&quot;Sex:          1.552166&quot;)</code></pre>
<pre><code>## Sex:          1.552166</code></pre>
<pre class="r"><code>cat(&quot;Education_Num: 1.250295&quot;)</code></pre>
<pre><code>## Education_Num: 1.250295</code></pre>
<pre class="r"><code>test_set$Income &lt;- (test_set$Income==&quot; &gt;50K&quot;)
test_set$Income &lt;- as.integer(test_set$Income)

regressor1 &lt;- rpart(formula = Income ~ Age + Work_Class + Highest_Education +
                      Education_Num + Martial_Status + Occupation + Relationship +
                      Race + Sex + Cap_Gain + Cap_Loss + Hours_Per_Week + Native_Country,
                    data = train_set,
                    control = rpart.control(minsplit = 10))

summary(regressor1)</code></pre>
<pre><code>## Call:
## rpart(formula = Income ~ Age + Work_Class + Highest_Education + 
##     Education_Num + Martial_Status + Occupation + Relationship + 
##     Race + Sex + Cap_Gain + Cap_Loss + Hours_Per_Week + Native_Country, 
##     data = train_set, control = rpart.control(minsplit = 10))
##   n= 24420 
## 
##           CP nsplit rel error    xerror        xstd
## 1 0.12956980      0 1.0000000 1.0000000 0.011361737
## 2 0.06614521      2 0.7408604 0.7446013 0.010193496
## 3 0.03383778      3 0.6747152 0.6845775 0.009859709
## 4 0.01000000      4 0.6408774 0.6412175 0.009601825
## 
## Variable importance
##      Relationship    Martial_Status          Cap_Gain     Education_Num 
##                24                24                10                10 
## Highest_Education               Sex        Occupation               Age 
##                10                 7                 7                 5 
##    Hours_Per_Week 
##                 3 
## 
## Node number 1: 24420 observations,    complexity param=0.1295698
##   predicted class= &lt;=50K  expected loss=0.2408272  P(node) =1
##     class counts: 18539  5881
##    probabilities: 0.759 0.241 
##   left son=2 (13354 obs) right son=3 (11066 obs)
##   Primary splits:
##       Relationship      splits as  RLLLLR, improve=1823.6520, (0 missing)
##       Martial_Status    splits as  LRRLLLL, improve=1802.6320, (0 missing)
##       Cap_Gain          &lt; 5119   to the left,  improve=1251.6450, (0 missing)
##       Highest_Education splits as  LLLLLLLLLRRLRLRL, improve= 978.1649, (0 missing)
##       Education_Num     &lt; 12.5   to the left,  improve= 978.1649, (0 missing)
##   Surrogate splits:
##       Martial_Status splits as  LRRLLLL, agree=0.993, adj=0.984, (0 split)
##       Sex            splits as  LR, agree=0.688, adj=0.311, (0 split)
##       Age            &lt; 33.5   to the left,  agree=0.649, adj=0.226, (0 split)
##       Occupation     splits as  LLLRRRLLLLRRLLR, agree=0.620, adj=0.161, (0 split)
##       Hours_Per_Week &lt; 43.5   to the left,  agree=0.604, adj=0.126, (0 split)
## 
## Node number 2: 13354 observations,    complexity param=0.03383778
##   predicted class= &lt;=50K  expected loss=0.06492437  P(node) =0.5468468
##     class counts: 12487   867
##    probabilities: 0.935 0.065 
##   left son=4 (13133 obs) right son=5 (221 obs)
##   Primary splits:
##       Cap_Gain          &lt; 7073.5 to the left,  improve=352.25120, (0 missing)
##       Highest_Education splits as  LLLLLLLLLRRLRLRL, improve=109.81130, (0 missing)
##       Education_Num     &lt; 12.5   to the left,  improve=109.81130, (0 missing)
##       Occupation        splits as  LLLLRLLLLLRLLLL, improve= 92.30208, (0 missing)
##       Hours_Per_Week    &lt; 42.5   to the left,  improve= 80.44179, (0 missing)
## 
## Node number 3: 11066 observations,    complexity param=0.1295698
##   predicted class= &lt;=50K  expected loss=0.4530996  P(node) =0.4531532
##     class counts:  6052  5014
##    probabilities: 0.547 0.453 
##   left son=6 (7738 obs) right son=7 (3328 obs)
##   Primary splits:
##       Highest_Education splits as  LLLLLLLLLRRLRLRL, improve=724.3926, (0 missing)
##       Education_Num     &lt; 12.5   to the left,  improve=724.3926, (0 missing)
##       Occupation        splits as  LRRLRLLLLLRRRRL, improve=713.6606, (0 missing)
##       Cap_Gain          &lt; 5095.5 to the left,  improve=582.3013, (0 missing)
##       Cap_Loss          &lt; 1782.5 to the left,  improve=197.6651, (0 missing)
##   Surrogate splits:
##       Education_Num  &lt; 12.5   to the left,  agree=1.000, adj=1.000, (0 split)
##       Occupation     splits as  LLLLRLLLLLRLLLL, agree=0.790, adj=0.303, (0 split)
##       Cap_Gain       &lt; 7493   to the left,  agree=0.717, adj=0.059, (0 split)
##       Native_Country splits as  LLLRLLLLLRRLLLL-LLLRRLLLRLLLLLRLLLLRRLLLLL, agree=0.706, adj=0.022, (0 split)
##       Cap_Loss       &lt; 1894.5 to the left,  agree=0.706, adj=0.022, (0 split)
## 
## Node number 4: 13133 observations
##   predicted class= &lt;=50K  expected loss=0.05002665  P(node) =0.5377969
##     class counts: 12476   657
##    probabilities: 0.950 0.050 
## 
## Node number 5: 221 observations
##   predicted class= &gt;50K   expected loss=0.04977376  P(node) =0.009049959
##     class counts:    11   210
##    probabilities: 0.050 0.950 
## 
## Node number 6: 7738 observations,    complexity param=0.06614521
##   predicted class= &lt;=50K  expected loss=0.3344533  P(node) =0.3168714
##     class counts:  5150  2588
##    probabilities: 0.666 0.334 
##   left son=12 (7333 obs) right son=13 (405 obs)
##   Primary splits:
##       Cap_Gain          &lt; 5095.5 to the left,  improve=356.46710, (0 missing)
##       Occupation        splits as  LRLLRLLLLLRRRRL, improve=212.37160, (0 missing)
##       Highest_Education splits as  LLLLLLLRR--R-L-R, improve=135.72350, (0 missing)
##       Education_Num     &lt; 8.5    to the left,  improve=135.72350, (0 missing)
##       Age               &lt; 33.5   to the left,  improve= 88.08525, (0 missing)
## 
## Node number 7: 3328 observations
##   predicted class= &gt;50K   expected loss=0.2710337  P(node) =0.1362817
##     class counts:   902  2426
##    probabilities: 0.271 0.729 
## 
## Node number 12: 7333 observations
##   predicted class= &lt;=50K  expected loss=0.2987863  P(node) =0.3002867
##     class counts:  5142  2191
##    probabilities: 0.701 0.299 
## 
## Node number 13: 405 observations
##   predicted class= &gt;50K   expected loss=0.01975309  P(node) =0.01658477
##     class counts:     8   397
##    probabilities: 0.020 0.980</code></pre>
<pre class="r"><code>y_pred &lt;- predict(regressor1, test_set)
y_pred &lt;- round(y_pred)

predictions &lt;- as.data.frame(y_pred)
predictions$` &lt;=50K`&lt;- NULL

library(caret)
confusionMatrix(factor(test_set$Income), factor(predictions$` &gt;50K`))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 5854  326
##          1  968  992
##                                           
##                Accuracy : 0.841           
##                  95% CI : (0.8329, 0.8489)
##     No Information Rate : 0.8381          
##     P-Value [Acc &gt; NIR] : 0.2403          
##                                           
##                   Kappa : 0.5105          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 0.8581          
##             Specificity : 0.7527          
##          Pos Pred Value : 0.9472          
##          Neg Pred Value : 0.5061          
##              Prevalence : 0.8381          
##          Detection Rate : 0.7192          
##    Detection Prevalence : 0.7592          
##       Balanced Accuracy : 0.8054          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<pre class="r"><code>cat(&quot;Second attempt at improving the model with feature selection&quot;)</code></pre>
<pre><code>## Second attempt at improving the model with feature selection</code></pre>
<pre class="r"><code>regressor1 &lt;- rpart(formula = Income ~ Age + Work_Class +
                      Education_Num + Relationship +
                      Race + Sex + Cap_Gain + Cap_Loss + Hours_Per_Week,
                    data = train_set,
                    control = rpart.control(minsplit = 10))

summary(regressor1)</code></pre>
<pre><code>## Call:
## rpart(formula = Income ~ Age + Work_Class + Education_Num + Relationship + 
##     Race + Sex + Cap_Gain + Cap_Loss + Hours_Per_Week, data = train_set, 
##     control = rpart.control(minsplit = 10))
##   n= 24420 
## 
##           CP nsplit rel error    xerror        xstd
## 1 0.12956980      0 1.0000000 1.0000000 0.011361737
## 2 0.06614521      2 0.7408604 0.7408604 0.010173437
## 3 0.03383778      3 0.6747152 0.6747152 0.009802339
## 4 0.01062744      4 0.6408774 0.6415576 0.009603905
## 5 0.01000000      6 0.6196225 0.6284645 0.009523109
## 
## Variable importance
##   Relationship       Cap_Gain  Education_Num            Sex            Age 
##             36             18             17             11              8 
## Hours_Per_Week     Work_Class       Cap_Loss 
##              5              3              2 
## 
## Node number 1: 24420 observations,    complexity param=0.1295698
##   predicted class= &lt;=50K  expected loss=0.2408272  P(node) =1
##     class counts: 18539  5881
##    probabilities: 0.759 0.241 
##   left son=2 (13354 obs) right son=3 (11066 obs)
##   Primary splits:
##       Relationship   splits as  RLLLLR,     improve=1823.6520, (0 missing)
##       Cap_Gain       &lt; 5119   to the left,  improve=1251.6450, (0 missing)
##       Education_Num  &lt; 12.5   to the left,  improve= 978.1649, (0 missing)
##       Age            &lt; 29.5   to the left,  improve= 742.1744, (0 missing)
##       Hours_Per_Week &lt; 41.5   to the left,  improve= 518.3798, (0 missing)
##   Surrogate splits:
##       Sex            splits as  LR,         agree=0.688, adj=0.311, (0 split)
##       Age            &lt; 33.5   to the left,  agree=0.649, adj=0.226, (0 split)
##       Hours_Per_Week &lt; 43.5   to the left,  agree=0.604, adj=0.126, (0 split)
##       Work_Class     splits as  LLLLLRRLR,  agree=0.590, adj=0.094, (0 split)
##       Cap_Gain       &lt; 2380.5 to the left,  agree=0.583, adj=0.079, (0 split)
## 
## Node number 2: 13354 observations,    complexity param=0.03383778
##   predicted class= &lt;=50K  expected loss=0.06492437  P(node) =0.5468468
##     class counts: 12487   867
##    probabilities: 0.935 0.065 
##   left son=4 (13133 obs) right son=5 (221 obs)
##   Primary splits:
##       Cap_Gain       &lt; 7073.5 to the left,  improve=352.25120, (0 missing)
##       Education_Num  &lt; 12.5   to the left,  improve=109.81130, (0 missing)
##       Hours_Per_Week &lt; 42.5   to the left,  improve= 80.44179, (0 missing)
##       Age            &lt; 28.5   to the left,  improve= 53.84101, (0 missing)
##       Cap_Loss       &lt; 2365.5 to the left,  improve= 52.15810, (0 missing)
## 
## Node number 3: 11066 observations,    complexity param=0.1295698
##   predicted class= &lt;=50K  expected loss=0.4530996  P(node) =0.4531532
##     class counts:  6052  5014
##    probabilities: 0.547 0.453 
##   left son=6 (7738 obs) right son=7 (3328 obs)
##   Primary splits:
##       Education_Num  &lt; 12.5   to the left,  improve=724.3926, (0 missing)
##       Cap_Gain       &lt; 5095.5 to the left,  improve=582.3013, (0 missing)
##       Cap_Loss       &lt; 1782.5 to the left,  improve=197.6651, (0 missing)
##       Age            &lt; 29.5   to the left,  improve=162.2896, (0 missing)
##       Hours_Per_Week &lt; 41.5   to the left,  improve=153.4967, (0 missing)
##   Surrogate splits:
##       Cap_Gain &lt; 7493   to the left,  agree=0.717, adj=0.059, (0 split)
##       Cap_Loss &lt; 1894.5 to the left,  agree=0.706, adj=0.022, (0 split)
##       Race     splits as  LRLLL,      agree=0.700, adj=0.004, (0 split)
## 
## Node number 4: 13133 observations
##   predicted class= &lt;=50K  expected loss=0.05002665  P(node) =0.5377969
##     class counts: 12476   657
##    probabilities: 0.950 0.050 
## 
## Node number 5: 221 observations
##   predicted class= &gt;50K   expected loss=0.04977376  P(node) =0.009049959
##     class counts:    11   210
##    probabilities: 0.050 0.950 
## 
## Node number 6: 7738 observations,    complexity param=0.06614521
##   predicted class= &lt;=50K  expected loss=0.3344533  P(node) =0.3168714
##     class counts:  5150  2588
##    probabilities: 0.666 0.334 
##   left son=12 (7333 obs) right son=13 (405 obs)
##   Primary splits:
##       Cap_Gain      &lt; 5095.5 to the left,  improve=356.46710, (0 missing)
##       Education_Num &lt; 8.5    to the left,  improve=135.72350, (0 missing)
##       Age           &lt; 33.5   to the left,  improve= 88.08525, (0 missing)
##       Cap_Loss      &lt; 1782.5 to the left,  improve= 84.27574, (0 missing)
##       Work_Class    splits as  LRLLLRLLL,  improve= 66.55492, (0 missing)
## 
## Node number 7: 3328 observations
##   predicted class= &gt;50K   expected loss=0.2710337  P(node) =0.1362817
##     class counts:   902  2426
##    probabilities: 0.271 0.729 
## 
## Node number 12: 7333 observations,    complexity param=0.01062744
##   predicted class= &lt;=50K  expected loss=0.2987863  P(node) =0.3002867
##     class counts:  5142  2191
##    probabilities: 0.701 0.299 
##   left son=24 (1183 obs) right son=25 (6150 obs)
##   Primary splits:
##       Education_Num  &lt; 8.5    to the left,  improve=112.71520, (0 missing)
##       Cap_Loss       &lt; 1782.5 to the left,  improve=100.54520, (0 missing)
##       Age            &lt; 33.5   to the left,  improve= 68.22059, (0 missing)
##       Work_Class     splits as  LRLLLRLLL,  improve= 54.02243, (0 missing)
##       Hours_Per_Week &lt; 35.5   to the left,  improve= 44.94644, (0 missing)
## 
## Node number 13: 405 observations
##   predicted class= &gt;50K   expected loss=0.01975309  P(node) =0.01658477
##     class counts:     8   397
##    probabilities: 0.020 0.980 
## 
## Node number 24: 1183 observations
##   predicted class= &lt;=50K  expected loss=0.0989011  P(node) =0.0484439
##     class counts:  1066   117
##    probabilities: 0.901 0.099 
## 
## Node number 25: 6150 observations,    complexity param=0.01062744
##   predicted class= &lt;=50K  expected loss=0.3372358  P(node) =0.2518428
##     class counts:  4076  2074
##    probabilities: 0.663 0.337 
##   left son=50 (5909 obs) right son=51 (241 obs)
##   Primary splits:
##       Cap_Loss       &lt; 1782.5 to the left,  improve=89.37983, (0 missing)
##       Age            &lt; 35.5   to the left,  improve=80.35238, (0 missing)
##       Education_Num  &lt; 9.5    to the left,  improve=49.10650, (0 missing)
##       Work_Class     splits as  LRLLLRLLL,  improve=44.76100, (0 missing)
##       Hours_Per_Week &lt; 34.5   to the left,  improve=42.62646, (0 missing)
## 
## Node number 50: 5909 observations
##   predicted class= &lt;=50K  expected loss=0.3200203  P(node) =0.2419738
##     class counts:  4018  1891
##    probabilities: 0.680 0.320 
## 
## Node number 51: 241 observations
##   predicted class= &gt;50K   expected loss=0.2406639  P(node) =0.00986896
##     class counts:    58   183
##    probabilities: 0.241 0.759</code></pre>
<pre class="r"><code>y_pred &lt;- predict(regressor1, test_set)
y_pred &lt;- round(y_pred)

predictions &lt;- as.data.frame(y_pred)
predictions$` &lt;=50K`&lt;- NULL

confusionMatrix(factor(test_set$Income), factor(predictions$` &gt;50K`))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 5830  350
##          1  900 1060
##                                           
##                Accuracy : 0.8464          
##                  95% CI : (0.8384, 0.8542)
##     No Information Rate : 0.8268          
##     P-Value [Acc &gt; NIR] : 1.06e-06        
##                                           
##                   Kappa : 0.5355          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.8663          
##             Specificity : 0.7518          
##          Pos Pred Value : 0.9434          
##          Neg Pred Value : 0.5408          
##              Prevalence : 0.8268          
##          Detection Rate : 0.7162          
##    Detection Prevalence : 0.7592          
##       Balanced Accuracy : 0.8090          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<pre class="r"><code>cat(&quot;Slight decrease in precision, slight increase in recall, and slight increase in accuracy; this model is roughly the same in performance as the first model&quot;)</code></pre>
<pre><code>## Slight decrease in precision, slight increase in recall, and slight increase in accuracy; this model is roughly the same in performance as the first model</code></pre>
<pre class="r"><code>cat (&quot;We can also calculate F1 score to measure test accuracy:&quot;)</code></pre>
<pre><code>## We can also calculate F1 score to measure test accuracy:</code></pre>
<pre class="r"><code>F1_Score(test_set$Income, predictions$` &gt;50K`)</code></pre>
<pre><code>## [1] 0.9031758</code></pre>
<pre class="r"><code>#F1 score of .9037</code></pre>
<p>I think this data has lots of information for analysis which could be a bit overwhelming. After performing some model fitting on this data, the corrected model seems to have slight decrease in precision, slight increase in recall, and slight increase in accuracy. The corrected model is roughly the same in performance as the first model.</p>
<p>Following is a table of several exponential-family distributions in common use and the data they are typically used for, along with the canonical link functions and their inverses (sometimes referred to as the mean function, as done here):</p>
<table>
<caption>
Common distributions with typical uses and canonical link functions
</caption>
<tbody>
<tr>
<th>
Distribution
</th>
<th>
Support of distribution
</th>
<th>
Typical uses
</th>
<th>
Link name
</th>
<th>
Link function, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5e2ebd12256b9e1b8dcdfdd4bd625f37df639ded" aria-hidden="true" alt="{\displaystyle \mathbf {X} {\boldsymbol {\beta }}=g(\mu )\,\!}"></span>
</th>
<th>
Mean function
</th>
</tr>
<tr>
<td>
<a title="Normal distribution">Normal</a>
</td>
<td>
real: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e577bfa9ed1c0f83ed643206abae3cd2f234cf9c" aria-hidden="true" alt="(-\infty ,+\infty )"></span>
</td>
<td>
Linear-response data
</td>
<td>
Identity
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/63238c06f9c1927aee60b40fec3adccd419cf32a" aria-hidden="true" alt="\mathbf {X} {\boldsymbol {\beta }}=\mu \,\!"></span>
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/12c514082234f52d09595635789f474de0279b7d" aria-hidden="true" alt="\mu =\mathbf {X} {\boldsymbol {\beta }}\,\!"></span>
</td>
</tr>
<tr>
<td>
<a title="Exponential distribution">Exponential</a>
</td>
<td rowspan="2">
real: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/de77e40eb7e2582eef8a5a1da1bc027b7d9a8d6e" aria-hidden="true" alt="(0,+\infty )"></span>
</td>
<td rowspan="2">
Exponential-response data, scale parameters
</td>
<td rowspan="2">
<a title="Multiplicative inverse">Negative inverse</a>
</td>
<td rowspan="2">
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6532ae0a7d9f63020f9a3e4175c391fb1130f99" aria-hidden="true" alt="\mathbf {X} {\boldsymbol {\beta }}=-\mu ^{-1}\,\!"></span>
</td>
<td rowspan="2">
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/11209fa27eda9b964da5691b83fd3652d59ddcc0" aria-hidden="true" alt="\mu =-(\mathbf {X} {\boldsymbol {\beta }})^{-1}\,\!"></span>
</td>
</tr>
<tr>
<td>
<a title="Gamma distribution">Gamma</a>
</td>
</tr>
<tr>
<td>
<a title="Inverse Gaussian distribution">Inverse<br>Gaussian</a>
</td>
<td>
real: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/de77e40eb7e2582eef8a5a1da1bc027b7d9a8d6e" aria-hidden="true" alt="(0,+\infty )"></span>
</td>
<td>
</td>
<td>
Inverse<br>squared
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a3b87590326202b24e85ce5762989fd34bff8c2" aria-hidden="true" alt="{\displaystyle \mathbf {X} {\boldsymbol {\beta }}=\mu ^{-2}\,\!}"></span>
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f2b2781a377e3d9ed78c1b1e026fda1e8895402" aria-hidden="true" alt="{\displaystyle \mu =(\mathbf {X} {\boldsymbol {\beta }})^{-1/2}\,\!}"></span>
</td>
</tr>
<tr>
<td>
<a title="Poisson distribution">Poisson</a>
</td>
<td>
integer: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b1da8ed7e74b31b6314f23f122a1198c104fcaad" aria-hidden="true" alt="0,1,2,\ldots "></span>
</td>
<td>
count of occurrences in fixed amount of time/space
</td>
<td>
<a title="Natural logarithm">Log</a>
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/245ed014e9dd7f9624171201d1a4daecb1c20997" aria-hidden="true" alt="{\displaystyle \mathbf {X} {\boldsymbol {\beta }}=\ln(\mu )\,\!}"></span>
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7fac36b3451b711d49417813988a6e8bb4db5719" aria-hidden="true" alt="{\displaystyle \mu =\exp(\mathbf {X} {\boldsymbol {\beta }})\,\!}"></span>
</td>
</tr>
<tr>
<td>
<a title="Bernoulli distribution">Bernoulli</a>
</td>
<td>
integer: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/28de5781698336d21c9c560fb1cbb3fb406923eb" aria-hidden="true" alt="\{0,1\}"></span>
</td>
<td>
outcome of single yes/no occurrence
</td>
<td rowspan="5">
<a title="Logit">Logit</a>
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8756b6c8f78882b05820c4058a861002462ef4b4" aria-hidden="true" alt="{\displaystyle \mathbf {X} {\boldsymbol {\beta }}=\ln \left({\frac {\mu }{1-\mu }}\right)\,\!}"></span>
</td>
<td rowspan="5">
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b739082e7ee418a2163685f976c75b4906910158" aria-hidden="true" alt="{\displaystyle \mu ={\frac {\exp(\mathbf {X} {\boldsymbol {\beta }})}{1+\exp(\mathbf {X} {\boldsymbol {\beta }})}}={\frac {1}{1+\exp(-\mathbf {X} {\boldsymbol {\beta }})}}\,\!}"></span>
</td>
</tr>
<tr>
<td>
<a title="Binomial distribution">Binomial</a>
</td>
<td>
integer: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f0dabd0eecff746a5377991354a67ea28a4e684" aria-hidden="true" alt="0,1,\ldots ,N"></span>
</td>
<td>
count of # of “yes” occurrences out of N yes/no occurrences
</td>
<td>
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ecbce4c90689853e5656461e1165f5473d276a44" aria-hidden="true" alt="{\displaystyle \mathbf {X} {\boldsymbol {\beta }}=\ln \left({\frac {\mu }{n-\mu }}\right)\,\!}"></span>
</td>
</tr>
<tr>
<td rowspan="2">
<a title="Categorical distribution">Categorical</a>
</td>
<td>
integer: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/aa074207d3bea2e879410172ce89ba2435d37d11" aria-hidden="true" alt="[0,K)"></span>
</td>
<td rowspan="2">
outcome of single K-way occurrence
</td>
<td rowspan="3">
<span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8756b6c8f78882b05820c4058a861002462ef4b4" aria-hidden="true" alt="{\displaystyle \mathbf {X} {\boldsymbol {\beta }}=\ln \left({\frac {\mu }{1-\mu }}\right)\,\!}"></span>
</td>
</tr>
<tr>
<td>
K-vector of integer: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/738f7d23bb2d9642bab520020873cccbef49768d" aria-hidden="true" alt="[0,1]"></span>, where exactly one element in the vector has the value 1
</td>
</tr>
<tr>
<td>
<a title="Multinomial distribution">Multinomial</a>
</td>
<td>
<i>K</i>-vector of integer: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/703d57dca548a7f9d927247c2a27b67666aebdd5" aria-hidden="true" alt="[0,N]"></span>
</td>
<td>
count of occurrences of different types (1 .. <i>K</i>) out of <i>N</i> total <i>K</i>-way occurrences
</td>
</tr>
</tbody>
</table>
<p>In the cases of the exponential and gamma distributions, the domain of the canonical link function is not the same as the permitted range of the mean. In particular, the linear predictor may be positive, which would give an impossible negative mean. When maximizing the likelihood, precautions must be taken to avoid this. An alternative is to use a noncanonical link function.</p>
<p>In the case of the Bernoulli, binomial, categorical and multinomial distributions, the support of the distributions is not the same type of data as the parameter being predicted. In all of these cases, the predicted parameter is one or more probabilities, i.e. real numbers in the range <span class="math inline">\({\displaystyle [0,1]}\)</span>. The resulting model is known as logistic regression (or multinomial logistic regression in the case that K-way rather than binary values are being predicted).</p>
<p>For the Bernoulli and binomial distributions, the parameter is a single probability, indicating the likelihood of occurrence of a single event. The Bernoulli still satisfies the basic condition of the generalized linear model in that, even though a single outcome will always be either 0 or 1, the expected value will nonetheless be a real-valued probability, i.e. the probability of occurrence of a “yes” (or 1) outcome. Similarly, in a binomial distribution, the expected value is Np, i.e. the expected proportion of “yes” outcomes will be the probability to be predicted.</p>
<p>For categorical and multinomial distributions, the parameter to be predicted is a K-vector of probabilities, with the further restriction that all probabilities must add up to 1. Each probability indicates the likelihood of occurrence of one of the K possible values. For the multinomial distribution, and for the vector form of the categorical distribution, the expected values of the elements of the vector can be related to the predicted probabilities similarly to the binomial and Bernoulli distributions.</p>
</div>
