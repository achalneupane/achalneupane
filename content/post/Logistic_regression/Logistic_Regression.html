---
Authors: ["**Achal Neupane**"]
title: "Logistic Regression"
date: 2020-09-18T17:26:23-05:00
draft: false
output: html_document
tags:
- R
- Statistics
summary: Statistics series
---



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<p>In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.</p>
<p>Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled “0” and “1”. In the logistic model, the log-odds (the logarithm of the odds) for the value labeled “1” is a linear combination of one or more independent variables (“predictors”); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled “1” can vary between 0 (certainly the value “0”) and 1 (certainly the value “1”), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio.</p>
<p>In a binary logistic regression model, the dependent variable has two levels (categorical). Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.</p>
<p>More from this wiki article: <a href="https://en.wikipedia.org/wiki/Logistic_regression" class="uri">https://en.wikipedia.org/wiki/Logistic_regression</a>.</p>
<p>In this tutorial, we will start off with the analysis of <span class="math inline">\(\textbf{plasma}\)</span> dataset.</p>
<p>We will first fit a quadratic regression model, i.e., a model of the form
<span class="math display">\[\text{Model 2:   } velocity = \beta_1 \times distance + \beta_2 \times distance^2 +\epsilon\]</span></p>
<pre class="r"><code>library(gamair)
library(knitr)
library(ggplot2)
library(rsq)
data(hubble)
library(HSAUR3)

model2 &lt;- lm(y~x + I(x^2) -1, data = hubble)
# summary(model2)
knitr::kable(summary(model2)$coefficients, caption = &quot;Summary of the model2&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 1: </span>Summary of the model2</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
<th align="right">t value</th>
<th align="right">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">x</td>
<td align="right">90.9046424</td>
<td align="right">16.5725817</td>
<td align="right">5.4852433</td>
<td align="right">0.0000164</td>
</tr>
<tr class="even">
<td align="left">I(x^2)</td>
<td align="right">-0.8837375</td>
<td align="right">0.9925378</td>
<td align="right">-0.8903817</td>
<td align="right">0.3828949</td>
</tr>
</tbody>
</table>
<p>Then we will plot the fitted curve from Model 2 on the scatterplot of the data.</p>
<pre class="r"><code># fitted curve
# index  &lt;- seq(0, 22, 0.1)
index &lt;- seq(min(hubble$x),max(hubble$x),0.1)
index2 &lt;- index^2
# predicted &lt;- predict(model2,list(x = index, x2=index2))
predicted &lt;- model2$fitted.values
#create a data frame of x nd y values for plotting for ggplot
data &lt;- as.data.frame(cbind(x = hubble$x,predicted))
# Scatter Plot
plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
plot(y~x, data = hubble, main = &quot;base R: Scatter plot with fitted curve from Model2&quot;, xlab = &quot;Distance&quot;, ylab = &quot;Velocity&quot;)
lines(data$x[order(data$x)], data$predicted[order(data$predicted)], col = &quot;green&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>ggplot(data = model2, aes(x = model2$model$x, y = model2$model$y)) +
  geom_point() +
  geom_line(aes(x = model2$model$x, y = model2$fitted.values), colour = &quot;green&quot;) +
  labs(title = &quot;ggplot: Scatter plot with fitted curve from Model2&quot;, x = &quot;Distance&quot;, y = &quot;velocity&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<p>Next, we will add the simple linear regression fit on this plot. We use different color and line type to differentiate the two and add a legend to our plot.</p>
<pre class="r"><code># Simple lm
hmod &lt;- lm(y~x - 1 , data = hubble)
plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
plot(y~x, data = hubble, main = &quot;base R: scatter plot for hubble data&quot;, xlab = &quot;Distance&quot;, ylab = &quot;Velocity&quot;)
lines(data$x[order(data$x)], data$predicted[order(data$predicted)], col = &quot;green&quot;)
abline(hmod, lty=2, col=2)
# Legend
legend(&quot;bottomright&quot;, c(&quot;Quadratic&quot;, &quot;Linear&quot;), lty = 1:2, col = 2:1)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>## ggplot version
ggplot(data = model2, aes(x = model2$model$x, y = model2$model$y)) +
  geom_point() +
  geom_line(aes(x = model2$model$x, y = model2$fitted.values, colour = &quot;Quadratic&quot;)) +
  geom_line(data = hmod, aes(x = hmod$model$x, y = hmod$fitted.values, colour = &quot;Linear&quot;)) +
  labs(title = &quot;ggplot: Scatter plot with fitted curve from Model2&quot;, x = &quot;Distance&quot;, y = &quot;velocity&quot;, colour = &quot;Models&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<p>Here, we will try to understand which model is the most sensible considering the nature of the data/ or by looking at the plot.</p>
<p>The simple model seems more sensible to me. The data points seem to follow a line from lower left to upper right of the plot without a clear curvature. However, strictly saying, there isn’t much difference between the two models. The quadratic model here is still regarded as a <code>"linear regression"</code> model since the term <code>"linear"</code> relates to the parameters of the model and not to the powers of the explanatory variables.</p>
<pre class="r"><code>summary(model2) # # Quadratic regression model</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x + I(x^2) - 1, data = hubble)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -713.15 -152.76  -54.85  163.92  557.01 
## 
## Coefficients:
##        Estimate Std. Error t value Pr(&gt;|t|)    
## x       90.9046    16.5726   5.485 1.64e-05 ***
## I(x^2)  -0.8837     0.9925  -0.890    0.383    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 260.1 on 22 degrees of freedom
## Multiple R-squared:  0.944,  Adjusted R-squared:  0.9389 
## F-statistic: 185.3 on 2 and 22 DF,  p-value: 1.715e-14</code></pre>
<pre class="r"><code>mod.2 &lt;- summary(model2)
summary(hmod)  # Simple linear model</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x - 1, data = hubble)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -736.5 -132.5  -19.0  172.2  558.0 
## 
## Coefficients:
##   Estimate Std. Error t value Pr(&gt;|t|)    
## x   76.581      3.965   19.32 1.03e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 258.9 on 23 degrees of freedom
## Multiple R-squared:  0.9419, Adjusted R-squared:  0.9394 
## F-statistic: 373.1 on 1 and 23 DF,  p-value: 1.032e-15</code></pre>
<pre class="r"><code>hmod.1 &lt;- summary(hmod) 
cat (&quot;Adjusted R-square&quot;)</code></pre>
<pre><code>## Adjusted R-square</code></pre>
<pre class="r"><code>kable(cbind(Quadratic = mod.2$adj.r.squared, Linear = hmod.1$adj.r.squared), caption = &quot;Adjusted R-square&quot;, row.names = FALSE )</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 2: </span>Adjusted R-square</caption>
<thead>
<tr class="header">
<th align="right">Quadratic</th>
<th align="right">Linear</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.9388554</td>
<td align="right">0.9394063</td>
</tr>
</tbody>
</table>
<p>The statistics appear to support the simple model as the better one. Since the Adjusted r-squared statistic is higher for the simple model (0.9394) Vs. Quadratic (0.9388554) which indicates that the simple model explains more of the variability in the response data than does the quadratic model.</p>
<p>The <span class="math inline">\(\textbf{leuk}\)</span> data from package <span class="math inline">\(\textbf{MASS}\)</span> shows the survival times
from diagnosis of patients suffering from leukemia and the values of two
explanatory variables, the white blood cell count (wbc) and the presence or
absence of a morphological characteristic of the white blood cells (ag).</p>
<p>Next, we will try to define a binary outcome variable according to whether or not patients lived for at least 24 weeks after diagnosis. We Call it <span class="math inline">\(\textit{surv24}\)</span>.</p>
<pre class="r"><code>#add a binary column named surv24 for time greater than or less than 24. 
library(MASS)
library(dplyr)
q3_subset &lt;- leuk %&gt;%
  mutate(surv24 = ifelse(time &gt;= 24, 1,0))</code></pre>
<p>We now fit a logistic regression model to the data with <span class="math inline">\(\textit{surv24}\)</span> as response. It is advisable to transform the very large white blood counts to avoid regression coefficients very close to 0 (and odds ratio close to 1). We may use log transformation in this case.</p>
<pre class="r"><code>surv24.model &lt;- glm(surv24 ~ log(wbc) + ag, data=q3_subset,family = &#39;binomial&#39;)
kable(summary(surv24.model)$coefficient, caption = &quot;Summary coefficients of the glm&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-6">Table 3: </span>Summary coefficients of the glm</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
<th align="right">z value</th>
<th align="right">Pr(&gt;|z|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">3.4555870</td>
<td align="right">2.9821469</td>
<td align="right">1.158758</td>
<td align="right">0.2465548</td>
</tr>
<tr class="even">
<td align="left">log(wbc)</td>
<td align="right">-0.4821891</td>
<td align="right">0.3149136</td>
<td align="right">-1.531179</td>
<td align="right">0.1257252</td>
</tr>
<tr class="odd">
<td align="left">agpresent</td>
<td align="right">1.7621259</td>
<td align="right">0.8093190</td>
<td align="right">2.177295</td>
<td align="right">0.0294586</td>
</tr>
</tbody>
</table>
<p>Now constructing some graphics useful in the interpretation of the final model we just fit. We will create a scatter plot of the data fitting the two curves of test results to the fitted output of the model prediciton</p>
<pre class="r"><code>x.extension &lt;- seq(0, max(log(q3_subset$wbc)+4.5), by = 0.5)
espframe &lt;- data.frame(&quot;x.extension&quot; = x.extension, &quot;agpress&quot; = (exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3]))), &quot;agabs&quot; = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension)/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension)))


plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
plot(x = log(leuk$wbc), y = surv24.model$fitted.values, col = leuk$ag, xlim = c(0,15), ylim = c(0,1), ylab = &quot;Survive (Time, surv24wks)&quot;, xlab = &quot;log (wbc counts)&quot;, main = &quot;base R: plot of logistic model of Leuk data&quot;)
lines(x = x.extension, y = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension)/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension)))
lines(x = x.extension, y = exp(surv24.model$coefficients[1] +surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])/(1+exp(surv24.model$coefficient[1] + surv24.model$coefficients[2]*x.extension + surv24.model$coefficients[3])))
legend(&quot;bottomleft&quot;, legend = c(&quot;Ag Absent&quot;, &quot;Ag Present&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1,1))</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>leuk.gg &lt;- data.frame(&quot;logwbc&quot; = log(leuk$wbc), surv24 = q3_subset$surv24, &quot;fv&quot; = surv24.model$fitted.values, &quot;ag&quot; = leuk$ag)

leuk.gg &lt;- cbind(leuk.gg, espframe)
ggplot(leuk.gg, aes(x = logwbc, y = fv, colour = ag)) + 
  geom_point() +
  # scale_colour_discrete(guide = FALSE) +
  # guides(colour=FALSE) +
  geom_line(aes(x = x.extension, y = agpress, colour = &quot;present&quot;)) +
  geom_line(aes(x = x.extension, y = agabs, colour = &quot;absent&quot;)) +
  labs ( title = &quot;ggplot: plot of logistic model of Leuk data&quot;, x = &quot;log of WBC count&quot;, y = &quot;Survive (Time, surv24wks)&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p>Survival Vs WBC count with logistic model on actual data points</p>
<pre class="r"><code># # base plot version
line.1.dat &lt;- leuk.gg[leuk.gg$ag == &#39;absent&#39;, ]
line.2.dat &lt;- leuk.gg[leuk.gg$ag == &#39;present&#39;, ]
plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
plot(
  x = leuk.gg$logwbc,
  y = leuk.gg$surv24,
  xlim=c(0,15),
  ylim = c(0,1),
  col = leuk.gg$ag,
  xlab = &quot;WBC counts&quot;,
  ylab = &quot;Probability of Death prior to 24 Weeks&quot;,
  main = &quot;base R: Survival Vs WBC Counts in Leukaemia Patients&quot;
)
lines(x.extension, leuk.gg$agpress, col = &quot;green&quot;)
lines(x.extension, leuk.gg$agabs, col = &quot;black&quot;)
legend(
  &quot;topleft&quot;,
  title = &quot;AG test&quot;,
  legend = c(&quot;absent&quot;, &quot;present&quot;),
  inset = c(1, 0),
  xpd = TRUE,
  horiz = FALSE,
  col = c(&quot;black&quot;, &quot;green&quot;),
  lty = c(1,1),
  pch = c(1, 2),
  bty = &quot;n&quot;
)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>ggplot(leuk.gg, aes(x = logwbc, y = surv24, color = ag)) +
  geom_point() +
  scale_colour_manual(name = &quot;AG test&quot;, values = c(&#39;black&#39;, &#39;green&#39;)) +
  geom_line(aes(x = x.extension, y = agpress, colour = &quot;present&quot;)) +
  geom_line(aes(x = x.extension, y = agabs, colour = &quot;absent&quot;)) +
  labs(title = &#39;ggplot: Survival Vs WBC Counts in Leukaemia Patients&#39;,
       x = &#39;log WBC Count&#39;,
       y = &#39;Probability of Death prior to 24 Weeks&#39;) +
  theme_classic()</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>We will now fit a model with an interaction term between the two predictors and see which model fits the data better.</p>
<pre class="r"><code>#fitting the model with the interaction term ag * log(wbc)
surv24.model2 &lt;- lm(surv24 ~ ag * log(wbc), data=q3_subset,family=&#39;binomial&#39;)
kable(summary(surv24.model2)$coefficients, caption = &quot;Summary of the linear model with an interaction&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 4: </span>Summary of the linear model with an interaction</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
<th align="right">t value</th>
<th align="right">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.0258017</td>
<td align="right">0.8719219</td>
<td align="right">-0.0295918</td>
<td align="right">0.9765953</td>
</tr>
<tr class="even">
<td align="left">agpresent</td>
<td align="right">2.4360783</td>
<td align="right">1.1398202</td>
<td align="right">2.1372479</td>
<td align="right">0.0411387</td>
</tr>
<tr class="odd">
<td align="left">log(wbc)</td>
<td align="right">0.0286636</td>
<td align="right">0.0898818</td>
<td align="right">0.3189031</td>
<td align="right">0.7520857</td>
</tr>
<tr class="even">
<td align="left">agpresent:log(wbc)</td>
<td align="right">-0.2156187</td>
<td align="right">0.1183543</td>
<td align="right">-1.8218074</td>
<td align="right">0.0788139</td>
</tr>
</tbody>
</table>
<pre class="r"><code>mod2 = summary(surv24.model2)
mod = summary(surv24.model)
# we can also calculate adjusted r-square value for glm using 

mod.rsq.adj = rsq(surv24.model,adj=TRUE,type=c(&#39;v&#39;,&#39;kl&#39;,&#39;sse&#39;,&#39;lr&#39;,&#39;n&#39;))
mod2.rsq.adj = rsq(surv24.model2,adj=TRUE,type=c(&#39;v&#39;,&#39;kl&#39;,&#39;sse&#39;,&#39;lr&#39;,&#39;n&#39;))
# if not using package rsq
# adj.rsq = rbind(mod2$adj.r.squared, (1 -(mod$deviance/mod$null.deviance)) * 32/(32-2-2))
adj.rsq = rbind(mod2.rsq.adj, mod.rsq.adj)

row.names(adj.rsq) &lt;- c(&quot;Linear model with interation&quot;, &quot;Linear model&quot;)
kable(adj.rsq, col.names = &quot;Adjusted R-square values&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Adjusted R-square values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear model with interation</td>
<td align="right">0.2308546</td>
</tr>
<tr class="even">
<td align="left">Linear model</td>
<td align="right">0.1890705</td>
</tr>
</tbody>
</table>
<p>Since the adjusted R-square value for Linear model with the interaction is higher, I would say the model with an interaction fits the data better.</p>
<p>Next, we will load the <span class="math inline">\(\textbf{Default}\)</span> dataset from <span class="math inline">\(\textbf{ISLR}\)</span> library. The dataset contains information on ten thousand customers. The aim here is to predict which
customers will default on their credit card debt. It is a four-dimensional
dataset with 10000 observations. The question of interest is to predict
individuals who will default. We want to examine how each predictor variable is
related to the response (default).</p>
<p>First, we will perform a descriptive analysis on the dataset to have an insight. We can use summaries and appropriate exploratory graphics to answer the question of interest.</p>
<pre class="r"><code># Set up data
data(&quot;Default&quot;, package = &quot;ISLR&quot;)

kable(summary(Default[,1:2]), caption = &quot;Summary of default and student status&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-10">Table 5: </span>Summary of default and student status</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">default</th>
<th align="left">student</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left">No :9667</td>
<td align="left">No :7056</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Yes: 333</td>
<td align="left">Yes:2944</td>
</tr>
</tbody>
</table>
<pre class="r"><code>kable(summary(Default[,3:4]), caption = &quot;Summary of Income and Balance&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-10">Table 5: </span>Summary of Income and Balance</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">balance</th>
<th align="left">income</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left">Min. : 0.0</td>
<td align="left">Min. : 772</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">1st Qu.: 481.7</td>
<td align="left">1st Qu.:21340</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">Median : 823.6</td>
<td align="left">Median :34553</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Mean : 835.4</td>
<td align="left">Mean :33517</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">3rd Qu.:1166.3</td>
<td align="left">3rd Qu.:43808</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Max. :2654.3</td>
<td align="left">Max. :73554</td>
</tr>
</tbody>
</table>
<pre class="r"><code>#create default binary
default_binary     &lt;-
  ifelse(regexpr(&#39;Yes&#39;, Default$default) == -1, 0, 1)
dflt_str &lt;-
  ifelse(regexpr(&#39;Yes&#39;, Default$default) == -1,
         &quot;Not Defaulted&quot;,
         &quot;Defaulted&quot;)

stdn     &lt;- ifelse(regexpr(&#39;Yes&#39;, Default$student) == -1, 0, 1)
stdn_str &lt;-
  ifelse(regexpr(&#39;Yes&#39;, Default$student) == -1, &quot;Not-Student&quot;, &quot;Student&quot;)

blnc &lt;- Default$balance
incm &lt;- Default$income

df &lt;-  data.frame(default_binary, dflt_str, stdn, stdn_str, blnc, incm)

# par(mfrow = c(1, 1))
hist(blnc, main = &quot;Balance&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code># ggplot() + geom_histogram(aes(blnc), bins = 13, color = &quot;black&quot;, fill = &quot;white&quot;)</code></pre>
<p>Here, <code>Balance</code> appears roughly normal.</p>
<pre class="r"><code>hist(incm, main = &quot;Income&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>cat(&quot;Dual means in income appears explained by student status&quot;)</code></pre>
<pre><code>## Dual means in income appears explained by student status</code></pre>
<pre class="r"><code>layout(matrix(1:2, ncol = 2))
hist(
  subset(df$incm, df$stdn == 1),
  main = &quot;Income by Student Status&quot;,
  ylab = &quot;Income&quot;,
  xlab = &quot;Student: Yes&quot;
)
hist(
  subset(df$incm, df$stdn == 0),
  main = &quot;&quot;,
  ylab = &quot;Income&quot;,
  xlab = &quot;Student: No&quot;
)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>Income appears roughly normal with two means</p>
<pre class="r"><code>layout(matrix(1:2, ncol = 2))
hist(
  subset(df$incm, df$default_binary == 1),
  main = &quot;Income by Default Status&quot;,
  ylab = &quot;Income&quot;,
  xlab = &quot;Default: Yes&quot;
)
hist(
  subset(df$incm, df$default_binary == 0),
  main = &quot;&quot;,
  ylab = &quot;Income&quot;,
  xlab = &quot;Default: No&quot;
)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p><strong>And</strong> the dual means in income appears NOT to be explained by default status</p>
<p>Clustering of income v. balance explained by student status:</p>
<pre class="r"><code>plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
plot(
  Default$income ~ Default$balance,
  col = Default$student,
  main = &quot;base R: Income by Balance&quot;,
  ylab = &quot;Income&quot;,
  xlab = &quot;Balance&quot;,
  pch = 18
)
legend(
  &quot;topright&quot;,
  c(&quot;Yes&quot;, &quot;No&quot;),
  title = &quot;Student?&quot;,
  # bty = &quot;n&quot;,
  fill = c(&quot;red&quot;, &quot;black&quot;),
  pch = c(18,18)
)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>ggplot(data = Default, aes(x = balance, y = income, colour = student)) + 
  geom_point() +
  labs(title = &quot;ggplot: Income by Balance&quot;) + 
  guides(colour=guide_legend(title=&quot;Student?&quot;)) +
  scale_color_manual(values = c(&quot;No&quot; = &quot;black&quot;, &quot;Yes&quot; = &quot;red&quot;))</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<pre class="r"><code>plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
boxplot(balance~student, data = Default, main = &quot;base R: Balance grouped by Student status&quot;, xlab = &quot;student&quot;, ylab = &quot;balance&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-3.png" width="672" /></p>
<pre class="r"><code>ggplot(data = Default, aes(x = student, y = balance)) +
  geom_boxplot() +
  labs(title = &quot;ggplot: Balance grouped by Student status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-4.png" width="672" /></p>
<pre class="r"><code>plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
boxplot(balance~default, data = Default, main = &quot;base R: Balance grouped by Default status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-5.png" width="672" /></p>
<pre class="r"><code>ggplot(data = Default, aes(x = default, y = balance)) +
  geom_boxplot() +
  labs(title = &quot;ggplot: Balance grouped by Default status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-6.png" width="672" /></p>
<pre class="r"><code>plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
boxplot(income~student, data = Default, main = &quot;base R: Income grouped by Student status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-7.png" width="672" /></p>
<pre class="r"><code>ggplot(data = Default, aes(x = student, y = income)) +
  geom_boxplot() +
  labs(title = &quot;ggplot: Income grouped by Student status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-8.png" width="672" /></p>
<pre class="r"><code>plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
boxplot(income~default, data = Default, main = &quot;base R: Income grouped by Default status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-9.png" width="672" /></p>
<pre class="r"><code>ggplot(data = Default, aes(x = default, y = income)) +
  geom_boxplot() +
  labs(title = &quot;ggplot: Income grouped by Default status&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-13-10.png" width="672" /></p>
<pre class="r"><code>tapply(df$incm, df$dflt_str, FUN = summary)</code></pre>
<pre><code>## $Defaulted
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    9664   19028   31515   32089   43067   66466 
## 
## $`Not Defaulted`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     772   21405   34589   33566   43824   73554</code></pre>
<p>Median and Max income are lower for defaulted than not defaulted loans.</p>
<pre class="r"><code>tapply(df$blnc, df$dflt_str, FUN = summary)</code></pre>
<pre><code>## $Defaulted
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   652.4  1511.6  1789.1  1747.8  1988.9  2654.3 
## 
## $`Not Defaulted`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     0.0   465.7   802.9   803.9  1128.2  2391.0</code></pre>
<p>Median and max balance are higher for defaulted rather than not defaulted loans.</p>
<p>Based on the outputs, we can tell that a fewer people default compare to those that don’t default. Defaulters and non-defaulters appear to have the same income range, given student status. Defaulters appear to have higher balances. Also note, if students default, they likely do it with over <span class="math inline">\({1,000}\)</span> balance. If non-students default, they are likely do it with over <span class="math inline">\({500}\)</span> balance.</p>
<p>We then build a logistic regression model to see which predictor variables were important or if there were any interactions.</p>
<pre class="r"><code># Also see, https://stackoverflow.com/questions/13366755/what-does-the-r-formula-y1-mean
regression_model0 &lt;- glm(default_binary ~ stdn + blnc + incm, family = binomial())
summary(regression_model0)</code></pre>
<pre><code>## 
## Call:
## glm(formula = default_binary ~ stdn + blnc + incm, family = binomial())
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4691  -0.1418  -0.0557  -0.0203   3.7383  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.087e+01  4.923e-01 -22.080  &lt; 2e-16 ***
## stdn        -6.468e-01  2.363e-01  -2.738  0.00619 ** 
## blnc         5.737e-03  2.319e-04  24.738  &lt; 2e-16 ***
## incm         3.033e-06  8.203e-06   0.370  0.71152    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.5  on 9996  degrees of freedom
## AIC: 1579.5
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<pre class="r"><code># with interactions
regression_model1 &lt;- glm(default_binary ~ stdn + blnc + incm + stdn * blnc + stdn * incm + blnc * incm, family = binomial())
summary(regression_model1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = default_binary ~ stdn + blnc + incm + stdn * blnc + 
##     stdn * incm + blnc * incm, family = binomial())
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4848  -0.1417  -0.0554  -0.0202   3.7579  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.104e+01  1.866e+00  -5.914 3.33e-09 ***
## stdn        -5.201e-01  1.344e+00  -0.387    0.699    
## blnc         5.882e-03  1.180e-03   4.983 6.27e-07 ***
## incm         4.050e-06  4.459e-05   0.091    0.928    
## stdn:blnc   -2.551e-04  7.905e-04  -0.323    0.747    
## stdn:incm    1.447e-05  2.779e-05   0.521    0.602    
## blnc:incm   -1.579e-09  2.815e-08  -0.056    0.955    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.1  on 9993  degrees of freedom
## AIC: 1585.1
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>Without taking interactions into account, it appears that two predictors-
student and balance are significant. With interactions involved, it appears that
only balance predictor is important.</p>
<p>Next, we will assess the performance of the logistic regression classifier based on the error rates.</p>
<pre class="r"><code># Error Rate
dflt.fitted0 &lt;- predict(regression_model0, type = &quot;response&quot;)
dflt.fitted1 &lt;- predict(regression_model1, type = &quot;response&quot;)

levs &lt;- c(&quot;Defaulted&quot;, &quot;Not Defaulted&quot;)
Tr &lt;- default_binary

Predicted0 &lt;-
  factor(ifelse(dflt.fitted0 &gt;= 0.50, &quot;Defaulted&quot;, &quot;Not Defaulted&quot;),
         levels = levs)
Predicted1 &lt;-
  factor(ifelse(dflt.fitted1 &gt;= 0.50, &quot;Defaulted&quot;, &quot;Not Defaulted&quot;),
         levels = levs)
Tr1 &lt;-
  factor(ifelse(Tr &gt;= 0.50, &quot;Defaulted&quot;, &quot;Not Defaulted&quot;), levels = levs)
rate0 &lt;- table(Predicted0, True = Tr1)
rate1 &lt;- table(Predicted1, True = Tr1)
rate0</code></pre>
<pre><code>##                True
## Predicted0      Defaulted Not Defaulted
##   Defaulted           105            40
##   Not Defaulted       228          9627</code></pre>
<pre class="r"><code>error_rate0 &lt;- 1 - (rate0[1, 1] + rate0[2, 2]) / sum(rate0)
error_rate0</code></pre>
<pre><code>## [1] 0.0268</code></pre>
<pre class="r"><code>rate1</code></pre>
<pre><code>##                True
## Predicted1      Defaulted Not Defaulted
##   Defaulted           104            40
##   Not Defaulted       229          9627</code></pre>
<pre class="r"><code>error_rate1 &lt;- 1 - (rate1[1, 1] + rate1[2, 2]) / sum(rate1)
error_rate1</code></pre>
<pre><code>## [1] 0.0269</code></pre>
<pre class="r"><code># analysis of variance
anova(regression_model0, regression_model1, test = &#39;Chisq&#39;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: default_binary ~ stdn + blnc + incm
## Model 2: default_binary ~ stdn + blnc + incm + stdn * blnc + stdn * incm + 
##     blnc * incm
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1      9996     1571.5                     
## 2      9993     1571.1  3  0.47911   0.9235</code></pre>
<p>The model without interactions has an AIC of 1579.5 and the interaction model has
an AIC of 1585.1 (slightly higher). But, both have almost similar error rates
~2.7%. Also, since analysis of deviance also shows that the chi-square test has
no significance at 5% level, we can conclude that both models are almost the
same as a working model.</p>
<p>Here, we will perform additional exploratory analysis of the dataset.</p>
<pre class="r"><code># density plot
plasma &lt;- plasma

layout(matrix(1:2,ncol=2))
cdplot(ESR ~ fibrinogen, data=plasma)
cdplot(ESR ~ globulin,data=plasma)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>It appears that above a certain level of fibrogen, ESR drops sucessively. This is not the case for globulin. ESR Logistic Regression and Confidence Interval Estimates:</p>
<pre class="r"><code>plasma_glm_1 &lt;- glm(ESR ~ fibrinogen, data = plasma, family=binomial())
confint(plasma_glm_1,parm=&#39;fibrinogen&#39;)</code></pre>
<pre><code>##     2.5 %    97.5 % 
## 0.3387619 3.9984921</code></pre>
<p>Here, fibrinogen might have value as a predictor of ESR. We can look at the summary.</p>
<pre class="r"><code>summary(plasma_glm_1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = ESR ~ fibrinogen, family = binomial(), data = plasma)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9298  -0.5399  -0.4382  -0.3356   2.4794  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)  -6.8451     2.7703  -2.471   0.0135 *
## fibrinogen    1.8271     0.9009   2.028   0.0425 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 30.885  on 31  degrees of freedom
## Residual deviance: 24.840  on 30  degrees of freedom
## AIC: 28.84
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The summary output indicates a 5% significance of fibrinogenand and increase of the log-odds of ESR &gt; 20 by about 1.83 with confidence interval (CI) of 0.33 to 3.99.</p>
<pre class="r"><code>exp(coef(plasma_glm_1)[&#39;fibrinogen&#39;])</code></pre>
<pre><code>## fibrinogen 
##   6.215715</code></pre>
<p>Fibrinogen might have value as a predictor of ESR. To make the results more readable, it is useful to apply an exponent function. This exponentiates the log-odds of fibriogen and CI to correspond with the data.</p>
<pre class="r"><code>exp(confint(plasma_glm_1, parm=&#39;fibrinogen&#39;))</code></pre>
<pre><code>##     2.5 %    97.5 % 
##  1.403209 54.515884</code></pre>
<p>We can also perform logistic regression of both explanatory variables (fibrinogen and globulin) and text for the deviance.</p>
<pre class="r"><code>plasma_glm_2 &lt;- glm(ESR ~ fibrinogen + globulin, data = plasma, family = binomial())
summary(plasma_glm_2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = ESR ~ fibrinogen + globulin, family = binomial(), 
##     data = plasma)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9683  -0.6122  -0.3458  -0.2116   2.2636  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -12.7921     5.7963  -2.207   0.0273 *
## fibrinogen    1.9104     0.9710   1.967   0.0491 *
## globulin      0.1558     0.1195   1.303   0.1925  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 30.885  on 31  degrees of freedom
## Residual deviance: 22.971  on 29  degrees of freedom
## AIC: 28.971
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>cat(&quot;# comparison of models&quot;)</code></pre>
<pre><code>## # comparison of models</code></pre>
<pre class="r"><code>anova(plasma_glm_1, plasma_glm_2, test= &#39;Chisq&#39;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: ESR ~ fibrinogen
## Model 2: ESR ~ fibrinogen + globulin
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1        30     24.840                     
## 2        29     22.971  1   1.8692   0.1716</code></pre>
<p>Now, we can make the bubble plot of the predicted values of model <code>plasma_glm_2</code>. The plot shows that the probablity of ‘good’ ESR reading increases as fibrinogen increases. This is true of globulin only up to a point.</p>
<pre class="r"><code>prob &lt;- predict(plasma_glm_2, type=&#39;response&#39;)

plot.new()
par(mfrow = c(1, 1), pty = &quot;s&quot;)
plot(globulin ~ fibrinogen,data=plasma,xlim=c(2,6),ylim=c(25,55),pch=&#39;.&#39;, main = &quot;Bubble plot of the predicted values of model II&quot;)
symbols(plasma$fibrinogen,plasma$globulin,circles=prob,add=T)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div id="part-2" class="section level2">
<h2>PART 2</h2>
<p>In this section, we use the <span class="math inline">\(\textbf{bladdercancer}\)</span> data from the <span class="math inline">\(\textbf{HSAUR3}\)</span> library. We will Construct graphical and numerical summaries that will show the relationship between tumor size and the number of recurrent tumors. In this case a mosaic plot may be a great way to assess this relationship.</p>
<pre class="r"><code>data(&quot;bladdercancer&quot;, package = &quot;HSAUR3&quot;)
# base R plot version
# head(bladdercancer)
mosaicplot(xtabs( ~ number + tumorsize, data = bladdercancer),
           main = &quot;base R: The Number of recurrent tumors compared with tumor size&quot;,
           shade = TRUE)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code># ggplot version:
# install.packages(&#39;ggmosaic&#39;)
library(ggmosaic)
ggplot(data = bladdercancer) +
  geom_mosaic(aes(x = product(tumorsize, number), fill = tumorsize), na.rm =
                FALSE) +
  labs(x = &quot;Number&quot;, x = &quot;Tumour Size&quot;, title = &#39;ggplot: The Number of recurrent tumors compared with tumor size&#39;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
<pre class="r"><code># We can also visualize this by creating percentage table using `prop.table`
# function.
table_rows_percentage &lt;-
  table(bladdercancer$tumorsize, bladdercancer$number)
colnames(table_rows_percentage) &lt;-
  c(&quot;Tumour_1 (counts)&quot;,
    &quot;Tumour_2 (counts)&quot;,
    &quot;Tumour_3 (counts)&quot;,
    &quot;Tumour_4 (counts)&quot;)
cat(&quot;Table of tumour number and frequency:&quot;)</code></pre>
<pre><code>## Table of tumour number and frequency:</code></pre>
<pre class="r"><code>table_rows_percentage</code></pre>
<pre><code>##        
##         Tumour_1 (counts) Tumour_2 (counts) Tumour_3 (counts) Tumour_4 (counts)
##   &lt;=3cm                15                 5                 1                 1
##   &gt;3cm                  5                 2                 1                 1</code></pre>
<pre class="r"><code>tt &lt;- prop.table(table_rows_percentage, 1)
colnames(tt) &lt;-
  c(&quot;Tumour_1(%)&quot;, &quot;Tumour_2(%)&quot;, &quot;Tumour_3(%)&quot;, &quot;Tumour_4(%)&quot;)

# Table of tumour number and frequency in %:
tt</code></pre>
<pre><code>##        
##         Tumour_1(%) Tumour_2(%) Tumour_3(%) Tumour_4(%)
##   &lt;=3cm  0.68181818  0.22727273  0.04545455  0.04545455
##   &gt;3cm   0.55555556  0.22222222  0.11111111  0.11111111</code></pre>
<p>Based on the mosaic plot, frequency table or the percentage table above, we can tell that the observed frequency for 1 or 2 tumors greater than 3cm (&gt;3cm) is lower than expected and the observed frequency for 3 or 4 tumors less than or equal to 3 cm (&lt;=3cm) is also lower than what we would expect for this data.</p>
<p>Next, we will build a Poisson regression that estimates the effect of size of tumor on the number of recurrent tumors.</p>
<p>If we test the model dropping the time variable. It shows that the intercept is significant (P&lt;0.05), but the tumour size is not significant.</p>
<pre class="r"><code>mod1 &lt;- glm(number ~ tumorsize,data=bladdercancer,family=poisson())
summary(mod1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = number ~ tumorsize, family = poisson(), data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6363  -0.3996  -0.3996   0.4277   1.7326  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)     0.3747     0.1768   2.120    0.034 *
## tumorsize&gt;3cm   0.2007     0.3062   0.655    0.512  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.80  on 30  degrees of freedom
## Residual deviance: 12.38  on 29  degrees of freedom
## AIC: 87.191
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Additionally, we can also test models considering the time interaction. If we consider time interaction with the tumour size, we can clearly see that none of the variables are significant.</p>
<pre class="r"><code>mod2 &lt;- glm(number ~time + tumorsize + tumorsize*time,data=bladdercancer,family=poisson(link=log))
summary(mod2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = number ~ time + tumorsize + tumorsize * time, family = poisson(link = log), 
##     data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6943  -0.5581  -0.2413   0.2932   1.4644  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)         0.03957    0.43088   0.092    0.927
## time                0.02138    0.02418   0.884    0.377
## tumorsize&gt;3cm       0.46717    0.66713   0.700    0.484
## time:tumorsize&gt;3cm -0.01676    0.03821  -0.439    0.661
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.800  on 30  degrees of freedom
## Residual deviance: 11.566  on 27  degrees of freedom
## AIC: 90.377
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>If we drop time interaction from previous model (mod2), we still do not get anything significant with the time or tumour size. However, the AIC value drops to 88.56.</p>
<pre class="r"><code>mod3 &lt;- glm(number ~ time + tumorsize,data=bladdercancer,family=poisson())
summary(mod3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = number ~ time + tumorsize, family = poisson(), 
##     data = bladdercancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8183  -0.4753  -0.2923   0.3319   1.5446  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)    0.14568    0.34766   0.419    0.675
## time           0.01478    0.01883   0.785    0.433
## tumorsize&gt;3cm  0.20511    0.30620   0.670    0.503
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 12.800  on 30  degrees of freedom
## Residual deviance: 11.757  on 28  degrees of freedom
## AIC: 88.568
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>In all three models we compared, we can also see that the residual and null deviance values are low compared to the degrees of freedom. If our null deviance is really small, it means that the null model explains the data pretty well. Likewise, with our residual deviance. Additionaly, we can perform a Chi-squared test for the null deviance to check
whether any of the predictors have an influence on the response variables in
our three models using function <code>pchisq</code>:</p>
<pre class="r"><code># Source: https://stat.ethz.ch/education/semesters/as2015/asr/Uebungen/Uebungen/solution8.pdf
pchisq((mod1$null.deviance-mod1$deviance), df = (mod1$df.null-mod1$df.residual), lower = FALSE)</code></pre>
<pre><code>## [1] 0.5171827</code></pre>
<pre class="r"><code>pchisq((mod2$null.deviance-mod2$deviance), df = (mod2$df.null-mod2$df.residual), lower = FALSE)</code></pre>
<pre><code>## [1] 0.7448414</code></pre>
<pre class="r"><code>pchisq((mod3$null.deviance-mod3$deviance), df  = (mod3$df.null-mod3$df.residual), lower = FALSE)</code></pre>
<pre><code>## [1] 0.5935891</code></pre>
<p>The p-values in all three models are larger than 0.05, which tells us that there is no significant predictor in our model. Additionally, if we compare all three models we built above for analysis of deviance using ANOVA, we do not find any of these models to be significant.</p>
<pre class="r"><code>anova(mod1,mod2,mod3,test=&#39;Chisq&#39;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: number ~ tumorsize
## Model 2: number ~ time + tumorsize + tumorsize * time
## Model 3: number ~ time + tumorsize
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1        29     12.380                     
## 2        27     11.566  2  0.81458   0.6655
## 3        28     11.757 -1 -0.19095   0.6621</code></pre>
<p>Based on these analyses, we can tell that the acceptance of the null hypothesis is evident in this case because there is nothing within the data to explain an increment in the number of tumors. Since we tested both tumour size and time variables, we can tell that <strong>neither time</strong> nor the <strong>tumour size</strong> have any effect on increasing <strong>number</strong> of tumours.</p>
<p>The following data is the number of new AIDS cases in Belgium between the years 1981-1993. Let <span class="math inline">\(t\)</span> denote time</p>

<p>Do the following</p>
<p>With this, we will first plot the relationship between AIDS cases against time.</p>
<pre class="r"><code>y = c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240)
t = 1:13

data &lt;- as.data.frame(cbind(t, y))

# base R plot version
plot(y ~ t,
     main = &quot;base R: Number of AIDs cases from 1981-1993&quot;,
     xlab = &quot;Time in Years from 1981&quot;,
     ylab = &quot;Number of Aids cases&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<pre class="r"><code># ggplot version
ggplot() + aes(x = t, y = y) + geom_point() + labs(title = &quot;ggplot: Number of AIDs cases from 1981-1993&quot;, x = &quot;Time in Years from 1981&quot;, y = &quot;Number of Aids cases&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-31-2.png" width="672" /></p>
<p>Here, the number of new AIDS cases has an increasing trend over time and seems to be leveling off between 1981-1991 then it remains somewhat unchanged until 1993. The maximum number of new AIDS cases occurs in 1991.</p>
<p>Then, fit a Poisson regression model <span class="math inline">\(log(\mu_i)=\beta_0+\beta_1t_i\)</span>.</p>
<pre class="r"><code>cat(&quot;#2b&quot;)</code></pre>
<pre><code>## #2b</code></pre>
<pre class="r"><code>#Poisson model
aids.pois &lt;- glm(y ~ t, data = data, family = &quot;poisson&quot;)
summary(aids.pois)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ t, family = &quot;poisson&quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.6784  -1.5013  -0.2636   2.1760   2.7306  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) 3.140590   0.078247   40.14   &lt;2e-16 ***
## t           0.202121   0.007771   26.01   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 872.206  on 12  degrees of freedom
## Residual deviance:  80.686  on 11  degrees of freedom
## AIC: 166.37
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code># Coefficients
exp(coef(aids.pois)) # coefficients</code></pre>
<pre><code>## (Intercept)           t 
##   23.117491    1.223996</code></pre>
<pre class="r"><code>exp(confint(aids.pois)) # confidence interval</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 19.789547 26.894433
## t            1.205624  1.242922</code></pre>
<pre class="r"><code>#use code below for residual plots
plot(aids.pois, which = 1, main = &quot;base R: Residual Vs fitted plot for y ~ t&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre class="r"><code># https://stackoverflow.com/questions/36731027/how-can-i-plot-the-residuals-of-lm-with-ggplot
# ggplot version
ggplot(aids.pois, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth(group = 1, formula = y ~ x) + labs(title = &quot;ggplot: Residual Vs fitted plot for y ~ t&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
<p>Here, both (b0) and (b1) are statistically significant from zero. Interpretation of the coefficients calculated by exponentiating the estimates:
exp(b1) =1.22: A one year increase will result in a 22% increase in the mean number of new AIDs cases.<br />
exp(b0)=23.1: When t=0, the average number of AID cases is 23.1.</p>
<p>Likewise, comparing the residual deviance of the model, we can tell that the model is over-spread by 7.80 times on 11 degrees of freedom. Based on the residual plot, the residual values are further away from zero at time 1, 2, and 13,indicating they are outliers. Additionally, there is a clear pattern to the residual plot which indicates that mean does not increase as the variance increases because there is not a constant spread in the residuals.</p>
<p>Additionally, we can see a curved pattern in the Residual vs. Fitted plot. This tells us that a transformation or adding a quadratic term to the model would be suitable.</p>
<p>So, we add a quadratic term in time ( ) and fit the model.</p>
<pre class="r"><code>data$t2 &lt;- data$t ^ 2
aids2.pois &lt;- glm(y ~ t + t2, data = data, family = &quot;poisson&quot;)
summary(aids2.pois)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ t + t2, family = &quot;poisson&quot;, data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.45903  -0.64491   0.08927   0.67117   1.54596  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.901459   0.186877  10.175  &lt; 2e-16 ***
## t            0.556003   0.045780  12.145  &lt; 2e-16 ***
## t2          -0.021346   0.002659  -8.029 9.82e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 872.2058  on 12  degrees of freedom
## Residual deviance:   9.2402  on 10  degrees of freedom
## AIC: 96.924
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code># Coefficients
exp(coef(aids2.pois)) # coefficients</code></pre>
<pre><code>## (Intercept)           t          t2 
##   6.6956535   1.7436895   0.9788799</code></pre>
<pre class="r"><code>exp(confint(aids2.pois)) # confidence interval</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 4.5976982 9.5678396
## t           1.5965138 1.9104525
## t2          0.9737254 0.9839292</code></pre>
<pre class="r"><code>#use code below for residual plots
plot(aids2.pois, which = 1, main = &quot;base R: Residual Vs fitted plot for y ~ t + t2&quot;)</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code># ggplot version
ggplot(aids2.pois, aes(x = .fitted, y = .resid)) + geom_point() + labs(title = &quot;ggplot: Residuals Vs fitted plot for y ~ t + t2&quot;) + geom_smooth()</code></pre>
<p><img src="/post/Logistic_regression/Logistic_Regression_files/figure-html/unnamed-chunk-33-2.png" width="672" /></p>
<p>Here also, all the model parameters are statistically significant from zero.</p>
<p>Interpretation of the coefficients calculated by exponentiating the estimates:
exp(b1) =1.74: Taking all other parameters constant, a one year increase will result in a 74% increase in the mean number of new AID cases.</p>
<p>exp(b2) =0.98 : Taking all other parameters constant, a one year increase will result in a 2% decrease in the mean number of new AID cases.</p>
<p>exp(b0) =6.7 : When t=0 and <span class="math inline">\(t^2\)</span>=0, the average number of AID cases is 6.7.</p>
<p>Additionally, the residuals vs. fitted values plot looks much better than the previous model, and the residuals seems randomly distributed around 0.</p>
<p>We will now compare the two models using AIC to see which of the two models is better.</p>
<pre class="r"><code>AIC(aids.pois)</code></pre>
<pre><code>## [1] 166.3698</code></pre>
<pre class="r"><code>AIC(aids2.pois)</code></pre>
<pre><code>## [1] 96.92358</code></pre>
<p>Based on the AIC values and the residual plots, model 2 is a better fit for this data.</p>
<p>Now, we use <span class="math inline">\(\textit{ anova()}\)</span>-function to perform <span class="math inline">\(\chi^2\)</span> test for model selection. Did adding the quadratic term improve model?</p>
<pre class="r"><code>anova(aids.pois, aids2.pois, test = &quot;Chisq&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: y ~ t
## Model 2: y ~ t + t2
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1        11     80.686                          
## 2        10      9.240  1   71.446 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Based on the chi-square test statistic and p-value—in this case we reject the null hypothesis at the <span class="math inline">\(\textbf{alpha}\)</span> = 0.05 level that model 1 is true. We can tell that the larger model is better, which in this case, adding the quadratic term did improve the model.</p>
<p>Next, we will load the <span class="math inline">\(\textbf{ Default}\)</span> dataset from <span class="math inline">\(\textbf{ISLR}\)</span> library. The dataset contains information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt. It is a 4 dimensional dataset with 10000 observations. We had developed a logistic regression model previously. Now consider the following two models</p>

<p>For the two competing models do the following</p>
<p>Here, with the whole data, we will compare the two models. We can use AIC and/or error rate for comparison.</p>
<pre class="r"><code>data(&quot;Default&quot;, package = &quot;ISLR&quot;)
Default$default&lt;-as.numeric(Default$default==&quot;Yes&quot;)

mod.log1&lt;-glm(default ~ student + balance , data = Default, family = binomial())
# summary(mod.log1)

mod.log2&lt;-glm(default ~ balance , data = Default, family = binomial())

cat(&quot;AIC for mod.log1:&quot;)</code></pre>
<pre><code>## AIC for mod.log1:</code></pre>
<pre class="r"><code>AIC(mod.log1)</code></pre>
<pre><code>## [1] 1577.682</code></pre>
<pre class="r"><code>cat(&quot;AIC for mod.log2:&quot;)</code></pre>
<pre><code>## AIC for mod.log2:</code></pre>
<pre class="r"><code>AIC(mod.log2)</code></pre>
<pre><code>## [1] 1600.452</code></pre>
<pre class="r"><code>anova(mod.log1, mod.log2, test=&quot;Chisq&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: default ~ student + balance
## Model 2: default ~ balance
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1      9997     1571.7                          
## 2      9998     1596.5 -1   -24.77 6.459e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Here, the first model with both student and balance has the smaller AIC. The anova-function was also used to perform a chi-square test for model selection and again concluded the first model was better.</p>
<p>Now, we will use a validation set approach to choose the best model. Be aware that we have a few people who defaulted in the data.</p>
<pre class="r"><code># Validation approach&quot;
index&lt;-sample(1:nrow(Default), size=0.6*nrow(Default))
train&lt;- Default[index, ]
val&lt;- Default[-index, ]


mod.train1&lt;-glm(default ~ student + balance , data = train, family = binomial())
# summary(mod.train1)
mod.train2&lt;-glm(default ~ balance , data = train, family = binomial())
# summary(mod.train2)
pred1&lt;-predict(mod.train1, val, type = &quot;response&quot;)
pred2&lt;-predict(mod.train2, val, type = &quot;response&quot;)

# Error rate:
err.rate1&lt;- mean((pred1&gt;0.5 &amp; val$default==0) | (pred1&lt;0.5 &amp; val$default==1))
err.rate2&lt;- mean((pred2&gt;0.5 &amp; val$default==0) | (pred2&lt;0.5 &amp; val$default==1))

cat(&quot;Error rate of model1 =&quot;, err.rate1)</code></pre>
<pre><code>## Error rate of model1 = 0.02525</code></pre>
<pre class="r"><code>cat(&quot;Error rate of model2 =&quot;, err.rate2)</code></pre>
<pre><code>## Error rate of model2 = 0.027</code></pre>
<p>Here, we split the data into 60/40 between the training and validation data sets and made sure the default rate was similar between the two dataset. Then fitted the models to the training data and used the validation set to calculate the error rate using 0.5 as out threshold.</p>
<p>For model 1, the MSE is 0.023.
For model 2, the MSE is 0.024.</p>
<p>Based on these values we would chose model 1 as our best model. We will also examine other validation techniques below.</p>
<p>Next, we will use LOOCV approach to choose the best model.</p>
<pre class="r"><code># 3c LOOCV
library(boot)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)
cv.err &lt;- cv.glm(Default,mod.log1, cost)$delta
cv.err2 &lt;- cv.glm(Default, mod.log2, cost)$delta

cat(&quot;LOOCV of model1&quot;)</code></pre>
<pre><code>## LOOCV of model1</code></pre>
<pre class="r"><code>cv.err</code></pre>
<pre><code>## [1] 0.0267 0.0267</code></pre>
<pre class="r"><code>cat(&quot;LOOCV of model2&quot;)</code></pre>
<pre><code>## LOOCV of model2</code></pre>
<pre class="r"><code>cv.err2</code></pre>
<pre><code>## [1] 0.02750000 0.02749994</code></pre>
<p>LOOCV prediction error is adjusted for bias and we still want the smallest prediction errors.
For model 1, the adjusted prediction error is 0.0267.
For model 2, the adjusted prediction error is 0.02749994.</p>
<p>Therefore, we choose model 1 as the best model because it has the smaller adjusted prediction rate using the LOOCV approach.</p>
<p>We will now use 10-fold cross-validation approach to choose the best model. We will report on the validation misclassification (error) rate for both models in each of the three assessment methods.</p>
<pre class="r"><code># 3d 10-fold cross validation
cv.err1.10 &lt;- cv.glm(Default, mod.log1, cost ,K=10)$delta
cv.err2.10 &lt;- cv.glm(Default, mod.log2, cost ,K=10)$delta

cat(&quot;10-fold cross validation of Model1&quot;)</code></pre>
<pre><code>## 10-fold cross validation of Model1</code></pre>
<pre class="r"><code>cv.err1.10</code></pre>
<pre><code>## [1] 0.02670 0.02669</code></pre>
<pre class="r"><code>cat(&quot;10-fold cross validation of Model2&quot;)</code></pre>
<pre><code>## 10-fold cross validation of Model2</code></pre>
<pre class="r"><code>cv.err2.10</code></pre>
<pre><code>## [1] 0.02770 0.02768</code></pre>
<p>Using K=10 for the 10-fold cross-validation approach, we obtain the following error rates:
For model 1, the CV error rate is 0.02667
For model 2, the CV error rate is 0.0278</p>
<p>Again, we can choose model 1 as our best model. Though it was little easier to calculate the 10-fold cross validation error rate than the LOOCV error rate, but our conclusion is still the same.</p>
<p>From the <span class="math inline">\(\textbf{ISLR}\)</span> library, we will load the <span class="math inline">\(\textbf{Smarket}\)</span> dataset. This contains Daily percentage returns for the <code>S&amp;P 500 stock</code> index between 2001 and 2005. There are 1250 observations and 9 variables. The variable of interest is Direction which is a factor with levels Down and Up indicating whether the market had a positive or negative return on a given day. Since the goal is to predict the direction of the stock market in the future, here it would make sense to use the data from years 2001 - 2004 as training and 2005 as validation. According to this, create a training set and testing set.</p>
<p>We will Perform logistic regressions and assess the error rates.</p>
<pre class="r"><code>data(&quot;Smarket&quot;, package = &quot;ISLR&quot;)
Smarket$Direction &lt;- as.numeric(Smarket$Direction == &quot;Up&quot;)

train.mark &lt;- subset(Smarket, Year &lt;= 2004)
val.mark &lt;- subset(Smarket, Year &gt; 2004)


#Model 1
mod.train.mark &lt;-
  glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 ,
      data = train.mark,
      family = binomial())
summary(mod.train.mark)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, family = binomial(), 
##     data = train.mark)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.339  -1.189   1.070   1.163   1.326  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  0.032269   0.063379   0.509    0.611
## Lag1        -0.055510   0.051706  -1.074    0.283
## Lag2        -0.044218   0.051681  -0.856    0.392
## Lag3         0.008918   0.051517   0.173    0.863
## Lag4         0.008556   0.051514   0.166    0.868
## Lag5        -0.003243   0.051089  -0.063    0.949
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1383.3  on 997  degrees of freedom
## Residual deviance: 1381.3  on 992  degrees of freedom
## AIC: 1393.3
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<pre class="r"><code>err.rate1 &lt;-
  mean((
    predict(mod.train.mark, val.mark, type = &quot;response&quot;) - val.mark$Direction
  ) ^ 2)

cat(&quot;Error rate model1:&quot;)</code></pre>
<pre><code>## Error rate model1:</code></pre>
<pre class="r"><code>err.rate1</code></pre>
<pre><code>## [1] 0.2483559</code></pre>
<pre class="r"><code>#Model 2
mod.train.mark2 &lt;-
  glm(Direction ~ Lag1 + Lag2 + Lag3,
      data = train.mark,
      family = binomial())
summary(mod.train.mark2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3, family = binomial(), 
##     data = train.mark)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.338  -1.189   1.072   1.163   1.335  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  0.032230   0.063377   0.509    0.611
## Lag1        -0.055523   0.051709  -1.074    0.283
## Lag2        -0.044300   0.051674  -0.857    0.391
## Lag3         0.008815   0.051495   0.171    0.864
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1383.3  on 997  degrees of freedom
## Residual deviance: 1381.4  on 994  degrees of freedom
## AIC: 1389.4
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<pre class="r"><code>err.rate2 &lt;-
  mean((
    predict(mod.train.mark2, val.mark, type = &quot;response&quot;) - val.mark$Direction
  ) ^ 2)

cat(&quot;Error rate model2:&quot;)</code></pre>
<pre><code>## Error rate model2:</code></pre>
<pre class="r"><code>err.rate2</code></pre>
<pre><code>## [1] 0.2483144</code></pre>
<p>The error rate for model 1 which includes predictor variables lag 1-5 is: 0.4126984.
The error rate for model 2 which includes predictor variables lag 1-3 is: 0.4087302.</p>
<p>So, we can choose the simpler model 2 based on the error rate. This error rate suggests that we are able to predict the direction of the stock market. We can predict the right outcome at around 60% of the time, which is still better than predicting randomly.</p>
</div>
