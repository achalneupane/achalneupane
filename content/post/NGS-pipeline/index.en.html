---
title: "NGS_pipeline"
author: "Achal Neupane"
date: 2020-12-01T21:13:14-05:00
categories: ["Shell"]
tags: ["NGS", "pipeline"]
---



<div id="docker-images-for-ngs-data-analysis" class="section level2">
<h2>Docker images for NGS data analysis</h2>
<div id="author-achal-neupane-phd" class="section level3">
<h3>Author: Achal Neupane, PhD</h3>
<p>These docker images are publicly available on my dockerhub: <a href="https://hub.docker.com/u/achalneupane" class="uri">https://hub.docker.com/u/achalneupane</a></p>
<p><a href="#summary">Summary</a></p>
<p><a href="#variant-calling">Variant calling</a></p>
<p><a href="#1--from-raw-data-to-gvcf">1 – From raw data to gVCF</a></p>
<p><a href="#main-pipeline">Main pipeline</a></p>
<p><a href="#alternative-pipelines">Alternative pipelines</a></p>
<p><a href="#2--joint-calling">2 – Joint Calling</a></p>
<p><a href="#quality-control-qc">Quality Control (QC)</a></p>
<p><a href="#1--vqsr">1 – VQSR</a></p>
<p><a href="#2-hard-filtering">2 –Hard Filtering</a></p>
<p><a href="#3-plink-qc">3 –PLINK QC</a></p>
<p><a href="#hardy-weinberg-equilibrium">Hardy Weinberg Equilibrium</a></p>
<p><a href="#genotyping-rate">Genotyping Rate</a></p>
<p><a href="#missingness-per-individual">Missingness per individual</a></p>
<p><a href="#missingness-per-wxs">Missingness per WXS</a></p>
<p><a href="#missingness-per-sequencing-project">Missingness per Sequencing Project</a></p>
<p><a href="#heteorzigosity">Heteorzigosity</a></p>
<p><a href="#sex-check">Sex Check</a></p>
<p><a href="#pcas">PCAs</a></p>
<p><a href="#pca-with-hapmap">PCA with Hapmap</a></p>
<p><a href="#duplicates---ibd">Duplicates - IBD</a></p>
<p><a href="#ibd">IBD</a></p>
<p><a href="#appendices">Appendices</a></p>
<p><a href="#appendix-a--code-to-upload-samples-as-they-go--pipeline-a">Appendix A – Code to upload samples as they go + pipeline
A</a></p>
<p><a href="#appendix-b--pipeline-b">Appendix B – Pipeline B</a></p>
<p><a href="#appendix-c--pipeline-c-cram-or-bam-to-fastq">Appendix C – pipeline C Cram or Bam to
FastQ</a></p>
<p><a href="#appendix-d--bam-to-fastq-processing-specific-to-the-adni-project">Appendix D- BAM to FASTQ processing (specific to the ADNI
project)</a></p>
<p><a href="#appendix-e--sra-to-fastq">Appendix E- SRA to FASTQ</a></p>
<p><a href="#appendix-f--joint-calling-pipeline">Appendix F- Joint calling pipeline</a></p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>Here, I summarize all the details involved in the
processing and QC (Figure 1) of whole exome and whole genome sequence data for
my Alzheimer projects. This report also provides brief summary of pipelines required to process
sequencing data. Our latest datafreeze, Bloomfield, was completed in October
2021.</p>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147440389-c8e2c764-95a0-490b-bc47-561882bf54e6.png" alt="" />
<p class="caption">Figure 1</p>
</div>
<p>Figure 1. Summary of next generation sequence (NGS) data processing and quality
control (QC) pipeline.</p>
</div>
<div id="variant-calling" class="section level1">
<h1>Variant calling</h1>
<div id="from-raw-data-to-gvcf" class="section level2">
<h2>1 – From raw data to gVCF</h2>
<div id="main-pipeline" class="section level3">
<h3>Main pipeline</h3>
<p>202110_bloomfield datafreeze consist of 9810 samples either in raw fastq, bam,
or cram formats. To process these samples, several pipelines and Docker images
were created implementing Bedtools (version 2.27.1), Samtools (version 1.9),
Picard (version 2.20.8) and GATK (version 4.1.2.0). In summary, we have 48
projects (Table 1) that were processed using the same pipelines (Pipeline A, see
Figure 1) in our computing platform at McDonnell genome institute (MGI) at WashU.</p>
<p>All reference files required for variant calling are available on the local
directory at the MGI server</p>
<p>/gscmnt/gc2645/wgs/Genome_Ref/GRCh38. Reference genome GRCh38 (GRCh38.p13; the
thirteenth patch of the release) is available at MGI on
/gscmnt/gc2645/wgs/Genome_Ref/GRCh38.</p>
<p>Files for dbSNP and high confidence SNPs and Indels calibration callsets from
the 1K genome and Mills (Mills, Devine, Genome Research, 2011) public resources
are also available at MGI on
/gscmnt/gc2645/wgs/Genome_Ref/GRCh38/20190522_bundle.</p>
<p>Our pipeline was built to efficiently download/transfer raw files from external
servers, immediately process them, and delete the raw and unnecessary files
after successful completion of each variant calling step in the pipeline (See
Appendix A)</p>
<p>In summary, here are the series of steps (in order) we followed to process these
datasets; this pipeline assumes the starting file are fastq; for projects with
different starting raw data, refer to “Alternative pipelines” section:</p>
<ol style="list-style-type: decimal">
<li><p>We started with creating a metafile (fields separated by commas) with the
following contents for each sequencing project:</p>
<ol style="list-style-type: decimal">
<li><p>sample name (always possible to put final name, if different one
provided try to change it)</p></li>
<li><p>DNA barcode (if available and provided by the lab or collaborator,
alternatively we generated virtual barcodes)</p></li>
<li><p>sequencing project name (as specified in Table 1)</p></li>
<li><p>FULLSM (FULLSM was created as
“${Sample_name}^${DNA_barcode}^${Project_name}”)</p></li>
<li><p>READ group, (which refers to a set of reads that are generated from a
single run of a sequencing instrument was used to create a RGBASE as
“${FULLSM}.${READ_group}”)</p></li>
<li><p>file name</p></li>
<li><p>file name extension ( identifier specified as a suffix to the name of
either fastq, bam, cram – as inTable 1)</p></li>
</ol></li>
<li><p>We performed BWA alignment of reads based on human genome build GRCh38
(GRCh38.p13; the thirteenth patch release for the GRCh38 reference assembly)
using our docker image achalneupane/bwaref. Reference genome was downloaded
from:
<a href="https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0" class="uri">https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0</a></p></li>
<li><p>After performing the alignment of reads, we validated the bam output for any
improper formatting, faulty alignments, incorrect flag values, etc., using
Picard. We used our docker image achalneupane/validatesamref for this
purpose. Any subsequent bam files produced by our pipelines were also
“validated” using this tool.</p></li>
<li><p>We are also starting to include a docker image for VerifyBamID
(achalneupane/verifybamid) that will calculate freemix values. These
represent relative sample contamination.</p></li>
<li><p>We restricted our analysis to include only the exonic variants using
Bedtools version 2.27.1 implemented in our docker image
achalneupane/intersectbedref.</p></li>
<li><p>After performing the alignment for each RGBASE, we locate and tag the
duplicate reads using Markduplicates tool in GATK version 4.1.2. Duplicates
can originate from a single fragment of DNA either during library
construction or also as optical duplicates. This tool differentiates the
primary and duplicate reads using an algorithm that ranks reads by the sums
of their base-quality scores. We then merge multiple bam files arising from
each RGBASE into one single bam file.</p></li>
<li><p>We calculate depth of coverage using depthofcoverage tool in GATK to assess
sequence coverage by a wide array of metrics, partitioned by sample, read
group, or library. The docker image is available on dockerhub as
achalneupane/depthofcoverage.</p></li>
<li><p>We generate a recalibration table for Base Quality Score Recalibration
(BQSR) using BaseRecalibrator tool in GATK which was implemented in our
docker image achalneupane/baserecalv2. Here, we performed a by-locus
traversal operating only at sites that are in the known sites VCF. We used
dbSNP and high confidence SNPs and Indels calibration callsets from the 1K
genome and Mills (Mills, Devine, Genome Research, 2011) public resources as
known sites of variation.</p></li>
<li><p>We used haplotype caller in GATK to call for germline SNPs and indels via
local re-assembly of haplotypes implemented in our docker image
achalneupane/haplocallerv2.</p></li>
<li><p>Finally, variant evaluation and refinement was performed using “VariantEval”
tool in GATK calculating various quality control metrics. These metrics
include the number of raw or filtered SNP counts; ratio of transition
mutations to transversions; concordance of a particular sample’s calls to a
genotyping chip; number of s per sample, etc. This tool was implemented in
our docker image achalneupane/variantevalref</p></li>
</ol>
<p>In addition, we consistently used the same version of tools to process these
datasets throughout our pipelines.</p>
<p>Any processed output files were immediately transferred to our local storage on
Fenix. These files include, GATK report, coverage statistics report,
recalibration tables, and variant calling (gVCF), and their index files. The
aligned bam files after marking duplicates are also saved to Box to be used in
future collaborations/in case of “easier” recalling needed.</p>
</div>
<div id="alternative-pipelines" class="section level3">
<h3>Alternative pipelines</h3>
<ol style="list-style-type: decimal">
<li><p>For any projects that have raw reads in bam or cram format, we first
unwrapped those samples into paired fastq files per read groups using
RevertSam and SamToFastq tools in Picard version 2.20.8. This tool has been
implemented in our docker image achalneupane/cram2fastq (see, appendix C).</p></li>
<li><p>Although the aforementioned pipeline was used to process BAM/CRAM samples,
the pipeline to process BAM files from ADNI_WGS project was slightly
different. The BAM files from this project did not have the RGIDs defined in
the BAM header, so a new Docker image (achalneupane/bam2fqv2) was created
that pulls the RGID from the read header itself and unwraps the BAM per RGID
(see, appendix D). These files were relatively larger than the files from
other projects. To speed up the process, we first unwrapped them on Compute
1 cluster and aligned the processed FASTQ (interleaved reads) per RGID on
MGI.</p></li>
<li><p>We also downloaded samples for some projects from the dbGaP database in the
SRA format. The interleaved sequence reads were extracted from the SRA files
in FASTQ format that were processed using the pipeline E (see, Figure 1;
Appendix E).</p></li>
</ol>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147440990-03874293-f756-4987-a275-c45c80b7f8e9.png" alt="" />
<p class="caption">Figure 2</p>
</div>
<p>Figure 2 Summary of pipelines according to starting raw file format, and
integration into the main pipeline. These pipeline are shown in Appendices A-E.</p>
<p>Table 1. Summary of sequencing projects included in the current datafreeze –
Bloomfield. Details include nature of data (WXS being WES or WGS), nature of
source data (fastq, bam or cram), number of samples per sequencing project and
pileine (A to E) that samples for each particular project should follow.</p>
<table>
<colgroup>
<col width="34%" />
<col width="4%" />
<col width="45%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th>Seq project</th>
<th>WXS</th>
<th>source /raw data</th>
<th># gVCFs</th>
<th>Pipeline</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DIAN</td>
<td>WGS</td>
<td>r1.fq.gz</td>
<td>12</td>
<td>A</td>
</tr>
<tr class="even">
<td>Macrogen_WGS</td>
<td>WGS</td>
<td>r1.fq.gz</td>
<td>20</td>
<td>A</td>
</tr>
<tr class="odd">
<td>MAPT_A152T</td>
<td>WGS</td>
<td>r1.fq.gz</td>
<td>21</td>
<td>A</td>
</tr>
<tr class="even">
<td>LOAD_WES</td>
<td>WES</td>
<td>qual2sanger.r1.fq.gz (99%) / r1.fq.gz (1%)</td>
<td>33</td>
<td>A</td>
</tr>
<tr class="odd">
<td>Genentech_WES</td>
<td>WES</td>
<td>qual2sanger.r1.fq.gz / r1.fq.gz / qualsanger.r1.gz (1)</td>
<td>91</td>
<td>A</td>
</tr>
<tr class="even">
<td>Genentech_WGS</td>
<td>WGS</td>
<td>qual2sanger.r1.fq.gz</td>
<td>47</td>
<td>A</td>
</tr>
<tr class="odd">
<td>201812_MGI_DIANWGS_REDCLOUD</td>
<td>WGS</td>
<td>cram</td>
<td>546</td>
<td>C</td>
</tr>
<tr class="even">
<td>201909_MGI_gDNA_LINDSEY</td>
<td>WGS</td>
<td>cram</td>
<td>637</td>
<td>C</td>
</tr>
<tr class="odd">
<td>202007_MGI_UPittKambohPiB_WGS_ELLINGWOOD</td>
<td>WGS</td>
<td>cram</td>
<td>700</td>
<td>C</td>
</tr>
<tr class="even">
<td>phs000572_201508 (ADSP)</td>
<td>WES</td>
<td>sra</td>
<td>117</td>
<td>E</td>
</tr>
<tr class="odd">
<td>phs000572_201612 (ADSP)</td>
<td>WGS</td>
<td>sra</td>
<td>190</td>
<td>E</td>
</tr>
<tr class="even">
<td>Broad_WGS</td>
<td>WGS</td>
<td>bam</td>
<td>174</td>
<td>C</td>
</tr>
<tr class="odd">
<td>TGI_WES</td>
<td>WES</td>
<td>bam</td>
<td>298</td>
<td>C</td>
</tr>
<tr class="even">
<td>MGI_FASeEOAD_201605</td>
<td>WES</td>
<td>bam</td>
<td>423</td>
<td>C</td>
</tr>
<tr class="odd">
<td>MGI_DIAN_201610</td>
<td>WGS</td>
<td>bam</td>
<td>4</td>
<td>C</td>
</tr>
<tr class="even">
<td>MGI_Imaging_201612</td>
<td>WES</td>
<td>bam</td>
<td>499</td>
<td>C</td>
</tr>
<tr class="odd">
<td>Otogenetics_WES</td>
<td>WES</td>
<td>fq1/fq2</td>
<td>834</td>
<td>A</td>
</tr>
<tr class="even">
<td>phs000376 (familalPD)</td>
<td>WES</td>
<td>fq</td>
<td>44</td>
<td>B</td>
</tr>
<tr class="odd">
<td>phs000908 (Rare Variant PD)</td>
<td>WES</td>
<td>fq1/fq2</td>
<td>200</td>
<td>A</td>
</tr>
<tr class="even">
<td>phs000572_201707 (ADSP)</td>
<td>WES</td>
<td>sra</td>
<td>94</td>
<td>E</td>
</tr>
<tr class="odd">
<td>PPMI_WES</td>
<td>WES</td>
<td>fq</td>
<td>591</td>
<td>A</td>
</tr>
<tr class="even">
<td>ADNI_WGS</td>
<td>WGS</td>
<td>bam</td>
<td>809</td>
<td>D</td>
</tr>
<tr class="odd">
<td>MGI_Gregg_201704</td>
<td>WES</td>
<td>bam</td>
<td>83</td>
<td>C</td>
</tr>
<tr class="even">
<td>phs000901(PD w/ CSF biomarker)</td>
<td>WES</td>
<td>fq</td>
<td>57</td>
<td>B</td>
</tr>
<tr class="odd">
<td>MGI_DIANEXR_201706</td>
<td>WGS</td>
<td>bam</td>
<td>19</td>
<td>C</td>
</tr>
<tr class="even">
<td>MGI_DIANEXR_201805</td>
<td>WGS</td>
<td>bam</td>
<td>3</td>
<td>C</td>
</tr>
<tr class="odd">
<td>CACHE_WGS (Keoni)</td>
<td>WGS</td>
<td>fq</td>
<td>215</td>
<td>B</td>
</tr>
<tr class="even">
<td>phs000572_201802(ADSP)</td>
<td>WES</td>
<td>sra</td>
<td>1540</td>
<td>E</td>
</tr>
<tr class="odd">
<td>MGI_DIANEXR_201902</td>
<td>WGS</td>
<td>r1.fq.gz</td>
<td>5</td>
<td>A</td>
</tr>
<tr class="even">
<td>Mayo_EOAD_201810</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>227</td>
<td>A</td>
</tr>
<tr class="odd">
<td>Mayo_Biobank-Control_201810 (PROCESS)</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>197</td>
<td>A</td>
</tr>
<tr class="even">
<td>Mayo_FTLD-TDP_201901</td>
<td>WGS</td>
<td>fq</td>
<td>11</td>
<td>B</td>
</tr>
<tr class="odd">
<td>MGI_DIANEXR_201906</td>
<td>WGS</td>
<td>r1.fq.gz</td>
<td>2</td>
<td>A</td>
</tr>
<tr class="even">
<td>201904_MGI_IDTexome_HURON</td>
<td>WES</td>
<td>cram</td>
<td>34</td>
<td>C</td>
</tr>
<tr class="odd">
<td>201907_USUHS_gDNA_SHERMAN</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>45</td>
<td>C</td>
</tr>
<tr class="even">
<td>202002_Mendelics_DIANEXR_WES_UNNAMED</td>
<td>WES</td>
<td>bam</td>
<td>2</td>
<td>C</td>
</tr>
<tr class="odd">
<td>202004_AGRF_EOAD_WGS-WES_UNNAMED</td>
<td>WGS</td>
<td>2 bam</td>
<td>2</td>
<td>C</td>
</tr>
<tr class="even">
<td>202103_ADSP_FUS-familial_WGS_UNNAMED</td>
<td>WGS</td>
<td>cram</td>
<td>180</td>
<td>C</td>
</tr>
<tr class="odd">
<td>202004_USUHS_EOAD-WGS_gDNA_EOLUS</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>613</td>
<td>A</td>
</tr>
<tr class="even">
<td>202104_ADSP_site27-sync-n303_WGS_UNNAMED</td>
<td>WGS</td>
<td>cram</td>
<td>183</td>
<td>C</td>
</tr>
<tr class="odd">
<td>202008_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="even">
<td>202009_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="odd">
<td>202011_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="even">
<td>202103_MGI_DIAN_WGS_UNNAMED</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="odd">
<td>202104_MGI_DIAN_WGS_UNNAMED</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="even">
<td>202104_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>WES</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="odd">
<td>202106_MGI_DIAN_WES_UNNAMED</td>
<td>WES</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="even">
<td>202106_MGI_DIAN_WGS_UNNAMED</td>
<td>WGS</td>
<td>fq1/fq2</td>
<td>1</td>
<td>A</td>
</tr>
<tr class="odd">
<td>TOTAL</td>
<td></td>
<td></td>
<td>9810</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="joint-calling" class="section level2">
<h2>2 – Joint Calling</h2>
<p>Joint calling of 9,810 samples was performed at the MGI. The Joint Calling is a
two-step process.</p>
<p>First, we used GenomicsDBImport to combine GVCFs before performing joint
genotyping. We ran this step per chromosome.<br />
GenomicsDBImport is used to import single-sample g.VCF and merge them into
GenomicsDB before joint genotyping. This step needs an input file listing all
the sample names and corresponding gVCF file names to include in a joint VCF.
The gVCF file names cannot have “^” and symlinks without “^” also did not work
with GenomicsDBImport, so temporary files were created replacing “^” with a
“.”.<br />
We then performed joint genotyping of gVCFs on each chromosome using
GenotypeGVCFs tool in GATK which was implemented in docker image
broadinstitute/gatk:4.1.2.0.</p>
<p>See Appendix F for a full description of the pipeline.</p>
</div>
</div>
<div id="quality-control-qc" class="section level1">
<h1>Quality Control (QC)</h1>
<p>The quality control (QC)</p>
<p>Here is a flowchart of the general QC done prior to Plink and the folder for
performing all these steps is here:</p>
<pre><code>${DIR}/01-Bloomfield-preQC/</code></pre>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147441761-50221321-3310-4736-b588-8dbde783c743.png" alt="" />
<p class="caption">Figure 3</p>
</div>
<p>Figure 3. Shcematic view of the quality control steps performed through GATK.</p>
<p>Details of each of these steps are given next</p>
<div id="vqsr" class="section level2">
<h2>1 – VQSR</h2>
<p>The data is split into WES and WGS data. The variants are also split in each
dataset as SNPs or INDELs as there are differences and requirements for
filtering. Bcftools annotate is used to split VCF files from multi-allelic to
bi-allelic, which is why variant numbers appear to increase at this step.
Variant Quality Score Recalibration (VQSR) is done by ApplyVQSR in the GATK
software. The algorithm in VQSR uses Gaussian mixture model that classifies
variants based on how their annotation values cluster given a training set of
high-confidence variants. The VQSR tool then uses this model to assign a new
confidence score to each variant, called VQSLOD. This is a log-ratio of the
variant’s probabilities belonging to the positive and negative model. For the
Variant Recalibrator, we have had problems with some chromosomes failing due to
not enough detected variants. To fix this, we had to decrease the max-gaussian
option to the following values:</p>
<p>WGS SNVs: max-gaussian 6 for all except chrY which has a value of 1.</p>
<p>WGS INDELs: max-gaussian 2 for all except chr16 and chrY which has a value of 1.</p>
<p>WES SNVs: max-gaussian 6 for all chromosomes except 2 for chr13 and 1 for chrY.</p>
<p>WES INDELs: max-gaussian 2 for all chromosomes except for chr12, chr16, chr21,
and chrY which have a value of 1.</p>
<p>All the analysis and files for the VQSR step is here:</p>
<pre><code>${DIR}/01-Bloomfield-preQC/01-VQSR-ExAC-tsSNP99.6-tsINDEL95</code></pre>
<p>The scripts for VQSR, 02a-VQSR-WGS-SNVs.sh and 02b-VQSR-WGS-INDELs.sh can be
found here:</p>
<pre><code>${DIR}/01-Bloomfield-preQC/</code></pre>
<p>After VQSR, we remove variants in low-complexity regions of the DNA.</p>
</div>
<div id="hard-filtering" class="section level2">
<h2>2 –Hard Filtering</h2>
<p>Before hard filtering, we combine WES and WGS data for both the SNPs and INDELs.
Multi-allelic variants were separated into bi-allelic variants, and annotation
and normalization was performed using
<a href="https://samtools.github.io/bcftools/bcftools.html">Bcftools</a>. Hard filtering
includes getting rid of bad quality variants by removing non-variants and
monomorphic variants. For the indels, we have to also manually filter by the
qualbydepth score or the QD score. This is the variant confidence normalized by
the unfiltered depth of variant samples. The plots below show the INDELs before
and after removing all variants lower than a QD score of 10. Generally, we try
to remove all of the smaller peaks on the lower left side of the original QD
plot, so we get a distribution as “normal” as possible.</p>
<table>
<colgroup>
<col width="35%" />
<col width="64%" />
</colgroup>
<thead>
<tr class="header">
<th>QD plot – Number of variants = 243,382</th>
<th>QD plot &gt;= 10 – Number of Variants = 191,174</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="https://user-images.githubusercontent.com/10935581/147441891-e1895ecd-efad-4caf-9b3f-fab75479e918.jpg" alt="Figure 4" /></td>
<td><img src="https://user-images.githubusercontent.com/10935581/147441928-208223a8-9b02-43af-bf95-c417e97ae35f.jpg" alt="Figure 5" /></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Hard filtering scripts for hard filtering including 03a_HQC_WXS_SNVs.sh and
03b-HQC_WXS_INDELs.sh are here:</p>
<pre><code>/${DIR}/01-Bloomfield-preQC/</code></pre>
</div>
<div id="plink-qc" class="section level2">
<h2>3 –PLINK QC</h2>
<p>This is the general overview of the QC performed for PLINK. Next, there is a
detail specification of each step and criteria for threshold when needed.</p>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147441999-fed263ee-e144-4d74-8399-22dea5c114af.png" alt="" />
<p class="caption">Figure 6</p>
</div>
<p>The QC on plink was done using these serial steps that will remove variants and
samples that do not meet quality criteria:</p>
<ol style="list-style-type: decimal">
<li><p>Hwe filter of 10e-30 for INDELs and 10e-8 for SNPs</p></li>
<li><p>Geno filter of 0.05</p></li>
<li><p>Merge together SNPs and INDELs</p></li>
<li><p>Apply missingness per individual</p></li>
<li><p>Apply differential missingness – done for WXS and WGS, and for 15 different
projects</p></li>
<li><p>Heterozigosity calculations</p></li>
<li><p>Sex-check</p></li>
<li><p>PCA</p></li>
<li><p>IBD</p></li>
</ol>
<p>All files and logs for this can be found here:</p>
<pre><code>/${DIR}/03-PLINK-QC-files</code></pre>
<div id="hardy-weinberg-equilibrium" class="section level3">
<h3>Hardy Weinberg Equilibrium</h3>
<p>All variants were tested for Hardy-Weinberg equilibrium. This is common practice
in genome wide association studies to detect variants that suffer from
significant genotyping error.</p>
<pre><code>BFILE_INDELS=&quot;Bloomfield_9810_INDELS&quot;

plink1.9 --bfile \${BFILE_INDELS} --nonfounders --hwe 10e-30 include-nonctrl
\--keep-allele-order --autosome --make-bed --out \${BFILE_INDELS}-autosomes

BFILE_SNPS=&quot;Bloomfield_9810_SNPS&quot;

plink1.9 --bfile \${BFILE_SNPS} --nonfounders --hwe 10e-8 include-nonctrl
\--keep-allele-order --autosome --make-bed --out \${BFILE_SNPS}-autosomes

plink1.9 --bfile \${BFILE_INDELS} --chr X,Y,XY --make-bed --keep-allele-order
\--out \${BFILE_INDELS}-X-Y-XY

plink1.9 --bfile \${BFILE_SNPS} --chr X,Y,XY --make-bed --keep-allele-order
\--out \${BFILE_SNPS}-X-Y-XY

\# Merge autosomes and chr X, Y, XY

plink1.9 --bfile \${BFILE_INDELS}-autosomes --bmerge \${BFILE_INDELS}-X-Y-XY.bed
\${BFILE_INDELS}-X-Y-XY.bim \${BFILE_INDELS}-X-Y-XY.fam --keep-allele-order
\--merge-mode 4 --indiv-sort 0 --out \${BFILE_INDELS}-hwe

plink1.9 --bfile \${BFILE_SNPS}-autosomes --bmerge \${BFILE_SNPS}-X-Y-XY.bed
\${BFILE_SNPS}-X-Y-XY.bim \${BFILE_SNPS}-X-Y-XY.fam --keep-allele-order
\--merge-mode 4 --indiv-sort 0 --out \${BFILE_SNPS}-hwe</code></pre>
<p>Percent of variants removed from applying the HWE flag at various p-values
(Table 2, Table 3); based on these values, and what is currently used in the
literature by similar studies, we decided to</p>
<p>Table 2. number of variants removed in the SNPs and INDELs datasets according to
differnet p-value thresholds.</p>
<table>
<thead>
<tr class="header">
<th>P.Value</th>
<th>SNPs</th>
<th>INDELs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.05</td>
<td>9.099%</td>
<td>16.254%</td>
</tr>
<tr class="even">
<td>10e-6</td>
<td>2.676%</td>
<td>6.738%</td>
</tr>
<tr class="odd">
<td>10e-8</td>
<td>1.991%</td>
<td>5.425%</td>
</tr>
<tr class="even">
<td>10e-30</td>
<td>0.407%</td>
<td>1.7605</td>
</tr>
<tr class="odd">
<td>Bonferroni threshold</td>
<td>1.759% (1.37e-8)</td>
<td>5.664% (2.67e-7)</td>
</tr>
</tbody>
</table>
<p>Table 3. DeFinetti plots for HWE values for SNPs and INDELS, pre and post
filtering</p>
<table>
<thead>
<tr class="header">
<th>SNPs</th>
<th>Pre-HWE Pre hwe filter – 10e-6</th>
<th>Post – HWE Filtered using 10e-8</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><img src="https://user-images.githubusercontent.com/10935581/147442038-d7f67a55-ff16-4272-a707-b5860484e064.jpg" alt="Figure 7" /></td>
<td><img src="https://user-images.githubusercontent.com/10935581/147442063-4d324cb3-c865-42fe-b89e-c1140baf585d.jpg" alt="Figure 8" /></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th>INDELs</th>
<th>Pre-HWE mark for 10e-6</th>
<th>Post – HWE Filtered using 10e-30</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><img src="https://user-images.githubusercontent.com/10935581/147442090-7b9d590c-3356-42e5-8fb7-57e23b623445.jpg" alt="Figure 9" /></td>
<td><img src="https://user-images.githubusercontent.com/10935581/147442110-f22a82e8-5d11-485c-bcc3-24dde3ea117c.jpg" alt="Figure 10" /></td>
</tr>
</tbody>
</table>
</div>
<div id="genotyping-rate" class="section level3">
<h3>Genotyping Rate</h3>
<p>The genotyping rate or –geno filter gets rid of all variants with missing call
rates over 0.05, or whatever value you prefer. In our dataset we use a value of
0.05 for SNPs and INDELs.</p>
<pre><code>GENO1=&quot;0.05&quot;

plink1.9 --bfile \${BFILE_INDELS}-hwe --geno \${GENO1} --make-bed
\--keep-allele-order --allow-no-sex --out \${BFILE_INDELS}-hwe-geno\${GENO1}

plink1.9 --bfile \${BFILE_SNPS}-hwe --geno \${GENO1} --make-bed
\--keep-allele-order --allow-no-sex --out \${BFILE_SNPS}-hwe-geno\${GENO1}</code></pre>
<p>This filter removed 472,406 SNP variants and 46,358 INDEL variants. The
resulting genotyping rate is 0.9080 for INDELs and 0.9885 for SNPs.</p>
</div>
<div id="missingness-per-individual" class="section level3">
<h3>Missingness per individual</h3>
<p>The –mind filter gets rid of samples in your dataset with a missing call rate
above a certain value. This is similar to the –geno filter but instead of
removing variants we are removing samples from further analysis.</p>
<pre><code>plink1.9 --bfile \${BFILE}-hwe-geno\${GENO1} --mind 0.1 --make-bed
\--keep-allele-order --out \${BFILE}-hwe-geno\${GENO1}-mind0.1</code></pre>
<p>There were 116 samples removed by the –mind filter. Table 4 The samples below
show the number of individuals removed and from which project they belong.
Samples from phs000901 were known to have extremely low coverage with 0.005%
bases above 10x coverage.</p>
<p>Table 4. Relationship of samples per project that failed missingness 0.1 filter.</p>
<table>
<thead>
<tr class="header">
<th>Seq Project</th>
<th># samples with missingness &gt; 0.1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>phs000901</td>
<td>57</td>
</tr>
<tr class="even">
<td>CACHE_WGS</td>
<td>4</td>
</tr>
<tr class="odd">
<td>PPMI_WES</td>
<td>33</td>
</tr>
<tr class="even">
<td>MGI_Gregg_201704</td>
<td>2</td>
</tr>
<tr class="odd">
<td>Otogenetics_WES</td>
<td>2</td>
</tr>
<tr class="even">
<td>MGI_Imaging_201612</td>
<td>3</td>
</tr>
<tr class="odd">
<td>phs000908</td>
<td>12</td>
</tr>
<tr class="even">
<td>201909_MGI_gDNA_LINDSEY</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<div id="missingness-per-wxs" class="section level3">
<h3>Missingness per WXS</h3>
<p>This dataset contains a mixture of WES and WGS data Table 5; those two types of
sequence data are generated through differnet procudures which can give place to
variants that exclusively found in the WES or in the WGS dataset. To avoide this
batch effect, we perform differential missingness using WES or WGS as
“phenotype” with the plink command –test-missing. The METADATA file contains a
WXS column where 1 represents WGS sequencing type and 2 represents WES.</p>
<pre><code>BFILE=&quot;Bloomfield_9810-hwe-geno0.05-mind0.1&quot;

METADATA=&quot;${DIR}/01-Bloomfield-preQC/Bloomfield_9810_metadata.txt&quot;

plink1.9 --bfile ../02-intermediate-filtering-files/\${BFILE} --pheno
\${METADATA} --pheno-name WXS --test-missing --allow-no-sex --out
\${BFILE}-missing-WXS</code></pre>
<p>Variants with differential missingness between WES and WGS data sets, and a
Hardy-Weinberg equilibrium of p &lt; 1e-8, were removed from analysis.</p>
<pre><code>FILE2=&quot;\${BFILE}-missing-WXS.missing&quot;

plink1.9 --bfile ../02-intermediate-filtering-files/\${BFILE} --exclude
\${FILE2%.\*}.list --keep-allele-order --make-bed --out \${BFILE}-WXSm   
A total of 1,033,110 variants were removed and the final genotyping rate is
0.99939.</code></pre>
<p>Table 5. Proportion of WES and WGS in the dataset</p>
<table>
<thead>
<tr class="header">
<th>WXS</th>
<th>#</th>
<th>%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>WES</td>
<td>4940</td>
<td>50.4%</td>
</tr>
<tr class="even">
<td>WGS</td>
<td>4870</td>
<td>49.6%</td>
</tr>
<tr class="odd">
<td>TOTAL</td>
<td>9810</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="missingness-per-sequencing-project" class="section level3">
<h3>Missingness per Sequencing Project</h3>
<p>Similar to the previous step, this dataset contains samples coming from multiple
sequencing rounds and centers which can also be a source of batch effects; to
minimize batch effect, we also perform differential missingness using sequencing
project as the “phenotype” with the plink command –test-missing. We perform the
differential missingness step for each project that contributes with more than
2% of total samples.</p>
<p>Table 6. Relationship fo sequencing projects, number of smaplesper project and %
of samples contributing to the entire datafreeze.</p>
<table>
<colgroup>
<col width="34%" />
<col width="11%" />
<col width="11%" />
<col width="42%" />
</colgroup>
<thead>
<tr class="header">
<th>Sequencing project</th>
<th>N of samples</th>
<th>% of samples</th>
<th># variants identified by differential missigness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>201812_MGI_DIANWGS_REDCLOUD</td>
<td>546</td>
<td>5.56575</td>
<td></td>
</tr>
<tr class="even">
<td>201904_MGI_IDTexome_HURON</td>
<td>34</td>
<td>0.346585</td>
<td></td>
</tr>
<tr class="odd">
<td>201907_USUHS_gDNA_SHERMAN</td>
<td>45</td>
<td>0.458716</td>
<td></td>
</tr>
<tr class="even">
<td>201909_MGI_gDNA_LINDSEY</td>
<td>637</td>
<td>6.49337</td>
<td></td>
</tr>
<tr class="odd">
<td>202002_Mendelics_DIANEXR_WES_UNNAMED</td>
<td>2</td>
<td>0.0203874</td>
<td></td>
</tr>
<tr class="even">
<td>202004_AGRF_EOAD_WGS-WES_UNNAMED</td>
<td>2</td>
<td>0.0203874</td>
<td></td>
</tr>
<tr class="odd">
<td>202004_USUHS_EOAD-WGS_gDNA_EOLUS</td>
<td>613</td>
<td>6.24873</td>
<td></td>
</tr>
<tr class="even">
<td>202007_MGI_UPittKambohPiB_WGS_ELLINGWOOD</td>
<td>700</td>
<td>7.13558</td>
<td></td>
</tr>
<tr class="odd">
<td>202008_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="even">
<td>202009_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="odd">
<td>202011_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="even">
<td>202103_ADSP_FUS-familial_WGS_UNNAMED</td>
<td>180</td>
<td>1.83486</td>
<td></td>
</tr>
<tr class="odd">
<td>202103_MGI_DIAN_WGS_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="even">
<td>202104_ADSP_site27-sync-n303_WGS_UNNAMED</td>
<td>183</td>
<td>1.86544</td>
<td></td>
</tr>
<tr class="odd">
<td>202104_MGI_DIAN_WGS_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="even">
<td>202104_MGI_DIAN_WGS-WES_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="odd">
<td>202106_MGI_DIAN_WES_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="even">
<td>202106_MGI_DIAN_WGS_UNNAMED</td>
<td>1</td>
<td>0.0101937</td>
<td></td>
</tr>
<tr class="odd">
<td>ADNI_WGS</td>
<td>809</td>
<td>8.24669</td>
<td></td>
</tr>
<tr class="even">
<td>Broad_WGS</td>
<td>174</td>
<td>1.7737</td>
<td></td>
</tr>
<tr class="odd">
<td>CACHE_WGS</td>
<td>215</td>
<td>2.19164</td>
<td></td>
</tr>
<tr class="even">
<td>DIAN</td>
<td>12</td>
<td>0.122324</td>
<td></td>
</tr>
<tr class="odd">
<td>Genentech_WES</td>
<td>91</td>
<td>0.927625</td>
<td></td>
</tr>
<tr class="even">
<td>Genentech_WGS</td>
<td>47</td>
<td>0.479103</td>
<td></td>
</tr>
<tr class="odd">
<td>LOAD_WES</td>
<td>33</td>
<td>0.336391</td>
<td></td>
</tr>
<tr class="even">
<td>Macrogen_WGS</td>
<td>20</td>
<td>0.203874</td>
<td></td>
</tr>
<tr class="odd">
<td>MAPT_A152T</td>
<td>21</td>
<td>0.214067</td>
<td></td>
</tr>
<tr class="even">
<td>Mayo_Biobank-Control_201810</td>
<td>197</td>
<td>2.00815</td>
<td></td>
</tr>
<tr class="odd">
<td>Mayo_EOAD_201810</td>
<td>227</td>
<td>2.31397</td>
<td></td>
</tr>
<tr class="even">
<td>Mayo_FTLD-TDP_201901</td>
<td>11</td>
<td>0.11213</td>
<td></td>
</tr>
<tr class="odd">
<td>MGI_DIAN_201610</td>
<td>4</td>
<td>0.0407747</td>
<td></td>
</tr>
<tr class="even">
<td>MGI_DIANEXR_201706</td>
<td>19</td>
<td>0.19368</td>
<td></td>
</tr>
<tr class="odd">
<td>MGI_DIANEXR_201805</td>
<td>3</td>
<td>0.030581</td>
<td></td>
</tr>
<tr class="even">
<td>MGI_DIANEXR_201902</td>
<td>5</td>
<td>0.0509684</td>
<td></td>
</tr>
<tr class="odd">
<td>MGI_DIANEXR_201906</td>
<td>2</td>
<td>0.0203874</td>
<td></td>
</tr>
<tr class="even">
<td>MGI_FASeEOAD_201605</td>
<td>423</td>
<td>4.31193</td>
<td></td>
</tr>
<tr class="odd">
<td>MGI_Gregg_201704</td>
<td>83</td>
<td>0.846075</td>
<td></td>
</tr>
<tr class="even">
<td>MGI_Imaging_201612</td>
<td>499</td>
<td>5.08665</td>
<td></td>
</tr>
<tr class="odd">
<td>Otogenetics_WES</td>
<td>834</td>
<td>8.50153</td>
<td></td>
</tr>
<tr class="even">
<td>phs000376</td>
<td>44</td>
<td>0.448522</td>
<td></td>
</tr>
<tr class="odd">
<td>phs000572_201508</td>
<td>117</td>
<td>1.19266</td>
<td></td>
</tr>
<tr class="even">
<td>phs000572_201612</td>
<td>190</td>
<td>1.9368</td>
<td></td>
</tr>
<tr class="odd">
<td>phs000572_201707</td>
<td>94</td>
<td>0.958206</td>
<td></td>
</tr>
<tr class="even">
<td>phs000572_201802</td>
<td>1540</td>
<td>15.6983</td>
<td></td>
</tr>
<tr class="odd">
<td>phs000901</td>
<td>57</td>
<td>0.58104</td>
<td></td>
</tr>
<tr class="even">
<td>phs000908</td>
<td>200</td>
<td>2.03874</td>
<td></td>
</tr>
<tr class="odd">
<td>PPMI_WES</td>
<td>591</td>
<td>6.02446</td>
<td></td>
</tr>
<tr class="even">
<td>TGI_WES</td>
<td>298</td>
<td>3.03772</td>
<td></td>
</tr>
</tbody>
</table>
<p>Based on the table above, the sequencing projects that contribute with more than
2% of total samples are:</p>
<p>phs000908, Mayo_Biobank-Control_201810, CACHE_WGS, Mayo_EOAD_201810, TGI_WES,
MGI_FASeEOAD_201605, MGI_Imaging_201612, 201812_MGI_DIANWGS_REDCLOUD, PPMI_WES,
202004_USUHS_EOAD-WGS_gDNA_EOLUS, 201909_MGI_gDNA_LINDSEY,
202007_MGI_UPittKambohPiB_WGS_ELLINGWOOD, ADNI_WGS, Otogenetics_WES,
phs000572_201802</p>
<p>To perform differential missingness we add as many dummy variables as sequencing
projects with the values “1” or “2” representing whether each sample has been
sequenced (1) or not (2) under that particular sequencing project. Next,
--test-missing is run as many times as sequencing projects identified.</p>
<pre><code>plink1.9 --bfile \${BFILE} --pheno \${PHENOSCOPE} --pheno-name
Mayo_Biobank-Control_201810 --test-missing --allow-no-sex --out
\${BFILE}-scope_missing15</code></pre>
<p>Then, all the output lists are collapsed into a single one and variants with a
p-value p&lt;1e-8 are removed from the dataset.</p>
<pre><code>FILE=&quot;\${BFILE}-scope_missing15.missing&quot;

sed -i &#39;s/ \\+/\\t/g&#39; \${FILE}

sed -i &#39;s/\^\\t//g&#39; \${FILE}

awk &#39;{if (\$5\&lt;=1.00e-08) print \$0}&#39; \${FILE} \| cut -f2 \&gt;\&gt;
\${BFILE}.scope-missing.list

\#\# Put &quot;BAD&quot; variants in a file and remove from dataset - Part to do manually

awk &#39;{if (\$5\&lt;=1.00e-08) print \$0}&#39; \${FILE2} \| cut -f2 \&gt; \${FILE2%.\*}.list

\#\# NOW extract these BAD variants

plink1.9 --bfile \${BFILE} --exclude \${FILE2%.\*}.list --keep-allele-order
\--make-bed --out \${BFILE}-WXSm</code></pre>
<p>There were 278,143 unique variants to remove from missingness per sequencing
project.</p>
</div>
<div id="heteorzigosity" class="section level3">
<h3>Heteorzigosity</h3>
<p>All commands and files were run here:</p>
<p>${DIR}/01-Bloomfield-preQC/03-PLINK-QC-files</p>
<p>#Fenix</p>
<pre><code>BFILE=&quot;Bloomfield_9810-hwe-geno0.05-mind0.1-WXSm&quot;

plink1.9 --bfile \${BFILE} --het --out \${BFILE}_QC_het

sed -i &quot;s/[[:space:]]\\+/\\t/g&quot; \${BFILE}_QC_het.het

sed -i &#39;s/\^\\t//g&#39; \${BFILE}_QC_het.het</code></pre>
<p>#R</p>
<pre><code>het \&lt;- read.table(&quot;Bloomfield_9810-hwe-geno0.05-mind0.1-WXSm_QC_het.het&quot;,
head=TRUE)

pdf(&quot;heterozygosity.pdf&quot;)

het\$HET_RATE = (het\$&quot;N.NM.&quot; - het\$&quot;O.HOM.&quot;)/het\$&quot;N.NM.&quot;

hist(het\$HET_RATE, xlab=&quot;Heterozygosity Rate&quot;, ylab=&quot;Frequency&quot;, main=
&quot;Heterozygosity Rate&quot;)

dev.off()

\#\#\#\#\# Find samples whoes het rate varies 3sd from the mean. using SNPs that
are not in LD (R_check)

\`het \&lt;- read.table(&quot;Bloomfield_9810-hwe-geno0.05-mind0.1-WXSm_QC_het.het&quot;,
head=TRUE)

het\$HET_RATE = (het\$&quot;N.NM.&quot; - het\$&quot;O.HOM.&quot;)/het\$&quot;N.NM.&quot;

het_fail = subset(het, (het\$HET_RATE \&lt;
mean(het\$HET_RATE)-3\*sd(het\$HET_RATE)) \| (het\$HET_RATE \&gt;
mean(het\$HET_RATE)+3\*sd(het\$HET_RATE)));

het_fail\$HET_DST = (het_fail\$HET_RATE-mean(het\$HET_RATE))/sd(het\$HET_RATE);

write.table(het_fail, &quot;fail-het-qc.txt&quot;, row.names=FALSE, quote=F, sep=&quot;\\t&quot;)</code></pre>
<p>Here is a plot of the heterozygosity rates for each sample:</p>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147443278-aed9bce4-55d7-4233-8d18-b1c62dc346af.png" alt="" />
<p class="caption">Figure 11</p>
</div>
<p>All samples that were more than 3 standard deviations from the mean were added
to fail-het-qc.txt.</p>
</div>
<div id="sex-check" class="section level3">
<h3>Sex Check</h3>
<p>I ran the sex check according to the code below.</p>
<pre><code>BFILE=&quot;Bloomfield_9810-hwe-geno0.05-mind0.1&quot;

plink --bfile \${BFILE} --update-sex Bloomfield-gVCFID-SEX.csv --make-bed --out
\${BFILE}_with_sex

plink1.9 --bfile \${BFILE}_with_sex --check-sex --out \${BFILE}_sex</code></pre>
<p>I filtered only those individuals with ‘PROBLEM’ listed in
Bloomfield_9810-hwe-geno0.05-mind0.1_sex.sexcheck. Then, I filtered again based
on there being a 1 or 2 in the PEDSEX and SNPSEX columns so that we take
individuals that have information. The final list of individuals with different
reported sexes are here:</p>
<pre><code>${DIR}/01-Bloomfield-preQC/02-intermediate-filtering-files/Bloomfield_sex_check_problems2.txt.</code></pre>
<p>There are a total of 25 individuals.</p>
</div>
<div id="pcas" class="section level3">
<h3>PCAs</h3>
<p>We created a PCA plot to determine the ethnic makeup of our dataset. Here is the
code:</p>
<pre><code>BFILE=&quot;Bloomfield_9810-hwe-geno0.05-mind0.1_with_STATUS&quot;

plink1.9 --bfile \${BFILE} --allow-no-sex --cluster --geno 0.01 --genome --hwe
0.001 --ld-window-r2 0.2 --maf 0.01 --mds-plot 4 --min 0.2 --nonfounders --pca
header --out \${BFILE}-PCAS</code></pre>
<p>#R</p>
<pre><code>library(ggplot2)

PCAs\&lt;-
read.table(&quot;Bloomfield_9810-hwe-geno0.05-mind0.1_with_STATUS-PCAS.eigenvec&quot;,
header=T)

ggplot(PCAs, aes(x=PC1, y=PC2)) + geom_point() + xlab(&quot;PC1&quot;) + ylab(&quot;PC2&quot;) +
ggtitle(&quot;Bloomfield-9694-PCA&quot;)

ggsave(&quot;Bloomfield-PCA.jpg&quot;, plot = last_plot(), device = NULL, scale = 1, width
= 16, height = 9, dpi = 300, limitsize = TRUE)</code></pre>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147443425-92f67ffa-71f0-4979-8861-204deae1fb5e.jpg" alt="" />
<p class="caption">Figure 12</p>
</div>
<p>Similarly, the PCA color coded by project was created by adding the column that
holds the project to the data variable as the Color:</p>
<pre><code>ggplot(PCA2, aes(x=PC1, y=PC2, color=Project)) + geom_point() + xlab(&quot;PC1&quot;) +
ylab(&quot;PC2&quot;) + ggtitle(&quot;Bloomfield-9694-PCA-project-color-coded&quot;)</code></pre>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147443474-c73a5c35-b8fb-4dfd-aad7-3ab0bd9bbc59.jpg" alt="" />
<p class="caption">Figure 13</p>
</div>
</div>
<div id="pca-with-hapmap" class="section level3">
<h3>PCA with Hapmap</h3>
<p>In order to view this PCA plot with known ethnic groups via Hapmap, I used the
code below in R:</p>
<p># # I used this code to anchor HAPMAP samples for plotting PCA</p>
<pre><code>PCA \&lt;-
read.table(&quot;Bloomfield_9810-hwe-geno0.05-mind0.1_with_STATUS-HAPMAP-MERGED3-for_PCA_no_mind.eigenvec&quot;,
header =T, stringsAsFactors=FALSE)

HAPMAP.ethnicty \&lt;- read.table(&quot;relationships_w_pops_121708.txt&quot;, header = T )

head(HAPMAP.ethnicty)

PCA\$COHORT \&lt;- &quot;Bloomfield&quot;

PCA\$COHORT \&lt;- HAPMAP.ethnicty\$population[match(PCA\$IID,
HAPMAP.ethnicty\$IID)]

PCA \&lt;- PCA[c(1:4,23)]

PCA\$COHORT \&lt;- as.character(PCA\$COHORT)

PCA\$COHORT[is.na(PCA\$COHORT)] \&lt;- &quot;Bloomfield&quot;

write.table(PCA,
&quot;Bloomfield_WXS_SNPS_INDELS-hwe-geno0.05-mind0.1-HAPMAP-MERGED3-for_PCA.eigenvec-PC1-PC2-COHORT.txt&quot;,
sep =&quot;\\t&quot;, col.names = T, quote = F)

\#\#\# PLOT PCAs in R

library(ggplot2)

PCAs\&lt;-
read.table(&quot;Bloomfield_WXS_SNPS_INDELS-hwe-geno0.05-mind0.1-HAPMAP-MERGED3-for_PCA.eigenvec-PC1-PC2-COHORT.txt&quot;,
header=T)

\#\#plotting:

target \&lt;- c(&quot;CEU&quot;, &quot;Bloomfield&quot;, &quot;JPT&quot;, &quot;YRI&quot;)

PCAs\$COHORT \&lt;- factor(PCAs\$COHORT, levels = target)

PCAs \&lt;- PCAs[order(-as.numeric(factor(PCAs\$COHORT))),]

ggplot(PCAs, aes(x=PC1, y=PC2, color=COHORT)) + geom_point() + xlab(&quot;PC1&quot;) +
ylab(&quot;PC2&quot;) + ggtitle(&quot;Bloomfield 9694&quot;) +

scale_color_manual(values = c(&#39;red&#39;,&#39;black&#39;,&#39;blue&#39;, &quot;green&quot;))

ggsave(&quot;Bloomfield_9694_Hapmap_PCA.jpg&quot;, plot = last_plot(), device = NULL,
scale = 1, width = 8, height = 4, dpi = 600, limitsize = TRUE)</code></pre>
<p>It is also important to note that you should take the mind flag out of the
hapmap plink command. I caused the Hapmap data to de-anchor from the Bloomfield
data. Here is the PCA plot with Hapmap:</p>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147443547-b13812ae-3469-43af-8b2e-408c22b2796d.jpg" alt="" />
<p class="caption">Figure 14</p>
</div>
</div>
</div>
</div>
<div id="duplicates---ibd" class="section level1">
<h1>Duplicates - IBD</h1>
<div id="ibd" class="section level3">
<h3>IBD</h3>
<p>Identity by Descent is a way to study the relatedness of samples in your
dataset. Below is the code that I used to create the plot below for this
dataset.</p>
<pre><code>BFILE=&quot;Bloomfield_9810-hwe-geno0.05-mind0.1_with_STATUS&quot;

plink1.9 --bfile \${BFILE} --geno 0.01 --genome --hwe 0.01 --ld-window-r2 0.2
\--maf 0.15 --out \${BFILE}-IBD

library(ggplot2)

IBD\&lt;-read.table(&quot;Bloomfield_9810-hwe-geno0.05-mind0.1_with_STATUS-IBD.genome&quot;,
head=T)

ggplot(IBD, aes(x=Z0, y=Z1))+ geom_point() + ggtitle(&quot;Bloomfield-9694-IBD&quot;)

ggsave(&quot;Bloomfield-IBD.jpg&quot;, plot = last_plot(), device = NULL, scale = 1, width
= 16, height = 9, dpi = 300, limitsize = TRUE)</code></pre>
<div class="figure">
<img src="https://user-images.githubusercontent.com/10935581/147443591-da9e00f6-8b83-4912-b32a-1fe5dd9833b9.jpg" alt="" />
<p class="caption">Figure 15</p>
</div>
</div>
<div id="appendices" class="section level2">
<h2>Appendices</h2>
<div id="appendix-a-code-to-upload-samples-as-they-go-pipeline-a" class="section level3">
<h3>Appendix A – Code to upload samples as they go + pipeline A</h3>
<p># Run the pipeline starting with the $START line number in the WORKLIST;
WORKLIST is the list of all FULLSMs to be processed for gVCF. $number is the
Nth sample being processed in the $WORKLIST.</p>
<pre><code>export BASE=&quot;/gscmnt/gc2645/wgs&quot;; \\
export WORKDIR=&quot;\${BASE}/tmp&quot;; \\
export THREADS=16; \\
export BWA_PIPE_SORT=1; \\
export TIMING=1; \\
LOOKUP_COL_SM=1; \\
LOOKUP_COL_DNA=2; \\
LOOKUP_COL_PR=3; \\
LOOKUP_COL_RGBASE=4; \\
LOOKUP_COL_FQ1EXT=5; \\
LOOKUP_COL_FQ2EXT=6; \\
export RUN_TYPE=&quot;paddedexome&quot;; \\
for FULLSM in \$(sed -n &quot;\${START},\${END}p&quot; &quot;\${WORKLIST}&quot;); do \\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
echo &quot;Doing sample number\*\*\*\*\*\*\*\*\*\*: &quot; \$Snumber; \\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
((Snumber=\${Snumber}+1)); \\
SM=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f1)&quot;; \\
IFS=\$&#39;\\n&#39; export DNA=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_SM} == \\&quot;\${SM}\\&quot;&quot;
&quot;\${LOOKUP}&quot; \| cut -d, -f\${LOOKUP_COL_DNA} \| sort -u)); \\
if [ \${\#DNA[@]} -gt 1 ]; then echo &quot;Warning, \\\${DNA} not unique for \${SM}
(n=\${\#DNA[@]}: \${DNA[@]})&quot;; fi; \\
DNA=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f2)&quot;; \\
PR=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f3)&quot;; \\
export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}&quot;; \\
echo -e &quot;00 - Starting jobs per sample FULLSM \${FULLSM}&quot;; \\
for RGBASE in \$(grep &quot;\${SM},\${DNA},\${PR}&quot; &quot;\${LOOKUP}&quot; \| cut -d,
\-f\${LOOKUP_COL_RGBASE}); do \\
SM=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f1)&quot;; \\
DNA=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f2)&quot;; \\
PR=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f3)&quot;; \\
FBASE=&quot;/40/AD/AD_Seq_Data/01.-RawData/201907_USUHS_gDNA_SHERMAN/01.-RawData&quot;; \\
\# folder where RG files are located \\
RBASE=&quot;/home/achal/01.-RawData/201907_USUHS_gDNA_SHERMAN/201907_USUHS_gDNA_SHERMAN&quot;;
\\
FQ1EXT=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_RGBASE} == \\&quot;\${RGBASE}\\&quot;&quot; &quot;\${LOOKUP}&quot;
\| cut -d, -f\${LOOKUP_COL_FQ1EXT})); \\
FQ2EXT=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_RGBASE} == \\&quot;\${RGBASE}\\&quot;&quot; &quot;\${LOOKUP}&quot;
\| cut -d, -f\${LOOKUP_COL_FQ2EXT})); \\
RGFILE=&quot;\${RBASE}/\${RGBASE}.rgfile&quot;; \\
FQ1=&quot;\${FBASE}/\${RGBASE}\${FQ1EXT}&quot;; \\
FQ2=&quot;\${FBASE}/\${RGBASE}\${FQ2EXT}&quot;; \\
DEST=&quot;\${BASE}/WXS_Aquilla/01-RAW&quot;; \\
echo -e &quot;00a - Uploading FASTQ and rgfiles for sample \${FULLSM} and RGBASE
\${RGBASE}\\nFQ1:\${FQ1}\\nFQ2:\${FQ2}\\nRGFILE:\${RGFILE}\\nDNA:\${DNA}&quot;; \\
mkdir \${DEST}/\${PR}/\${FULLSM}; \\
\#\#\# rsync with copy referent files as we are copying symlinks for this PPMI
data from fenix
rsync -avh -L \${USER}@fenix.psych.wucon.wustl.edu:\${FQ1}
\${DEST}/\${PR}/\${FULLSM}/; \\
rsync -avh -L \${USER}@fenix.psych.wucon.wustl.edu:\${FQ2}
\${DEST}/\${PR}/\${FULLSM}/; \\
rsync -avh \${USER}@fenix.psych.wucon.wustl.edu:\${RGFILE}
\${DEST}/\${PR}/\${FULLSM}/; \\
export RGFILE=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.rgfile&quot;;
\\

export FULLSM_RGID=&quot;\${RGBASE}&quot;; \\
unset BAMFILE; \\
echo -e &quot;01 - Starting bwa per sample \${FULLSM} and RGBASE \${RGBASE}&quot;; \\
export RGFILE=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.rgfile&quot;;
\\
export FQ1=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}\${FQ1EXT}&quot;;
\\
export FQ2=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}\${FQ2EXT}&quot;;
\\
export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
echo \$FQ1; \\
echo \$FQ1; \\
export CLEANUP=1;
export REMOVE_INPUT=1; \\
echo \$RGFILE; \\
export MEM=65; \\
bsub \\
\-J &quot;\${RGBASE}_s01alnsrt&quot; \\
\-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${RGBASE}_s01alnsrt.%J&quot; \\
\-u &quot;\${EMAIL}&quot; \\
\-n \${THREADS} -W 4320 \\
\-M 86000000 \\
\-R &quot;rusage[mem=89152]&quot; \\
\-q research-hpc \\
\-a &#39;docker(achalneupane/bwaref)&#39; \\
entrypoint.sh; \\
echo -e &quot;02 - Starting validatesam per sample \${FULLSM} and RGBASE
\${RGBASE}\\nBAMFILE:\${BAMFILE}&quot;; \\

export MEM=16; \\
export
BAMFILE=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/\${RGBASE}.aln.srt.bam&quot;;
\\
echo \${BAMFILE}; \\
export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
bsub \\
\-w &quot;done(\\&quot;\${RGBASE}_s01alnsrt\\&quot;)&quot; \\
\-J &quot;\${RGBASE}_s02vldate&quot; \\
\-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${RGBASE}_s02vldate.%J&quot; \\
\-u &quot;\${EMAIL}&quot; \\
\-n1 -W 1360 \\
\-R &quot;rusage[mem=18192]&quot; \\
\-q research-hpc \\
\-a &quot;docker(achalneupane/validatesamref)&quot; \\
entrypoint.sh -IGNORE INVALID_VERSION_NUMBER -IGNORE INVALID_TAG_NM; \\
echo -e &quot;03 - Starting intersectbed per sample \${FULLSM} and RGBASE
\${RGBASE}\\nBAMFILE:\${BAMFILE}&quot;; \\
export RUN_TYPE=&quot;paddedexome&quot;; \\
export BEDFILE=&quot;\${BASE}/Genome_Ref/GRCh38/Capture_Padded.GRCh38.bed&quot;; \\
export COVERED_BED=&quot;\${BASE}/Genome_Ref/GRCh38/Capture_Covered.GRCh38.bed&quot;; \\
export PADDED_BED=&quot;\${BASE}/Genome_Ref/GRCh38/Capture_Padded.GRCh38.bed&quot;;\\
export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
export
BAMFILE=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/\${RGBASE}.aln.srt.bam&quot;;
\\
bsub \\
\-w &quot;done(\\&quot;\${RGBASE}_s02vldate\\&quot;)&quot; \\
\-J &quot;\${RGBASE}_s03intsct&quot; \\
\-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${RGBASE}_s03intsct.%J&quot; \\
\-u &quot;\${EMAIL}&quot; \\
\-n1 -W 1360 \\
\-q research-hpc \\
\-a &quot;docker(achalneupane/intersectbedref)&quot; \\
entrypoint.sh; \\
echo -e &quot;04 - Starting validatesam per sample \${FULLSM} and RGBASE
\${RGBASE}\\nBAMFILE:\${BAMFILE}&quot;; \\
export
BAMFILE=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/\${RGBASE}.aln.srt.isec-\${RUN_TYPE}.bam&quot;;
\\
bsub \\
\-w &quot;done(\\&quot;\${RGBASE}_s03intsct\\&quot;)&quot; \\
\-J &quot;\${RGBASE}_s04vldate&quot; \\
\-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${RGBASE}_s04vldate.%J&quot; \\
\-u &quot;\${EMAIL}&quot; \\
\-n1 -W 1360 \\
\-R &quot;rusage[mem=18192]&quot; \\
\-q research-hpc \\
\-a &quot;docker(achalneupane/validatesamref)&quot; \\
entrypoint.sh -IGNORE INVALID_VERSION_NUMBER -IGNORE INVALID_TAG_NM -IGNORE
MATE_NOT_FOUND; \\
echo -e &quot;05 - Starting bamtocram per sample \${FULLSM} and RGBASE
\${RGBASE}\\nBAMFILE:\${BAMFILE}\\nOUT_DIR:\${OUT_DIR}&quot;;\\
export
BAMFILE=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/\${RGBASE}.aln.srt.bam&quot;;
\\
export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/&quot;; \\
done; \\
echo -e &quot;DONE ANALYZING RGBASES per sample \${FULLSM}&quot;;\\
IFS=\$&#39;\\n&#39; RGBASES=(\$(grep &quot;\${FULLSM}&quot; &quot;\${LOOKUP}&quot; \| cut -d,
\-f\${LOOKUP_COL_RGBASE})); \\
INPUT_LIST=(); \\
WAIT_LIST=(); \\
CLEANUP_LIST=(); \\
for RGBASE in \${RGBASES[@]}; do \\
INPUT_LIST+=(&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/\${RGBASE}.aln.srt.isec-\${RUN_TYPE}.bam&quot;);
\\
WAIT_LIST+=(&quot;&amp;&amp;&quot; &quot;done(\\&quot;\${RGBASE}_s04vldate\\&quot;)&quot;); \\
CLEANUP_LIST+=(&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.\${FQ1EXT}&quot;);
\\
CLEANUP_LIST+=(&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.\${FQ2EXT}&quot;);
\\
done; \\
echo -e &quot;06 - Starting markduplicates for FULLSM \${FULLSM} with
INPUT_LIST:\${INPUT_LIST[@]}&quot;; \\
export FULLSM=&quot;\${FULLSM}&quot;; \\
export MEM=32; \\
export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}/&quot;; \\
bsub \\
\-w \${WAIT_LIST[@]:1} \\
\-J &quot;\${FULLSM}_s06mrkdup&quot; \\
\-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${FULLSM}_s06mrkdup.%J&quot; \\
\-u &quot;\${EMAIL}&quot; \\
\-n1 -W 1440 \\
\-M 46000000 \\
\-R &quot;rusage[mem=49152]&quot; \\
\-q research-hpc \\
\-a &quot;docker(achalneupane/markduplicates)&quot; \\
\${INPUT_LIST[@]};</code></pre>
</div>
<div id="appendix-b-pipeline-b" class="section level3">
<h3>Appendix B – Pipeline B</h3>
<pre><code>START=1;
END=1;
SHELLDROP=1
Snumber=1;
export BASE=&quot;/gscmnt/gc2645/wgs&quot;; \\
export WORKDIR=&quot;\${BASE}/tmp&quot;; \\
export THREADS=16; \\
export BWA_PIPE_SORT=1; \\
export TIMING=1; \\
LOOKUP_COL_SM=1; \\
LOOKUP_COL_DNA=2; \\
LOOKUP_COL_PR=3; \\
LOOKUP_COL_RGBASE=4; \\
LOOKUP_COL_FQEXT=5; \\
LOOKUP_COL_RGBASEtemp=6; \\
unset FBASE; \\
export RUN_TYPE=&quot;paddedexome&quot;; \\
export FASTQ_TYPE=&quot;interleaved&quot;; \\
export MODE=&quot;fq&quot;; \\
for FULLSM in \$(sed -n &quot;\${START},\${END}p&quot; &quot;\${WORKLIST}&quot;); do \\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
echo &quot;Doing sample number\*\*\*\*\*\*\*\*\*\*: &quot; \${Snumber}; \\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
echo
&quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
\\
((Snumber=\${Snumber}+1)); \\
SM=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f1)&quot;; \\
IFS=\$&#39;\\n&#39; export DNA=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_SM} == \\&quot;\${SM}\\&quot;&quot;
&quot;\${LOOKUP}&quot; \| cut -d, -f\${LOOKUP_COL_DNA} \| sort -u)); \\
if [ \${\#DNA[@]} -gt 1 ]; then echo &quot;Warning, \\\${DNA} not unique for \${SM}
(n=\${\#DNA[@]}: \${DNA[@]})&quot;; fi; \\
SM=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f1)&quot;; \\
DNA=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f2)&quot;; \\
PR=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f3)&quot;; \\
export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}&quot;; \\
echo -e &quot;00 - Starting jobs per sample FULLSM \${FULLSM}&quot;; \\
for RGBASE in \$(grep &quot;\${SM},\${DNA},\${PR}&quot; &quot;\${LOOKUP}&quot; \| cut -d,
\-f\${LOOKUP_COL_RGBASE}); do \\
FULLSMtemp=&quot;\${SM}\^unk\^\${PR}&quot;; \\
FBASEtemp=&quot;/40/AD/AD_Seq_Data/01.-RawData/CACHE_WGS/01.-RawData/\${FULLSMtemp}&quot;;
\\
FBASE=&quot;/40/AD/AD_Seq_Data/01.-RawData/CACHE_WGS/01.-RawData/\${FULLSM}&quot;; \\
FQEXT=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_RGBASE} == \\&quot;\${RGBASE}\\&quot;&quot; &quot;\${LOOKUP}&quot;
\| cut -d, -f\${LOOKUP_COL_FQEXT})); \\
RGBASEtemp=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_RGBASE} == \\&quot;\${RGBASE}\\&quot;&quot;
&quot;\${LOOKUP}&quot; \| cut -d, -f\${LOOKUP_COL_RGBASEtemp})); \\
RGFILEtemp=&quot;\${FBASEtemp}/\${RGBASEtemp}.rgfile&quot;; \\
RGFILE=&quot;\${FBASE}/\${RGBASE}.rgfile&quot;;\\
FQtemp=&quot;\${FBASEtemp}/\${RGBASEtemp}.\${FQEXT}&quot;;\\
FQ=&quot;\${FBASE}/\${RGBASE}.\${FQEXT}&quot;; \\
DEST=&quot;\${BASE}/WXS_Aquilla/01-RAW&quot;;\\
echo -e &quot;00a - Uploading FASTQ and rgfiles for sample \${FULLSM} and RGBASE
\${RGBASE}\\nFQ:\${FQ}\\nRGFILE:\${RGFILE}\\nDNA:\${DNA}&quot;; \\
mkdir \${DEST}/\${PR}/\${FULLSM}; \\
rsync -avh -L \${USER}@fenix.psych.wucon.wustl.edu:\${FQtemp}
\${DEST}/\${PR}/\${FULLSM}/ ;\\
mv -f &quot;\${DEST}/\${PR}/\${FULLSM}/\${RGBASEtemp}.\${FQEXT}&quot;
&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.\${FQEXT}&quot;; \\
rsync -avh \${USER}@fenix.psych.wucon.wustl.edu:\${RGFILEtemp}
\${DEST}/\${PR}/\${FULLSM}/ ;\\
mv -f &quot;\${DEST}/\${PR}/\${FULLSM}/\${RGBASEtemp}.rgfile&quot;
&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.rgfile&quot;; \\
export RGFILE=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.rgfile&quot;;
\\
export FULLSM_RGID=&quot;\${RGBASE}&quot;; \\
unset BAMFILE; \\
unset FQ1; \\
unset FQ2; \\
IFS=\$&#39;\\n&#39; FQEXT=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_RGBASE} == \\&quot;\${RGBASE}\\&quot;&quot;
&quot;\${LOOKUP}&quot; \| cut -d, -f\${LOOKUP_COL_FQEXT})); \\
echo -e &quot;01 - Starting bwa per sample \${FULLSM} and RGBASE \${RGBASE}&quot;;\\
export RGFILE=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.rgfile&quot;;
\\
export FQ=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.\${FQEXT}&quot;;
\\
export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
export MEM=32; \\
bsub \\
\-J &quot;\${RGBASE}_s01alnsrt&quot; \\
\-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${RGBASE}_s01alnsrt.%J&quot; \\
\-u &quot;\${EMAIL}&quot; \\
\-n \${THREADS} -W 2160 \\
\-M 46000000 \\
\-R &quot;rusage[mem=49152]&quot; \\
\-q research-hpc \\
\-a &#39;docker(achalneupane/bwaref)&#39; \\
entrypoint.sh; \\
\# The rest of the code after achalneupane/bwaref is the same as in Appendix A
!!</code></pre>
</div>
<div id="appendix-c-pipeline-c-cram-or-bam-to-fastq" class="section level3">
<h3>Appendix C – pipeline C Cram or Bam to FastQ</h3>
<ol style="list-style-type: decimal">
<li>Helper script to feed input CRAM/BAM and unwrap them into FASTQ (replace
“.crai” with “.bai” if using BAM as an input)</li>
</ol>
<pre><code>    Snumber=1
    START=1; \\
    END=156; \\
    DELAY=10
    EMAIL=&quot;achal@wustl.edu&quot;
    SHELLDROP=0
    export BASE=&quot;/gscmnt/gc2645/wgs&quot;; \\
    export WORKDIR=&quot;\${BASE}/tmp&quot;; \\
    export THREADS=16; \\
    export BWA_PIPE_SORT=1; \\
    export TIMING=1;\\
    PR=&quot;202103_ADSP_FUS-familial_WGS_UNNAMED&quot;; \\
    WORKLIST=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}-worklist_fullsm.csv&quot;; \\
    LOOKUP=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}-MGIID-sm-dna-pr-rgbase-cram-smdir-cramloc.csv&quot;;
    \\
    LOOKUP_COL_SM=2; \\
    LOOKUP_COL_DNA=3; \\
    LOOKUP_COL_PR=4; \\
    LOOKUP_COL_CRAMFILE=5; \\
    export RUN_TYPE=&quot;paddedexome&quot;; \\
    for FULLSM in \$(sed -n &quot;\${START},\${END}p&quot; &quot;\${WORKLIST}&quot;); do \\
    echo
    &quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
    \\
    echo
    &quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
    \\
    echo &quot;Doing sample
    number\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*:
    &quot; \$Snumber; \\
    echo
    &quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
    \\
    echo
    &quot;\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*&quot;;
    \\
    ((Snumber=Snumber+1)); \\
    export FULLSM=&quot;\${FULLSM}&quot;; \\
    SM=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f1)&quot;; \\
    IFS=\$&#39;\\n&#39; export DNA=(\$(awk -F, &quot;\\\$\${LOOKUP_COL_SM} == \\&quot;\${SM}\\&quot;&quot;
    &quot;\${LOOKUP}&quot; \| cut -d, -f\${LOOKUP_COL_DNA} \| sort -u)); \\
    if [ \${\#DNA[@]} -gt 1 ]; then echo &quot;Warning, \\\${DNA} not unique for
    \${SM} (n=\${\#DNA[@]}: \${DNA[@]})&quot;; fi; \\
    DNA=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f2)&quot;; \\
    PR=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f3)&quot;; \\
    CRAM=&quot;\$(grep &quot;\${SM},\${DNA},\${PR}&quot; &quot;\${LOOKUP}&quot; \| cut -d,
    \-f\${LOOKUP_COL_CRAMFILE})&quot;
    export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}&quot;; \\
    INPUT_LIST=(); \\
    WAIT_LIST=(); \\
    rsync -avh \${USER}@fenix.psych.wucon.wustl.edu:\${CRAMLOC}/\${CRAM}
    \${OUT_DIR}/; \\
    rsync -avh \${USER}@fenix.psych.wucon.wustl.edu:\${CRAMLOC}.\${CRAM}.crai
    \${OUT_DIR}/; \\
    export MEM=16; \\
    export CREATE_RGFILE=1; \\
    export DEBUG=1; \\
    export REMOVE_INPUT=1; \\
    export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}&quot; ;\\
    export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
    export CRAMBASE=&quot;\$(echo \${CRAM/.cram/})&quot;; \\
    export CRAMFILE=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${CRAM}&quot;; \\
    echo -e &quot;OUTPUT is in
    &quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${CRAMBASE}_s00rvtcram.%J&quot;
    \\n OUTDIR is \${OUT_DIR}&quot;;\\
    bsub \\
    \-J &quot;\${FULLSM}_s00rvtcram&quot; \\
    \-o
    &quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${CRAMBASE}_s00rvtcram.%J&quot;
    \\
    \-u &quot;\${EMAIL}&quot; \\
    \-n1 -W 1440 \\
    \-R &quot;rusage[mem=18192]&quot; \\
    \-q research-hpc \\
    \-a &#39;docker(achalneupane/cram2fastq)&#39; \\
    /bin/bash; \\
    echo -e &quot;00b - Starting revertcram for
    \${FULLSM}\\n\${CRAMFILE}\\n\${SM}\\n\${DNA}\\n\${PR}&quot;; \\
    SAMTOOLS=&quot;/gscmnt/gc2645/wgs/variant_calling/samtools&quot;; \\
    IFS=\$&#39;\\n&#39; RGS=(\$(\${SAMTOOLS} view -H &quot;\${CRAMFILE}&quot; \| grep &quot;\^@RG&quot;));
    \\
    for RG in \${RGS[@]}; do RGID=&quot;\$(echo \${RG} \| grep -oP
    &quot;(?\&lt;=ID:)[\^[:space:]]\*&quot;)&quot;; \\
    RGID_NEW=&quot;\$(echo \${RGID} \| cut -d: -f2- \| sed &#39;s/:/\^/g&#39;)&quot;; \\
    RGBASE=&quot;\${FULLSM}.\${RGID_NEW}&quot;; \\
    echo -e &quot;01 - Starting bwa per sample \${FULLSM} and RGBASE
    \${RGBASE}\\nFQ1:\${FQ1}\\nFQ2:\${FQ2}\\nRGFILE:\${RGFILE}\\nDNA:\${DNA}\\nPR:\${PR}\\nIFS:\${IFS}&quot;;\\
    export
    RGFILE=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.rgfile&quot;; \\
    export FULLSM_RGID=&quot;\${RGBASE}&quot;; \\
    unset BAMFILE; \\
    export OUT_DIR=&quot;\${BASE}/WXS_Aquilla/02-TRANSIT/\${PR}/\${FULLSM}&quot;; \\
    FQ1EXT=&quot;r1.fastq&quot;; \\
    FQ2EXT=&quot;r2.fastq&quot;; \\
    export
    FQ1=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.\${FQ1EXT}&quot;;
    \\
    export
    FQ2=&quot;\${BASE}/WXS_Aquilla/01-RAW/\${PR}/\${FULLSM}/\${RGBASE}.\${FQ2EXT}&quot;;
    \\
    export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
    export MEM=52; \\
    export CLEANUP=1;
    export REMOVE_INPUT=1; \\
    bsub \\
    \-w &quot;done(\\&quot;\${FULLSM}_s00rvtcram\\&quot;)&quot; \\
    \-J &quot;\${RGBASE}_s01alnsrt&quot; \\
    \-o &quot;\${BASE}/WXS_Aquilla/04-LOGS/\${RGBASE}_s01alnsrt.%J&quot; \\
    \-u &quot;\${EMAIL}&quot; \\
    \-n \${THREADS} -W 2880 \\
    \-M 66000000 \\
    \-R &quot;rusage[mem=69152]&quot; \\
    \-q research-hpc \\
    \-a &#39;docker(achalneupane/bwaref)&#39; \\
    /bin/bash; \\</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>The rest of the code after achalneupane/bwaref is the same as in Appendix A</li>
<li>Docker image for achalneupane/cram2fastq</li>
</ol>
<pre><code>    \# RevertSam and SamToFastq are the tools that unwraps the CRAM or BAM files
    into paired FASTQ per RG ID
    CUR_STEP=&quot;RevertSam&quot;
    start=\$(\${DATE}); echo &quot;[\$(display_date \${start})] \${CUR_STEP}
    starting&quot;
    &quot;\${TIMING[@]}&quot; /usr/local/openjdk-8/bin/java \${JAVAOPTS} -jar &quot;\${PICARD}&quot;
    \\
    &quot;\${CUR_STEP}&quot; \\
    \-I &quot;\${CRAMFILE}&quot; \\
    \-R \${REF} \\
    \-O /dev/stdout \\
    \-SORT_ORDER queryname \\
    \-COMPRESSION_LEVEL 0 \\
    \-VALIDATION_STRINGENCY SILENT \\
    \| /usr/local/openjdk-8/bin/java \${JAVAOPTS} -jar &quot;\${PICARD}&quot; \\
    SamToFastq \\
    \-I /dev/stdin \\
    \-R \${REF} \\
    \-OUTPUT_PER_RG true \\
    \-RG_TAG ID \\
    \-OUTPUT_DIR &quot;\${OUT_DIR}&quot; \\
    \-VALIDATION_STRINGENCY SILENT
    \# Transfer of files
    CUR_STEP=&quot;Transfer of files&quot;
    start=\$(\${DATE}); echo &quot;[\$(display_date \${start})] \${CUR_STEP}
    starting&quot;
    CRAMLINES=\$(samtools idxstats &quot;\${CRAMFILE}&quot; \| awk &#39;{s+=\$3+\$4} END
    {print s\*4}&#39;)
    FQLINES=\$(cat \${OUT_DIR}/\*.fastq \| wc -l)
    if [ ! -z \${DEBUG} ]; then
    echo &quot;\${CRAMLINES} lines in .cram/bam&quot;
    echo &quot;\${FQLINES} lines in all .fastq files&quot;
    if [ \$(echo &quot;scale=2;\${FQLINES}/\${CRAMLINES} \&gt; 0.90&quot; \| bc) -eq 0 ];
    then
    echo &quot;Warning, .fastq files contain less than 90% of the number of reads of
    .cram/bam file&quot;
    fi
    fi
    \#\# Create RGFILE
    CUR_STEP=&quot;Create RGFILE&quot;
    start=\$(\${DATE}); echo &quot;[\$(display_date \${start})] \${CUR_STEP}
    starting&quot;
    if [ ! -z &quot;\${CREATE_RGFILE}&quot; ]; then
    SM=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f1)&quot;
    DNA=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f2)&quot;
    PR=&quot;\$(echo &quot;\${FULLSM}&quot; \| cut -d\^ -f3)&quot;
    IFS=\$&#39;\\n&#39; RGS=(\$(samtools view -H &quot;\${CRAMFILE}&quot; \| grep &quot;\^@RG&quot;))
    echo &quot;Creating \${\#RGS[@]} .rgfiles for newly created .fastq files&quot;
    echo &quot;Moving \${\#RGS[@]} .fastq files to \${OUT_DIR}/&quot;
    for RG in \${RGS[@]}; do
    RGID=&quot;\$(echo \${RG} \| grep -oP &quot;(?\&lt;=ID:)[\^[:space:]]\*&quot;)&quot;
    RGID_NEW=&quot;\$(echo \${RGID} \| cut -d: -f2- \| sed &#39;s/:/\^/g&#39;)&quot;
    mv -vf &quot;\${OUT_DIR}/\${RGID//:/_}_1.fastq&quot;
    &quot;\${OUT_DIR}/\${FULLSM}.\${RGID_NEW}.r1.fastq&quot;
    if [ -f &quot;\${OUT_DIR}/\${RGID//:/_}_2.fastq&quot; ]; then mv -vf
    &quot;\${OUT_DIR}/\${RGID//:/_}_2.fastq&quot;
    &quot;\${OUT_DIR}/\${FULLSM}.\${RGID_NEW}.r2.fastq&quot;; fi
    RGPU=&quot;\$(echo \${RG} \| grep -oP &quot;(?\&lt;=PU:)[\^[:space:]]\*&quot;)&quot;
    RGLB=&quot;\${SM}.\${PR}&quot;
    echo
    &quot;@RG\\tID:\${RGID}\\tPL:illumina\\tPU:\${RGPU}\\tLB:\${RGLB}\\tSM:\${SM}\\tDS:\${SM}\^\${DNA}\^\${PR}&quot;
    \&gt; &quot;\${OUT_DIR}/\${FULLSM}.\${RGID_NEW}.rgfile&quot;
    done
    fi
    \#\# Cleaning up files
    CUR_STEP=&quot;Cleaning up files&quot;
    start=\$(\${DATE}); echo &quot;[\$(display_date \${start})] \${CUR_STEP}
    starting&quot;
    if [ \${exitcode} -eq 0 ] &amp;&amp; [ \${REMOVE_INPUT} -eq 1 ]; then
    rm -fv &quot;\${CRAMFILE}&quot; &quot;\${CRAMFILE%.cram}.crai&quot; &quot;\${CRAMFILE}.crai&quot;
    2\&gt;/dev/null
    echo -e &quot;REMOVE_INPUT was set to \${REMOVE_INPUT} then CRAMFILE is removed&quot;
    else
    echo -e &quot;REMOVE_INPUT was set to \${REMOVE_INPUT} then CRAMFILE is NOT
    removed&quot;
    fi</code></pre>
</div>
<div id="appendix-d--bam-to-fastq-processing-specific-to-the-adni-project" class="section level3">
<h3>Appendix D- BAM to FASTQ processing (specific to the ADNI project)</h3>
<ol style="list-style-type: decimal">
<li>Docker image (achalneupane/bam2fqv2) to extract interleaved FASTQ from BAM</li>
</ol>
<pre><code>    \# Extract read Group information
    IFS=\$&#39;\\n&#39; RGS=(\$(samtools view -@ \${THREADS} -h \${BAMFILE} \| head -n
    10000000 \| grep \^HS2000 \| cut -d\$&#39;\\t&#39; -f1\| cut -d: -f1,2 \| sort -V \|
    uniq \| grep \^HS2000))
    echo &quot;Readgroups are \${RGS[@]}&quot;
    unset IFS
    args=(tee)
    for RG in \${RGS[@]}; do
    args+=(\\\&gt;\\(grep -A3 --no-group-separator \\&quot;\^@\${RG/\^/:}:\\&quot; \\\| gzip
    \\\&gt; \${OUT_DIR}/\${SM}\^\${DNA}\^\${PR}.\${RG/:/.}.fq.gz\\))
    done
    args+=(\\\&gt;/dev/null)
    JAVA=&quot;/usr/local/openjdk-8/bin/java&quot;
    \# JAVAOPTS=&quot;-Xms2g -Xmx\${MEM}g -XX:+UseSerialGC
    \-Dpicard.useLegacyParser=false&quot;
    JAVAOPTS=&quot;-Xms4g -Xmx\${MEM}g -XX:ParallelGCThreads=\${THREADS}
    \-Djava.io.tmpdir=\${TMP_DIR}&quot;
    CUR_STEP=&quot;RevertSam&quot;
    start=\$(\${DATE}); echo &quot;[\$(display_date \${start})] \${CUR_STEP}
    starting&quot;
    &quot;\${TIMING[@]}&quot; \${JAVA} \${JAVAOPTS} -jar &quot;\${PICARD}&quot; \\
    &quot;\${CUR_STEP}&quot; \\
    I=&quot;\${BAMFILE}&quot; \\
    O=/dev/stdout \\
    SORT_ORDER=queryname \\
    COMPRESSION_LEVEL=0 \\
    VALIDATION_STRINGENCY=SILENT \\
    TMP_DIR=\${TMP_DIR} \\
    \| \${JAVA} \${JAVAOPTS} -jar &quot;\${PICARD}&quot; \\
    SamToFastq \\
    I=/dev/stdin \\
    FASTQ=/dev/stdout \\
    INTERLEAVE=TRUE \\
    VALIDATION_STRINGENCY=SILENT \\
    TMP_DIR=\${TMP_DIR} \| eval \${args[@]}
    arr=(\${PIPESTATUS[@]}); exitcode=0; for i in \${arr[@]}; do
    ((exitcode+=i)); done</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>The rest of the pipeline is the same as in Appendix C + Appendix B
### Appendix E- SRA to FASTQ</li>
<li>Function to download SRA and extract FASTQ</li>
</ol>
<pre><code>    getSRAtoFastq()
    {
    DIR=\$1
    SAMPLE=\$2
    mkdir -p \${DIR}/\${SAMPLE}
    SRR=&quot;\$(echo \$SAMPLE \| cut -d\^ -f2)&quot;
    echo &quot;Doing: &quot; \${SRR}
    \# LOOKUP=&quot;/30/dbGaP/6109/sra/lookup.csv&quot;
    \# If \*.Confirm.txt file is not present; then only run this. We will also
    validate SRA and fastqs as we download/process.
    if ! [ -f \${DIR}/\${SAMPLE}/\*&quot;.Confirm.txt&quot; ]; then
    prefetch --ngc /30/dbGaP/6109/prj_6109.ngc \${SRR}
    vdb-validate ./\${SRR} 2\&gt; \&gt;(grep -i Column)
    vdb-validate ./\${SRR} 2\&gt; &quot;\${DIR}/vdbValidate/vdb-validate_\${SRR}.txt&quot;
    cat &quot;\${DIR}/vdbValidate/vdb-validate_\${SRR}.txt&quot; \| grep Column \&gt;
    &quot;\${DIR}/vdbValidate/Columns_\${SRR}.txt&quot;
    col_numbers=&quot;\$(cat &quot;\${DIR}/vdbValidate/Columns_\${SRR}.txt&quot; \| wc -l)&quot;
    ok_numbers=&quot;\$(cat &quot;\${DIR}/vdbValidate/Columns_\${SRR}.txt&quot; \| grep ok \|
    wc -l)&quot;
    if [ \${col_numbers} -eq \${ok_numbers} ] &amp;&amp; [ \${col_numbers} -ne 0 ]; then
    \#start fq split
    echo \$col_numbers &quot;Cols are equal &quot; \$ok_numbers &quot;OKs&quot; \&gt;
    &quot;\${DIR}/\${SAMPLE}/\${SAMPLE}.txt&quot;
    IFS=\$&#39;\\n&#39;
    RGLINES=(\$(sam-dump --ngc /30/dbGaP/6109/prj_6109.ngc ./\${SRR} \| sed -n
    &#39;/\^[\^@]/!p;//q&#39; \| grep \^@RG))
    args=(tee)
    for RGLINE in \${RGLINES[@]}; do
    unset IFS
    RG=(\${RGLINE})
    args+=(\\\&gt;\\(grep -A3 --no-group-separator \\&quot;\\\\.\${RG[1]\#ID:}/[12]\$\\&quot;
    \\\| gzip \\\&gt;
    &quot;./\${SRR}.\${RG[1]\#ID:}.fastq-dump.split.defline.z.tee.fq.gz&quot;\\))
    done
    args+=(\\\&gt;/dev/null)
    echo &quot;Splitting \${SRR}.sra into \${\#RGLINES[@]} ReadGroups&quot;
    \#\#\# NOTE: split-e wouldn&#39;t work in the downstream pipeline!!!!!
    \# wget
    http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz
    \# fastq-dump --ngc /30/dbGaP/6109/prj_6109.ngc --skip-technical --split-e
    \--defline-seq &#39;@\$ac.\$si.\$sg/\$ri&#39; --defline-qual &#39;+&#39; -Z &quot;./\${SRR}&quot; \|
    eval \${args[@]}
    /30/dbGaP/6109/sra/phs000572_201802/fqgz/test/sratoolkit.2.10.8-ubuntu64/bin/fastq-dump-orig.2.10.8
    \--ngc /30/dbGaP/6109/prj_6109.ngc --split-3 --defline-seq
    &#39;@\$ac.\$si.\$sg/\$ri&#39; --defline-qual &#39;+&#39; -Z &quot;./\${SRR}&quot; \| eval \${args[@]}
    if [ \$? -ne 0 ]; then
    echo &quot;Error running fastq-dump, exiting.&quot;
    exit 1
    fi
    \# Validate the .fq.gz that are produced. Ensuring the dowloand was not
    faulty
    if [ \$(ls &quot;./\${SRR}&quot;\*fq 2\&gt;/dev/null \| wc -l) -eq 0 ]; then
    if [ \$(ls &quot;./\${SRR}&quot;\*fq.gz 2\&gt;/dev/null \| wc -l) -eq 0 ]; then
    echo &quot;Error, cannot find any .fq or .fq.gz files for \${SRR}&quot;
    exit 1
    else
    MODE=gz
    EXT=&quot;fq.gz&quot;
    fi
    else
    MODE=fq
    EXT=&quot;fq&quot;
    fi
    echo &quot;Validating .\${EXT} created from \${SRR}&quot;
    exitcode=0
    IFS=\$&#39;\\n&#39;
    SRAINFO=(\$(/usr/local/genome/bin/sra-stat --ngc /30/dbGaP/6109/prj_6109.ngc
    \--quick ./\${SRR}))
    for line in \${SRAINFO[@]}; do
    IFS=&quot;\|&quot;
    split1=(\${line})
    RG=\${split1[1]}
    IFS=&quot;:&quot;
    split2=(\${split1[2]})
    READS=\${split2[0]}
    ((READS\*=8))
    unset IFS
    echo -n &quot;Checking \${SRR} ReadGroup \${RG}, expect \${READS} lines...&quot;
    if [ \${MODE} = &quot;gz&quot; ]; then
    LINES=\$(zcat &quot;./\${SRR}.\${RG}.fastq-dump.split.defline.z.tee.\${EXT}&quot; \|
    wc -l)
    elif [ \${MODE} = &quot;fq&quot; ]; then
    LINES=\$(wc -l &quot;./\${SRR}.\${RG}.fastq-dump.split.defline.z.tee.\${EXT}&quot;)
    fi
    echo &quot;found \${LINES}&quot;
    if [ \${READS} -ne \${LINES} ]; then
    ((exitcode+=1))
    fi
    done
    if [ \${exitcode} -eq 0 ]; then
    SM=&quot;\$(echo \$SAMPLE \| cut -d\^ -f1)&quot;;
    DNA=&quot;\${SRR}&quot;
    PR=&quot;\$(echo \$SAMPLE \| cut -d\^ -f3)&quot;;
    FULLSM=&quot;\${SM}\^\${DNA}\^\${PR}&quot;
    IFS=\$&#39;\\n&#39;
    for RGLINE in \${RGLINES[@]}; do
    OLD_RGID=\$(echo \${RGLINE} \| grep -o &quot;ID:[\^[:space:]]\*&quot; \| sed
    &#39;s/ID://g&#39;)
    NEW_RGID=\$(echo \${OLD_RGID} \| sed &#39;s/\\./\^/g;s/_/\^/g&#39;)
    mv &quot;\${SRR}.\${OLD_RGID}.fastq-dump.split.defline.z.tee.\${EXT}&quot;
    &quot;./\${FULLSM}/\${FULLSM}.\${NEW_RGID}.\${EXT}&quot;
    echo \${RGLINE} \| sed
    &quot;s@\\bSM:\\([\^[:space:]]\*\\)\\([[:space:]]\\)@SM:\${SM}\\2@g;s/\\t/\\\\\\t/g&quot;
    \&gt; &quot;./\${FULLSM}/\${FULLSM}.\${NEW_RGID}.rgfile&quot;
    done
    echo &quot;All spots from \${SRR} are represented in associated .\${EXT} files&quot;
    \&gt; &quot;./\${FULLSM}/\${FULLSM}.Confirm.txt&quot;
    rm -rf \${SRR}
    unset IFS
    elif [ \${exitcode} -ne 0 ]; then
    echo &quot;Errors encountered&quot; \&gt;\&gt; &quot;\${DIR}/DIRerrors/\${SAMPLE}.txt&quot;
    rm -rf \${SRR}
    fi
    fi
    rm -rf \${SRR}
    srr_count=&quot;\$(find \${DIR}/\*/\*.Confirm.txt \| wc -l)&quot;;
    echo &quot;Now getting SRR count: &quot; \$srr_count
    fi
    }</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Run the function getSRAtoFastq</li>
</ol>
<pre><code>    export -f getSRAtoFastq export \${DIR} export
    WORKLIST=&quot;\${DIR}/phs000572_201707_94samples_achal.csv&quot; export srr_count=0
    \# I will download 8 SRAs in parallel
    while [ \${srr_count} -lt 94 ]; do parallel -j8 getSRAtoFastq \${DIR} {}
    :::: &quot;\${WORKLIST}&quot;
    srr_count=&quot;\$(find \${DIR}/\*/\*.Confirm.txt \| wc -l)&quot;; echo &quot;Now getting
    SRR count: &quot; \$srr_count done</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>The rest of the pipeline is the same as in Appendix B
### Appendix F- Joint calling pipeline</li>
</ol>
<pre><code>1.  GenomicsDBIMPORT
    EMAIL=&quot;achal@wustl.edu&quot;
    arr=&quot;\$(echo {1..22} X Y)&quot;
    len=\${\#arr[\*]}
    \#iterate with a for loop
    export BASE=&quot;/gscmnt/gc2645/wgs&quot;; \\
    export TILEDB_DISABLE_FILE_LOCKING=1; \\
    export MEM=40; \\
    export mylist=&quot;\${BASE}/WXS_Aquilla/03-FINAL/VCFs/ID_LIST_genomicsDB.list&quot;;
    \\
    \# /tmp folder on MGI is limited to 250gb, but this process requires \~3TB
    of tmp space to process \~8000 samples
    export tmpPATH=&quot;\${BASE}/WXS_Aquilla/gvcfTest/temp/&quot;; \\
    export THREADS=16;
    for (( i=0; i\&lt;len; i++ ));
    do
    CHR=&quot;chr\${arr[\$i]}&quot;
    echo &quot;Doing ::&quot; \${CHR}
    \# Define WORKING Directory
    export WORKDIR=&quot;\${BASE}/WXS_Aquilla//03-FINAL/VCFs/\${CHR}&quot;; \\
    bsub \\
    \-J &quot;OtoDB\${CHR}&quot; \\
    \-u &quot;\${EMAIL}&quot; \\
    \-n \${THREADS} -W 25160 \\
    \-M 490000000 \\
    \-R &quot;rusage[mem=49152]&quot; \\
    \-o &quot;\${BASE}/WXS_Aquilla//03-FINAL/VCFs/\${CHR}.%J&quot; \\
    \-q research-hpc \\
    \-a &quot;docker(broadinstitute/gatk:4.1.2.0)&quot; \\
    /gatk/gatk --java-options &quot;-Xms4G -Xmx\${MEM}G
    \-DGATK_STACKTRACE_ON_USER_EXCEPTION=true&quot; GenomicsDBImport \\
    \--genomicsdb-workspace-path \${WORKDIR} \\
    \--batch-size 50 \\
    \-L \${CHR} \\
    \--sample-name-map \${mylist} \\
    \--tmp-dir=\${tmpPATH} \\
    \--max-num-intervals-to-import-in-parallel 10 \\
    \--reader-threads \${THREADS}
    Done</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>GenotypeGVCFs</li>
</ol>
<pre><code>    EMAIL=&quot;achal@wustl.edu&quot;
    arr=&quot;\$(echo {1..22} X Y)&quot;
    len=\${\#arr[\*]}
    \#iterate with for loop
    cd /gscmnt/gc2645/wgs/WXS_Aquilla/03-FINAL/VCFs/
    export BASE=&quot;/gscmnt/gc2645/wgs&quot;; \\
    export TILEDB_DISABLE_FILE_LOCKING=1; \\
    export REF=&quot;\${BASE}/Genome_Ref/GRCh38/Homo_sapiens_assembly38.fasta&quot;; \\
    export tmpPATH=&quot;--tmp-dir=\${BASE}/WXS_Aquilla/gvcfTest/temp&quot;; \\
    for (( i=0; i\&lt;len; i++ )); \\
    do
    CHR=&quot;chr\${arr[\$i]}&quot;; \\
    echo &quot;Doing :: \${CHR}&quot;; \\
    \# Make sure you have three ///
    export DB=&quot;-V gendb:///\${BASE}/WXS_Aquilla/03-FINAL/VCFs/\${CHR}&quot;; \\
    export
    OUTvcf=&quot;\${BASE}/WXS_Aquilla/03-FINAL/VCFs/Bloomfield_\${CHR}.vcf.gz&quot;; \\
    export MEM=40; \\
    bsub \\
    \-J &quot;Geno\${CHR}&quot; \\
    \-u &quot;\${EMAIL}&quot; \\
    \-n1 -W 25160 \\
    \-M 49000000 \\
    \-R &quot;rusage[mem=49152]&quot; \\
    \-o
    &quot;\${BASE}/WXS_Aquilla/03-FINAL/VCFs/Bloomfield_joint_call_logs_\${CHR}.%J&quot;
    \\
    \-q research-hpc \\
    \-a &quot;docker(broadinstitute/gatk:4.1.2.0)&quot; \\
    /gatk/gatk --java-options &quot;-Xms4G -Xmx\${MEM}G
    \-DGATK_STACKTRACE_ON_USER_EXCEPTION=true&quot; GenotypeGVCFs \\
    \-R \${REF} \\
    \${DB} \\
    \-O \${OUTvcf} \\
    \${tmpPATH}
    sleep 20
    done</code></pre>
</div>
</div>
</div>
