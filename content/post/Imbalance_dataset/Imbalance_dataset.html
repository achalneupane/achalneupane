---
Authors: ["**Achal Neupane**"]
title: "Imbalance dataset: an example of credit-card fraud detection problem"
date: 2019-04-19T17:26:23-05:00
draft: false
output: html_document
tags:
- Python
- Statistics
- Machine Learning
summary: Statistics series
---



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<p>The challenge is to recognize fraudulent credit card transactions so that the customers of credit card companies are not charged for items that they did not purchase.</p>
<p>Main challenges involved in credit card fraud detection are:</p>
<ol style="list-style-type: decimal">
<li>Enormous Data is processed every day and the model build must be fast enough to respond to the scam in time.</li>
<li>Imbalanced Data i.e most of the transactions (99.8%) are not fraudulent which makes it really hard for detecting the fraudulent ones</li>
<li>Data availability as the data is mostly private.</li>
<li>Misclassified Data can be another major issue, as not every fraudulent transaction is caught and reported.</li>
<li>Adaptive techniques used against the model by the scammers.</li>
</ol>
<p>How to tackle these challenges?</p>
<ol style="list-style-type: decimal">
<li>The model used must be simple and fast enough to detect the anomaly and classify it as a fraudulent transaction as quickly as possible.</li>
<li>Imbalance can be dealt with by properly using some methods which we will talk about in the next paragraph</li>
<li>For protecting the privacy of the user the dimensionality of the data can be reduced.</li>
<li>A more trustworthy source must be taken which double-check the data, at least for training the model.</li>
<li>We can make the model simple and interpretable so that when the scammer adapts to it with just some tweaks we can have a new model up and running to deploy.</li>
</ol>
<pre class="python"><code># import the necessary packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import gridspec</code></pre>
<pre class="python"><code># Load the dataset from the csv file using pandas best way is to mount the drive
# on colab and copy the path for the csv file; download from
# https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download&amp;select=creditcard.csv
data = pd.read_csv(&quot;/Users/aneupane/Desktop/python/kaggle_data/creditcard.csv.zip&quot;)
data.head(n=5)


# Print the shape of the data
# data = data.sample(frac = 0.1, random_state = 48)</code></pre>
<pre><code>##    Time        V1        V2        V3  ...       V27       V28  Amount  Class
## 0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0
## 1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0
## 2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0
## 3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0
## 4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0
## 
## [5 rows x 31 columns]</code></pre>
<pre class="python"><code>print(data.shape)</code></pre>
<pre><code>## (284807, 31)</code></pre>
<pre class="python"><code>print(data.describe())

# Determine number of fraud cases in dataset</code></pre>
<pre><code>##                 Time            V1  ...         Amount          Class
## count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000
## mean    94813.859575  1.168375e-15  ...      88.349619       0.001727
## std     47488.145955  1.958696e+00  ...     250.120109       0.041527
## min         0.000000 -5.640751e+01  ...       0.000000       0.000000
## 25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000
## 50%     84692.000000  1.810880e-02  ...      22.000000       0.000000
## 75%    139320.500000  1.315642e+00  ...      77.165000       0.000000
## max    172792.000000  2.454930e+00  ...   25691.160000       1.000000
## 
## [8 rows x 31 columns]</code></pre>
<pre class="python"><code>fraud = data[data.Class == 1] # how many frauds
valid = data[data.Class == 0] # how many valid
outlierFraction = len(fraud)/float(len(valid))
print(outlierFraction)</code></pre>
<pre><code>## 0.0017304750013189597</code></pre>
<pre class="python"><code>print(&#39;Fraud Cases: {}&#39;.format(len(data[data[&#39;Class&#39;] == 1])))</code></pre>
<pre><code>## Fraud Cases: 492</code></pre>
<pre class="python"><code>print(&#39;Valid Transactions: {}&#39;.format(len(data[data[&#39;Class&#39;] == 0])))</code></pre>
<pre><code>## Valid Transactions: 284315</code></pre>
<p>Only 0.17% fraudulent transaction out all the transactions. The data is highly Unbalanced. Lets first apply our models without balancing it and if we don’t get a good accuracy then we can find a way to balance this dataset. But first, let’s implement the model without it and will balance the data only if needed.</p>
<pre class="python"><code>print(&quot;Amount details of the fraudulent transaction&quot;)</code></pre>
<pre><code>## Amount details of the fraudulent transaction</code></pre>
<pre class="python"><code>fraud.Amount.describe()</code></pre>
<pre><code>## count     492.000000
## mean      122.211321
## std       256.683288
## min         0.000000
## 25%         1.000000
## 50%         9.250000
## 75%       105.890000
## max      2125.870000
## Name: Amount, dtype: float64</code></pre>
<pre class="python"><code>print(&quot;details of valid transaction&quot;)</code></pre>
<pre><code>## details of valid transaction</code></pre>
<pre class="python"><code>valid.Amount.describe()</code></pre>
<pre><code>## count    284315.000000
## mean         88.291022
## std         250.105092
## min           0.000000
## 25%           5.650000
## 50%          22.000000
## 75%          77.050000
## max       25691.160000
## Name: Amount, dtype: float64</code></pre>
<p>Correlation matrix graphically gives us an idea of how features correlate with each other and can help us predict what are the features that are most relevant for the prediction.</p>
<pre class="python"><code># Correlation matrix
corrmat = data.corr()
fig = plt.figure(figsize = (12, 9))
sns.heatmap(corrmat, vmax = .8, square = True)
plt.show()</code></pre>
<p><img src="/post/Imbalance_dataset/Imbalance_dataset_files/figure-html/unnamed-chunk-4-1.png" width="1152" /></p>
<p>In the HeatMap, we can clearly see that most of the features do not correlate to other features but there are some features that either has a positive or a negative correlation with each other. For example, V2 and V5 are highly negatively correlated with the feature called Amount. We also see some correlation with V20 and Amount. This gives us a deeper understanding of the Data available to us.</p>
<p>Dividing the data into inputs parameters and outputs value format</p>
<pre class="python"><code># dividing the X and the Y from the dataset
X = data.drop([&#39;Class&#39;], axis = 1)
Y = data[&quot;Class&quot;]
print(X.shape)</code></pre>
<pre><code>## (284807, 30)</code></pre>
<pre class="python"><code>print(Y.shape)
# getting just the values for the sake of processing
# (its a numpy array with no columns)</code></pre>
<pre><code>## (284807,)</code></pre>
<pre class="python"><code>xData = X.values
yData = Y.values

df=pd.DataFrame([Y]).T.value_counts()</code></pre>
<p>Next, we will be dividing the dataset into two main groups. One for training the model and the other for Testing our trained model’s performance.</p>
<pre class="python"><code># Using Scikit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
xTrain, xTest, yTrain, yTest = train_test_split(
        xData, yData, test_size = 0.2, random_state = 42)


# Building the Random Forest Classifier (RANDOM FOREST)
from sklearn.ensemble import RandomForestClassifier
# random forest model creation
rfc = RandomForestClassifier()
rfc.fit(xTrain, yTrain)

# predictions</code></pre>
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier()</pre></div></div></div></div></div>
<pre class="python"><code>yPred = rfc.predict(xTest)</code></pre>
<p>Now, we will evaluate the model parameters.</p>
<pre class="python"><code># Evaluating the classifier
# printing every score of the classifier
# scoring in anything
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, matthews_corrcoef
from sklearn.metrics import confusion_matrix

n_outliers = len(fraud)
n_errors = (yPred != yTest).sum()
print(&quot;The model used is Random Forest classifier&quot;)</code></pre>
<pre><code>## The model used is Random Forest classifier</code></pre>
<pre class="python"><code>acc = accuracy_score(yTest, yPred)
print(&quot;The accuracy is {}&quot;.format(acc))</code></pre>
<pre><code>## The accuracy is 0.9996137776061234</code></pre>
<pre class="python"><code>prec = precision_score(yTest, yPred)
print(&quot;The precision is {}&quot;.format(prec))</code></pre>
<pre><code>## The precision is 0.975</code></pre>
<pre class="python"><code>rec = recall_score(yTest, yPred)
print(&quot;The recall is {}&quot;.format(rec))</code></pre>
<pre><code>## The recall is 0.7959183673469388</code></pre>
<pre class="python"><code>f1 = f1_score(yTest, yPred)
print(&quot;The F1-Score is {}&quot;.format(f1))</code></pre>
<pre><code>## The F1-Score is 0.8764044943820225</code></pre>
<pre class="python"><code>MCC = matthews_corrcoef(yTest, yPred)
print(&quot;The Matthews correlation coefficient is {}&quot;.format(MCC))</code></pre>
<pre><code>## The Matthews correlation coefficient is 0.8807418913871203</code></pre>
<p>Visualizing the confusion matrix</p>
<pre class="python"><code># printing the confusion matrix
LABELS = [&#39;Normal&#39;, &#39;Fraud&#39;]
conf_matrix = confusion_matrix(yTest, yPred)
plt.figure(figsize =(12, 12))
sns.heatmap(conf_matrix, xticklabels = LABELS,
            yticklabels = LABELS, annot = True, fmt =&quot;d&quot;);
plt.title(&quot;Confusion matrix&quot;)
plt.ylabel(&#39;True class&#39;)
plt.xlabel(&#39;Predicted class&#39;)
plt.show()</code></pre>
<p><img src="/post/Imbalance_dataset/Imbalance_dataset_files/figure-html/unnamed-chunk-8-3.png" width="1152" /></p>
<p>As we can clearly see that with our Random Forest Model is getting a better results</p>
